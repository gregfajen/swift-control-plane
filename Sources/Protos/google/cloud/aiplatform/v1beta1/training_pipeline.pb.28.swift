// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/aiplatform/v1beta1/training_pipeline.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2020 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// The TrainingPipeline orchestrates tasks associated with training a Model. It
/// always executes the training task, and optionally may also
/// export data from AI Platform's Dataset which becomes the training input,
/// [upload][google.cloud.aiplatform.v1beta1.ModelService.UploadModel] the Model to AI Platform, and evaluate the
/// Model.
public struct Google_Cloud_Aiplatform_V1beta1_TrainingPipeline {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. Resource name of the TrainingPipeline.
  public var name: String {
    get {return _storage._name}
    set {_uniqueStorage()._name = newValue}
  }

  /// Required. The user-defined name of this TrainingPipeline.
  public var displayName: String {
    get {return _storage._displayName}
    set {_uniqueStorage()._displayName = newValue}
  }

  /// Specifies AI Platform owned input data that may be used for training the
  /// Model. The TrainingPipeline's [training_task_definition][google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition] should make
  /// clear whether this config is used and if there are any special requirements
  /// on how it should be filled. If nothing about this config is mentioned in
  /// the [training_task_definition][google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition], then it should be assumed that the
  /// TrainingPipeline does not depend on this configuration.
  public var inputDataConfig: Google_Cloud_Aiplatform_V1beta1_InputDataConfig {
    get {return _storage._inputDataConfig ?? Google_Cloud_Aiplatform_V1beta1_InputDataConfig()}
    set {_uniqueStorage()._inputDataConfig = newValue}
  }
  /// Returns true if `inputDataConfig` has been explicitly set.
  public var hasInputDataConfig: Bool {return _storage._inputDataConfig != nil}
  /// Clears the value of `inputDataConfig`. Subsequent reads from it will return its default value.
  public mutating func clearInputDataConfig() {_uniqueStorage()._inputDataConfig = nil}

  /// Required. A Google Cloud Storage path to the YAML file that defines the training task
  /// which is responsible for producing the model artifact, and may also include
  /// additional auxiliary work.
  /// The definition files that can be used here are found in
  /// gs://google-cloud-aiplatform/schema/trainingjob/definition/.
  /// Note: The URI given on output will be immutable and probably different,
  /// including the URI scheme, than the one given on input. The output URI will
  /// point to a location where the user only has a read access.
  public var trainingTaskDefinition: String {
    get {return _storage._trainingTaskDefinition}
    set {_uniqueStorage()._trainingTaskDefinition = newValue}
  }

  /// Required. The training task's parameter(s), as specified in the
  /// [training_task_definition][google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition]'s `inputs`.
  public var trainingTaskInputs: SwiftProtobuf.Google_Protobuf_Value {
    get {return _storage._trainingTaskInputs ?? SwiftProtobuf.Google_Protobuf_Value()}
    set {_uniqueStorage()._trainingTaskInputs = newValue}
  }
  /// Returns true if `trainingTaskInputs` has been explicitly set.
  public var hasTrainingTaskInputs: Bool {return _storage._trainingTaskInputs != nil}
  /// Clears the value of `trainingTaskInputs`. Subsequent reads from it will return its default value.
  public mutating func clearTrainingTaskInputs() {_uniqueStorage()._trainingTaskInputs = nil}

  /// Output only. The metadata information as specified in the [training_task_definition][google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition]'s
  /// `metadata`. This metadata is an auxiliary runtime and final information
  /// about the training task. While the pipeline is running this information is
  /// populated only at a best effort basis. Only present if the
  /// pipeline's [training_task_definition][google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition] contains `metadata` object.
  public var trainingTaskMetadata: SwiftProtobuf.Google_Protobuf_Value {
    get {return _storage._trainingTaskMetadata ?? SwiftProtobuf.Google_Protobuf_Value()}
    set {_uniqueStorage()._trainingTaskMetadata = newValue}
  }
  /// Returns true if `trainingTaskMetadata` has been explicitly set.
  public var hasTrainingTaskMetadata: Bool {return _storage._trainingTaskMetadata != nil}
  /// Clears the value of `trainingTaskMetadata`. Subsequent reads from it will return its default value.
  public mutating func clearTrainingTaskMetadata() {_uniqueStorage()._trainingTaskMetadata = nil}

  /// Describes the Model that may be uploaded (via [ModelService.UploadMode][])
  /// by this TrainingPipeline. The TrainingPipeline's
  /// [training_task_definition][google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition] should make clear whether this Model
  /// description should be populated, and if there are any special requirements
  /// regarding how it should be filled. If nothing is mentioned in the
  /// [training_task_definition][google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition], then it should be assumed that this field
  /// should not be filled and the training task either uploads the Model without
  /// a need of this information, or that training task does not support
  /// uploading a Model as part of the pipeline.
  /// When the Pipeline's state becomes `PIPELINE_STATE_SUCCEEDED` and
  /// the trained Model had been uploaded into AI Platform, then the
  /// model_to_upload's resource [name][google.cloud.aiplatform.v1beta1.Model.name] is populated. The Model
  /// is always uploaded into the Project and Location in which this pipeline
  /// is.
  public var modelToUpload: Google_Cloud_Aiplatform_V1beta1_Model {
    get {return _storage._modelToUpload ?? Google_Cloud_Aiplatform_V1beta1_Model()}
    set {_uniqueStorage()._modelToUpload = newValue}
  }
  /// Returns true if `modelToUpload` has been explicitly set.
  public var hasModelToUpload: Bool {return _storage._modelToUpload != nil}
  /// Clears the value of `modelToUpload`. Subsequent reads from it will return its default value.
  public mutating func clearModelToUpload() {_uniqueStorage()._modelToUpload = nil}

  /// Output only. The detailed state of the pipeline.
  public var state: Google_Cloud_Aiplatform_V1beta1_PipelineState {
    get {return _storage._state}
    set {_uniqueStorage()._state = newValue}
  }

  /// Output only. Only populated when the pipeline's state is `PIPELINE_STATE_FAILED` or
  /// `PIPELINE_STATE_CANCELLED`.
  public var error: Google_Rpc_Status {
    get {return _storage._error ?? Google_Rpc_Status()}
    set {_uniqueStorage()._error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return _storage._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {_uniqueStorage()._error = nil}

  /// Output only. Time when the TrainingPipeline was created.
  public var createTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._createTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._createTime = newValue}
  }
  /// Returns true if `createTime` has been explicitly set.
  public var hasCreateTime: Bool {return _storage._createTime != nil}
  /// Clears the value of `createTime`. Subsequent reads from it will return its default value.
  public mutating func clearCreateTime() {_uniqueStorage()._createTime = nil}

  /// Output only. Time when the TrainingPipeline for the first time entered the
  /// `PIPELINE_STATE_RUNNING` state.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return _storage._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {_uniqueStorage()._startTime = nil}

  /// Output only. Time when the TrainingPipeline entered any of the following states:
  /// `PIPELINE_STATE_SUCCEEDED`, `PIPELINE_STATE_FAILED`,
  /// `PIPELINE_STATE_CANCELLED`.
  public var endTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._endTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._endTime = newValue}
  }
  /// Returns true if `endTime` has been explicitly set.
  public var hasEndTime: Bool {return _storage._endTime != nil}
  /// Clears the value of `endTime`. Subsequent reads from it will return its default value.
  public mutating func clearEndTime() {_uniqueStorage()._endTime = nil}

  /// Output only. Time when the TrainingPipeline was most recently updated.
  public var updateTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._updateTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._updateTime = newValue}
  }
  /// Returns true if `updateTime` has been explicitly set.
  public var hasUpdateTime: Bool {return _storage._updateTime != nil}
  /// Clears the value of `updateTime`. Subsequent reads from it will return its default value.
  public mutating func clearUpdateTime() {_uniqueStorage()._updateTime = nil}

  /// The labels with user-defined metadata to organize TrainingPipelines.
  ///
  /// Label keys and values can be no longer than 64 characters
  /// (Unicode codepoints), can only contain lowercase letters, numeric
  /// characters, underscores and dashes. International characters are allowed.
  ///
  /// See https://goo.gl/xmQnxf for more information and examples of labels.
  public var labels: Dictionary<String,String> {
    get {return _storage._labels}
    set {_uniqueStorage()._labels = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Specifies AI Platform owned input data to be used for training, and
/// possibly evaluating, the Model.
public struct Google_Cloud_Aiplatform_V1beta1_InputDataConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The instructions how the input data should be split between the
  /// training, validation and test sets.
  /// If no split type is provided, the [fraction_split][google.cloud.aiplatform.v1beta1.InputDataConfig.fraction_split] is used by default.
  public var split: Google_Cloud_Aiplatform_V1beta1_InputDataConfig.OneOf_Split? = nil

  /// Split based on fractions defining the size of each set.
  public var fractionSplit: Google_Cloud_Aiplatform_V1beta1_FractionSplit {
    get {
      if case .fractionSplit(let v)? = split {return v}
      return Google_Cloud_Aiplatform_V1beta1_FractionSplit()
    }
    set {split = .fractionSplit(newValue)}
  }

  /// Split based on the provided filters for each set.
  public var filterSplit: Google_Cloud_Aiplatform_V1beta1_FilterSplit {
    get {
      if case .filterSplit(let v)? = split {return v}
      return Google_Cloud_Aiplatform_V1beta1_FilterSplit()
    }
    set {split = .filterSplit(newValue)}
  }

  /// Supported only for tabular Datasets.
  ///
  /// Split based on a predefined key.
  public var predefinedSplit: Google_Cloud_Aiplatform_V1beta1_PredefinedSplit {
    get {
      if case .predefinedSplit(let v)? = split {return v}
      return Google_Cloud_Aiplatform_V1beta1_PredefinedSplit()
    }
    set {split = .predefinedSplit(newValue)}
  }

  /// Supported only for tabular Datasets.
  ///
  /// Split based on the timestamp of the input data pieces.
  public var timestampSplit: Google_Cloud_Aiplatform_V1beta1_TimestampSplit {
    get {
      if case .timestampSplit(let v)? = split {return v}
      return Google_Cloud_Aiplatform_V1beta1_TimestampSplit()
    }
    set {split = .timestampSplit(newValue)}
  }

  /// Only applicable to Custom and Hyperparameter Tuning TrainingPipelines.
  ///
  /// The destination of the training data to be written to.
  ///
  /// Supported destination file formats:
  ///   * For non-tabular data: "jsonl".
  ///   * For tabular data: "csv" and "bigquery".
  ///
  /// Following AI Platform environment variables will be passed to containers
  /// or python modules of the training task when this field is set:
  ///
  /// * AIP_DATA_FORMAT : Exported data format.
  /// * AIP_TRAINING_DATA_URI : Sharded exported training data uris.
  /// * AIP_VALIDATION_DATA_URI : Sharded exported validation data uris.
  /// * AIP_TEST_DATA_URI : Sharded exported test data uris.
  public var destination: Google_Cloud_Aiplatform_V1beta1_InputDataConfig.OneOf_Destination? = nil

  /// The Google Cloud Storage location where the training data is to be
  /// written to. In the given directory a new directory will be created with
  /// name:
  /// `dataset-<dataset-id>-<annotation-type>-<timestamp-of-training-call>`
  /// where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
  /// All training input data will be written into that directory.
  ///
  /// The AI Platform environment variables representing Google Cloud Storage
  /// data URIs will always be represented in the Google Cloud Storage wildcard
  /// format to support sharded data. e.g.: "gs://.../training-*.jsonl"
  ///
  /// * AIP_DATA_FORMAT = "jsonl" for non-tabular data, "csv" for tabular data
  /// * AIP_TRAINING_DATA_URI  =
  ///
  /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/training-*.${AIP_DATA_FORMAT}"
  /// * AIP_VALIDATION_DATA_URI =
  ///
  /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/validation-*.${AIP_DATA_FORMAT}"
  /// * AIP_TEST_DATA_URI =
  ///
  /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/test-*.${AIP_DATA_FORMAT}"
  public var gcsDestination: Google_Cloud_Aiplatform_V1beta1_GcsDestination {
    get {
      if case .gcsDestination(let v)? = destination {return v}
      return Google_Cloud_Aiplatform_V1beta1_GcsDestination()
    }
    set {destination = .gcsDestination(newValue)}
  }

  /// The BigQuery project location where the training data is to be written
  /// to. In the given project a new dataset is created with name
  /// `dataset_<dataset-id>_<annotation-type>_<timestamp-of-training-call>`
  /// where timestamp is in YYYY_MM_DDThh_mm_ss_sssZ format. All training
  /// input data will be written into that dataset. In the dataset three
  /// tables will be created, `training`, `validation` and `test`.
  ///
  /// * AIP_DATA_FORMAT = "bigquery".
  /// * AIP_TRAINING_DATA_URI  =
  ///
  /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.training"
  /// * AIP_VALIDATION_DATA_URI =
  ///
  /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.validation"
  /// * AIP_TEST_DATA_URI =
  /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.test"
  public var bigqueryDestination: Google_Cloud_Aiplatform_V1beta1_BigQueryDestination {
    get {
      if case .bigqueryDestination(let v)? = destination {return v}
      return Google_Cloud_Aiplatform_V1beta1_BigQueryDestination()
    }
    set {destination = .bigqueryDestination(newValue)}
  }

  /// Required. The ID of the Dataset in the same Project and Location which data will be
  /// used to train the Model. The Dataset must use schema compatible with
  /// Model being trained, and what is compatible should be described in the
  /// used TrainingPipeline's [training_task_definition]
  /// [google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition].
  /// For tabular Datasets, all their data is exported to training, to pick
  /// and choose from.
  public var datasetID: String = String()

  /// Only applicable to Datasets that have DataItems and Annotations.
  ///
  /// A filter on Annotations of the Dataset. Only Annotations that both
  /// match this filter and belong to DataItems not ignored by the split method
  /// are used in respectively training, validation or test role, depending on
  /// the role of the DataItem they are on (for the auto-assigned that role is
  /// decided by AI Platform). A filter with same syntax as the one used in
  /// [ListAnnotations][google.cloud.aiplatform.v1beta1.DatasetService.ListAnnotations] may be used, but note
  /// here it filters across all Annotations of the Dataset, and not just within
  /// a single DataItem.
  public var annotationsFilter: String = String()

  /// Only applicable to custom training.
  ///
  /// Google Cloud Storage URI points to a YAML file describing annotation
  /// schema. The schema is defined as an OpenAPI 3.0.2 [Schema Object](
  ///
  /// https:
  /// //github.com/OAI/OpenAPI-Specification/b
  /// // lob/master/versions/3.0.2.md#schema-object)
  /// The schema files that can be used here are found in
  /// gs://google-cloud-aiplatform/schema/dataset/annotation/, note that the
  /// chosen schema must be consistent with
  /// [metadata][google.cloud.aiplatform.v1beta1.Dataset.metadata_schema_uri] of the Dataset specified by
  /// [dataset_id][google.cloud.aiplatform.v1beta1.InputDataConfig.dataset_id].
  ///
  /// Only Annotations that both match this schema and belong to DataItems not
  /// ignored by the split method are used in respectively training, validation
  /// or test role, depending on the role of the DataItem they are on.
  ///
  /// When used in conjunction with [annotations_filter][google.cloud.aiplatform.v1beta1.InputDataConfig.annotations_filter], the Annotations used
  /// for training are filtered by both [annotations_filter][google.cloud.aiplatform.v1beta1.InputDataConfig.annotations_filter] and
  /// [annotation_schema_uri][google.cloud.aiplatform.v1beta1.InputDataConfig.annotation_schema_uri].
  public var annotationSchemaUri: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The instructions how the input data should be split between the
  /// training, validation and test sets.
  /// If no split type is provided, the [fraction_split][google.cloud.aiplatform.v1beta1.InputDataConfig.fraction_split] is used by default.
  public enum OneOf_Split: Equatable {
    /// Split based on fractions defining the size of each set.
    case fractionSplit(Google_Cloud_Aiplatform_V1beta1_FractionSplit)
    /// Split based on the provided filters for each set.
    case filterSplit(Google_Cloud_Aiplatform_V1beta1_FilterSplit)
    /// Supported only for tabular Datasets.
    ///
    /// Split based on a predefined key.
    case predefinedSplit(Google_Cloud_Aiplatform_V1beta1_PredefinedSplit)
    /// Supported only for tabular Datasets.
    ///
    /// Split based on the timestamp of the input data pieces.
    case timestampSplit(Google_Cloud_Aiplatform_V1beta1_TimestampSplit)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_InputDataConfig.OneOf_Split, rhs: Google_Cloud_Aiplatform_V1beta1_InputDataConfig.OneOf_Split) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.fractionSplit, .fractionSplit): return {
        guard case .fractionSplit(let l) = lhs, case .fractionSplit(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.filterSplit, .filterSplit): return {
        guard case .filterSplit(let l) = lhs, case .filterSplit(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.predefinedSplit, .predefinedSplit): return {
        guard case .predefinedSplit(let l) = lhs, case .predefinedSplit(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.timestampSplit, .timestampSplit): return {
        guard case .timestampSplit(let l) = lhs, case .timestampSplit(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  /// Only applicable to Custom and Hyperparameter Tuning TrainingPipelines.
  ///
  /// The destination of the training data to be written to.
  ///
  /// Supported destination file formats:
  ///   * For non-tabular data: "jsonl".
  ///   * For tabular data: "csv" and "bigquery".
  ///
  /// Following AI Platform environment variables will be passed to containers
  /// or python modules of the training task when this field is set:
  ///
  /// * AIP_DATA_FORMAT : Exported data format.
  /// * AIP_TRAINING_DATA_URI : Sharded exported training data uris.
  /// * AIP_VALIDATION_DATA_URI : Sharded exported validation data uris.
  /// * AIP_TEST_DATA_URI : Sharded exported test data uris.
  public enum OneOf_Destination: Equatable {
    /// The Google Cloud Storage location where the training data is to be
    /// written to. In the given directory a new directory will be created with
    /// name:
    /// `dataset-<dataset-id>-<annotation-type>-<timestamp-of-training-call>`
    /// where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
    /// All training input data will be written into that directory.
    ///
    /// The AI Platform environment variables representing Google Cloud Storage
    /// data URIs will always be represented in the Google Cloud Storage wildcard
    /// format to support sharded data. e.g.: "gs://.../training-*.jsonl"
    ///
    /// * AIP_DATA_FORMAT = "jsonl" for non-tabular data, "csv" for tabular data
    /// * AIP_TRAINING_DATA_URI  =
    ///
    /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/training-*.${AIP_DATA_FORMAT}"
    /// * AIP_VALIDATION_DATA_URI =
    ///
    /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/validation-*.${AIP_DATA_FORMAT}"
    /// * AIP_TEST_DATA_URI =
    ///
    /// "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/test-*.${AIP_DATA_FORMAT}"
    case gcsDestination(Google_Cloud_Aiplatform_V1beta1_GcsDestination)
    /// The BigQuery project location where the training data is to be written
    /// to. In the given project a new dataset is created with name
    /// `dataset_<dataset-id>_<annotation-type>_<timestamp-of-training-call>`
    /// where timestamp is in YYYY_MM_DDThh_mm_ss_sssZ format. All training
    /// input data will be written into that dataset. In the dataset three
    /// tables will be created, `training`, `validation` and `test`.
    ///
    /// * AIP_DATA_FORMAT = "bigquery".
    /// * AIP_TRAINING_DATA_URI  =
    ///
    /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.training"
    /// * AIP_VALIDATION_DATA_URI =
    ///
    /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.validation"
    /// * AIP_TEST_DATA_URI =
    /// "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.test"
    case bigqueryDestination(Google_Cloud_Aiplatform_V1beta1_BigQueryDestination)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_InputDataConfig.OneOf_Destination, rhs: Google_Cloud_Aiplatform_V1beta1_InputDataConfig.OneOf_Destination) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.gcsDestination, .gcsDestination): return {
        guard case .gcsDestination(let l) = lhs, case .gcsDestination(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.bigqueryDestination, .bigqueryDestination): return {
        guard case .bigqueryDestination(let l) = lhs, case .bigqueryDestination(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}
}

/// Assigns the input data to training, validation, and test sets as per the
/// given fractions. Any of `training_fraction`, `validation_fraction` and
/// `test_fraction` may optionally be provided, they must sum to up to 1. If the
/// provided ones sum to less than 1, the remainder is assigned to sets as
/// decided by AI Platform. If none of the fractions are set, by default roughly
/// 80% of data will be used for training, 10% for validation, and 10% for test.
public struct Google_Cloud_Aiplatform_V1beta1_FractionSplit {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The fraction of the input data that is to be used to train the Model.
  public var trainingFraction: Double = 0

  /// The fraction of the input data that is to be used to validate the Model.
  public var validationFraction: Double = 0

  /// The fraction of the input data that is to be used to evaluate the Model.
  public var testFraction: Double = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Assigns input data to training, validation, and test sets based on the given
/// filters, data pieces not matched by any filter are ignored. Currently only
/// supported for Datasets containing DataItems.
/// If any of the filters in this message are to match nothing, then they can be
/// set as '-' (the minus sign).
public struct Google_Cloud_Aiplatform_V1beta1_FilterSplit {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. A filter on DataItems of the Dataset. DataItems that match
  /// this filter are used to train the Model. A filter with same syntax
  /// as the one used in [DatasetService.ListDataItems][google.cloud.aiplatform.v1beta1.DatasetService.ListDataItems] may be used. If a
  /// single DataItem is matched by more than one of the FilterSplit filters,
  /// then it will be assigned to the first set that applies to it in the
  /// training, validation, test order.
  public var trainingFilter: String = String()

  /// Required. A filter on DataItems of the Dataset. DataItems that match
  /// this filter are used to validate the Model. A filter with same syntax
  /// as the one used in [DatasetService.ListDataItems][google.cloud.aiplatform.v1beta1.DatasetService.ListDataItems] may be used. If a
  /// single DataItem is matched by more than one of the FilterSplit filters,
  /// then it will be assigned to the first set that applies to it in the
  /// training, validation, test order.
  public var validationFilter: String = String()

  /// Required. A filter on DataItems of the Dataset. DataItems that match
  /// this filter are used to test the Model. A filter with same syntax
  /// as the one used in [DatasetService.ListDataItems][google.cloud.aiplatform.v1beta1.DatasetService.ListDataItems] may be used. If a
  /// single DataItem is matched by more than one of the FilterSplit filters,
  /// then it will be assigned to the first set that applies to it in the
  /// training, validation, test order.
  public var testFilter: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Assigns input data to training, validation, and test sets based on the
/// value of a provided key.
///
/// Supported only for tabular Datasets.
public struct Google_Cloud_Aiplatform_V1beta1_PredefinedSplit {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The key is a name of one of the Dataset's data columns.
  /// The value of the key (either the label's value or value in the column)
  /// must be one of {`training`, `validation`, `test`}, and it defines to which
  /// set the given piece of data is assigned. If for a piece of data the key
  /// is not present or has an invalid value, that piece is ignored by the
  /// pipeline.
  public var key: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Assigns input data to training, validation, and test sets based on a
/// provided timestamps. The youngest data pieces are assigned to training set,
/// next to validation set, and the oldest to the test set.
///
/// Supported only for tabular Datasets.
public struct Google_Cloud_Aiplatform_V1beta1_TimestampSplit {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The fraction of the input data that is to be used to train the Model.
  public var trainingFraction: Double = 0

  /// The fraction of the input data that is to be used to validate the Model.
  public var validationFraction: Double = 0

  /// The fraction of the input data that is to be used to evaluate the Model.
  public var testFraction: Double = 0

  /// Required. The key is a name of one of the Dataset's data columns.
  /// The values of the key (the values in the column) must be in RFC 3339
  /// `date-time` format, where `time-offset` = `"Z"`
  /// (e.g. 1985-04-12T23:20:50.52Z). If for a piece of data the key is not
  /// present or has an invalid value, that piece is ignored by the pipeline.
  public var key: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.aiplatform.v1beta1"

extension Google_Cloud_Aiplatform_V1beta1_TrainingPipeline: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TrainingPipeline"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .standard(proto: "display_name"),
    3: .standard(proto: "input_data_config"),
    4: .standard(proto: "training_task_definition"),
    5: .standard(proto: "training_task_inputs"),
    6: .standard(proto: "training_task_metadata"),
    7: .standard(proto: "model_to_upload"),
    9: .same(proto: "state"),
    10: .same(proto: "error"),
    11: .standard(proto: "create_time"),
    12: .standard(proto: "start_time"),
    13: .standard(proto: "end_time"),
    14: .standard(proto: "update_time"),
    15: .same(proto: "labels"),
  ]

  fileprivate class _StorageClass {
    var _name: String = String()
    var _displayName: String = String()
    var _inputDataConfig: Google_Cloud_Aiplatform_V1beta1_InputDataConfig? = nil
    var _trainingTaskDefinition: String = String()
    var _trainingTaskInputs: SwiftProtobuf.Google_Protobuf_Value? = nil
    var _trainingTaskMetadata: SwiftProtobuf.Google_Protobuf_Value? = nil
    var _modelToUpload: Google_Cloud_Aiplatform_V1beta1_Model? = nil
    var _state: Google_Cloud_Aiplatform_V1beta1_PipelineState = .unspecified
    var _error: Google_Rpc_Status? = nil
    var _createTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _endTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _updateTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _labels: Dictionary<String,String> = [:]

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _name = source._name
      _displayName = source._displayName
      _inputDataConfig = source._inputDataConfig
      _trainingTaskDefinition = source._trainingTaskDefinition
      _trainingTaskInputs = source._trainingTaskInputs
      _trainingTaskMetadata = source._trainingTaskMetadata
      _modelToUpload = source._modelToUpload
      _state = source._state
      _error = source._error
      _createTime = source._createTime
      _startTime = source._startTime
      _endTime = source._endTime
      _updateTime = source._updateTime
      _labels = source._labels
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularStringField(value: &_storage._name) }()
        case 2: try { try decoder.decodeSingularStringField(value: &_storage._displayName) }()
        case 3: try { try decoder.decodeSingularMessageField(value: &_storage._inputDataConfig) }()
        case 4: try { try decoder.decodeSingularStringField(value: &_storage._trainingTaskDefinition) }()
        case 5: try { try decoder.decodeSingularMessageField(value: &_storage._trainingTaskInputs) }()
        case 6: try { try decoder.decodeSingularMessageField(value: &_storage._trainingTaskMetadata) }()
        case 7: try { try decoder.decodeSingularMessageField(value: &_storage._modelToUpload) }()
        case 9: try { try decoder.decodeSingularEnumField(value: &_storage._state) }()
        case 10: try { try decoder.decodeSingularMessageField(value: &_storage._error) }()
        case 11: try { try decoder.decodeSingularMessageField(value: &_storage._createTime) }()
        case 12: try { try decoder.decodeSingularMessageField(value: &_storage._startTime) }()
        case 13: try { try decoder.decodeSingularMessageField(value: &_storage._endTime) }()
        case 14: try { try decoder.decodeSingularMessageField(value: &_storage._updateTime) }()
        case 15: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &_storage._labels) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if !_storage._name.isEmpty {
        try visitor.visitSingularStringField(value: _storage._name, fieldNumber: 1)
      }
      if !_storage._displayName.isEmpty {
        try visitor.visitSingularStringField(value: _storage._displayName, fieldNumber: 2)
      }
      if let v = _storage._inputDataConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      }
      if !_storage._trainingTaskDefinition.isEmpty {
        try visitor.visitSingularStringField(value: _storage._trainingTaskDefinition, fieldNumber: 4)
      }
      if let v = _storage._trainingTaskInputs {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      }
      if let v = _storage._trainingTaskMetadata {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
      }
      if let v = _storage._modelToUpload {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
      }
      if _storage._state != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._state, fieldNumber: 9)
      }
      if let v = _storage._error {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 10)
      }
      if let v = _storage._createTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 11)
      }
      if let v = _storage._startTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 12)
      }
      if let v = _storage._endTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 13)
      }
      if let v = _storage._updateTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 14)
      }
      if !_storage._labels.isEmpty {
        try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: _storage._labels, fieldNumber: 15)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_TrainingPipeline, rhs: Google_Cloud_Aiplatform_V1beta1_TrainingPipeline) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._name != rhs_storage._name {return false}
        if _storage._displayName != rhs_storage._displayName {return false}
        if _storage._inputDataConfig != rhs_storage._inputDataConfig {return false}
        if _storage._trainingTaskDefinition != rhs_storage._trainingTaskDefinition {return false}
        if _storage._trainingTaskInputs != rhs_storage._trainingTaskInputs {return false}
        if _storage._trainingTaskMetadata != rhs_storage._trainingTaskMetadata {return false}
        if _storage._modelToUpload != rhs_storage._modelToUpload {return false}
        if _storage._state != rhs_storage._state {return false}
        if _storage._error != rhs_storage._error {return false}
        if _storage._createTime != rhs_storage._createTime {return false}
        if _storage._startTime != rhs_storage._startTime {return false}
        if _storage._endTime != rhs_storage._endTime {return false}
        if _storage._updateTime != rhs_storage._updateTime {return false}
        if _storage._labels != rhs_storage._labels {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1beta1_InputDataConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".InputDataConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .standard(proto: "fraction_split"),
    3: .standard(proto: "filter_split"),
    4: .standard(proto: "predefined_split"),
    5: .standard(proto: "timestamp_split"),
    8: .standard(proto: "gcs_destination"),
    10: .standard(proto: "bigquery_destination"),
    1: .standard(proto: "dataset_id"),
    6: .standard(proto: "annotations_filter"),
    9: .standard(proto: "annotation_schema_uri"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.datasetID) }()
      case 2: try {
        var v: Google_Cloud_Aiplatform_V1beta1_FractionSplit?
        if let current = self.split {
          try decoder.handleConflictingOneOf()
          if case .fractionSplit(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.split = .fractionSplit(v)}
      }()
      case 3: try {
        var v: Google_Cloud_Aiplatform_V1beta1_FilterSplit?
        if let current = self.split {
          try decoder.handleConflictingOneOf()
          if case .filterSplit(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.split = .filterSplit(v)}
      }()
      case 4: try {
        var v: Google_Cloud_Aiplatform_V1beta1_PredefinedSplit?
        if let current = self.split {
          try decoder.handleConflictingOneOf()
          if case .predefinedSplit(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.split = .predefinedSplit(v)}
      }()
      case 5: try {
        var v: Google_Cloud_Aiplatform_V1beta1_TimestampSplit?
        if let current = self.split {
          try decoder.handleConflictingOneOf()
          if case .timestampSplit(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.split = .timestampSplit(v)}
      }()
      case 6: try { try decoder.decodeSingularStringField(value: &self.annotationsFilter) }()
      case 8: try {
        var v: Google_Cloud_Aiplatform_V1beta1_GcsDestination?
        if let current = self.destination {
          try decoder.handleConflictingOneOf()
          if case .gcsDestination(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.destination = .gcsDestination(v)}
      }()
      case 9: try { try decoder.decodeSingularStringField(value: &self.annotationSchemaUri) }()
      case 10: try {
        var v: Google_Cloud_Aiplatform_V1beta1_BigQueryDestination?
        if let current = self.destination {
          try decoder.handleConflictingOneOf()
          if case .bigqueryDestination(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.destination = .bigqueryDestination(v)}
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.datasetID.isEmpty {
      try visitor.visitSingularStringField(value: self.datasetID, fieldNumber: 1)
    }
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.split {
    case .fractionSplit?: try {
      guard case .fractionSplit(let v)? = self.split else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case .filterSplit?: try {
      guard case .filterSplit(let v)? = self.split else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }()
    case .predefinedSplit?: try {
      guard case .predefinedSplit(let v)? = self.split else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }()
    case .timestampSplit?: try {
      guard case .timestampSplit(let v)? = self.split else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    }()
    case nil: break
    }
    if !self.annotationsFilter.isEmpty {
      try visitor.visitSingularStringField(value: self.annotationsFilter, fieldNumber: 6)
    }
    if case .gcsDestination(let v)? = self.destination {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
    }
    if !self.annotationSchemaUri.isEmpty {
      try visitor.visitSingularStringField(value: self.annotationSchemaUri, fieldNumber: 9)
    }
    if case .bigqueryDestination(let v)? = self.destination {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 10)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_InputDataConfig, rhs: Google_Cloud_Aiplatform_V1beta1_InputDataConfig) -> Bool {
    if lhs.split != rhs.split {return false}
    if lhs.destination != rhs.destination {return false}
    if lhs.datasetID != rhs.datasetID {return false}
    if lhs.annotationsFilter != rhs.annotationsFilter {return false}
    if lhs.annotationSchemaUri != rhs.annotationSchemaUri {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1beta1_FractionSplit: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".FractionSplit"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "training_fraction"),
    2: .standard(proto: "validation_fraction"),
    3: .standard(proto: "test_fraction"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularDoubleField(value: &self.trainingFraction) }()
      case 2: try { try decoder.decodeSingularDoubleField(value: &self.validationFraction) }()
      case 3: try { try decoder.decodeSingularDoubleField(value: &self.testFraction) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.trainingFraction != 0 {
      try visitor.visitSingularDoubleField(value: self.trainingFraction, fieldNumber: 1)
    }
    if self.validationFraction != 0 {
      try visitor.visitSingularDoubleField(value: self.validationFraction, fieldNumber: 2)
    }
    if self.testFraction != 0 {
      try visitor.visitSingularDoubleField(value: self.testFraction, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_FractionSplit, rhs: Google_Cloud_Aiplatform_V1beta1_FractionSplit) -> Bool {
    if lhs.trainingFraction != rhs.trainingFraction {return false}
    if lhs.validationFraction != rhs.validationFraction {return false}
    if lhs.testFraction != rhs.testFraction {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1beta1_FilterSplit: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".FilterSplit"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "training_filter"),
    2: .standard(proto: "validation_filter"),
    3: .standard(proto: "test_filter"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.trainingFilter) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.validationFilter) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.testFilter) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.trainingFilter.isEmpty {
      try visitor.visitSingularStringField(value: self.trainingFilter, fieldNumber: 1)
    }
    if !self.validationFilter.isEmpty {
      try visitor.visitSingularStringField(value: self.validationFilter, fieldNumber: 2)
    }
    if !self.testFilter.isEmpty {
      try visitor.visitSingularStringField(value: self.testFilter, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_FilterSplit, rhs: Google_Cloud_Aiplatform_V1beta1_FilterSplit) -> Bool {
    if lhs.trainingFilter != rhs.trainingFilter {return false}
    if lhs.validationFilter != rhs.validationFilter {return false}
    if lhs.testFilter != rhs.testFilter {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1beta1_PredefinedSplit: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".PredefinedSplit"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "key"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.key) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.key.isEmpty {
      try visitor.visitSingularStringField(value: self.key, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_PredefinedSplit, rhs: Google_Cloud_Aiplatform_V1beta1_PredefinedSplit) -> Bool {
    if lhs.key != rhs.key {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1beta1_TimestampSplit: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TimestampSplit"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "training_fraction"),
    2: .standard(proto: "validation_fraction"),
    3: .standard(proto: "test_fraction"),
    4: .same(proto: "key"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularDoubleField(value: &self.trainingFraction) }()
      case 2: try { try decoder.decodeSingularDoubleField(value: &self.validationFraction) }()
      case 3: try { try decoder.decodeSingularDoubleField(value: &self.testFraction) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.key) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.trainingFraction != 0 {
      try visitor.visitSingularDoubleField(value: self.trainingFraction, fieldNumber: 1)
    }
    if self.validationFraction != 0 {
      try visitor.visitSingularDoubleField(value: self.validationFraction, fieldNumber: 2)
    }
    if self.testFraction != 0 {
      try visitor.visitSingularDoubleField(value: self.testFraction, fieldNumber: 3)
    }
    if !self.key.isEmpty {
      try visitor.visitSingularStringField(value: self.key, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_TimestampSplit, rhs: Google_Cloud_Aiplatform_V1beta1_TimestampSplit) -> Bool {
    if lhs.trainingFraction != rhs.trainingFraction {return false}
    if lhs.validationFraction != rhs.validationFraction {return false}
    if lhs.testFraction != rhs.testFraction {return false}
    if lhs.key != rhs.key {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
