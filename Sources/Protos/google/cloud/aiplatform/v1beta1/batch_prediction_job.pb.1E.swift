// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/aiplatform/v1beta1/batch_prediction_job.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2020 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// A job that uses a [Model][google.cloud.aiplatform.v1beta1.BatchPredictionJob.model] to produce predictions
/// on multiple [input instances][google.cloud.aiplatform.v1beta1.BatchPredictionJob.input_config]. If
/// predictions for significant portion of the instances fail, the job may finish
/// without attempting predictions for all remaining instances.
public struct Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. Resource name of the BatchPredictionJob.
  public var name: String {
    get {return _storage._name}
    set {_uniqueStorage()._name = newValue}
  }

  /// Required. The user-defined name of this BatchPredictionJob.
  public var displayName: String {
    get {return _storage._displayName}
    set {_uniqueStorage()._displayName = newValue}
  }

  /// Required. The name of the Model that produces the predictions via this job,
  /// must share the same ancestor Location.
  /// Starting this job has no impact on any existing deployments of the Model
  /// and their resources.
  public var model: String {
    get {return _storage._model}
    set {_uniqueStorage()._model = newValue}
  }

  /// Required. Input configuration of the instances on which predictions are performed.
  /// The schema of any single instance may be specified via
  /// the [Model's][google.cloud.aiplatform.v1beta1.BatchPredictionJob.model]
  /// [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
  /// [instance_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri].
  public var inputConfig: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.InputConfig {
    get {return _storage._inputConfig ?? Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.InputConfig()}
    set {_uniqueStorage()._inputConfig = newValue}
  }
  /// Returns true if `inputConfig` has been explicitly set.
  public var hasInputConfig: Bool {return _storage._inputConfig != nil}
  /// Clears the value of `inputConfig`. Subsequent reads from it will return its default value.
  public mutating func clearInputConfig() {_uniqueStorage()._inputConfig = nil}

  /// The parameters that govern the predictions. The schema of the parameters
  /// may be specified via the [Model's][google.cloud.aiplatform.v1beta1.BatchPredictionJob.model]
  /// [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
  /// [parameters_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.parameters_schema_uri].
  public var modelParameters: SwiftProtobuf.Google_Protobuf_Value {
    get {return _storage._modelParameters ?? SwiftProtobuf.Google_Protobuf_Value()}
    set {_uniqueStorage()._modelParameters = newValue}
  }
  /// Returns true if `modelParameters` has been explicitly set.
  public var hasModelParameters: Bool {return _storage._modelParameters != nil}
  /// Clears the value of `modelParameters`. Subsequent reads from it will return its default value.
  public mutating func clearModelParameters() {_uniqueStorage()._modelParameters = nil}

  /// Required. The Configuration specifying where output predictions should
  /// be written.
  /// The schema of any single prediction may be specified as a concatenation
  /// of [Model's][google.cloud.aiplatform.v1beta1.BatchPredictionJob.model]
  /// [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
  /// [instance_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri]
  /// and
  /// [prediction_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.prediction_schema_uri].
  public var outputConfig: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputConfig {
    get {return _storage._outputConfig ?? Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputConfig()}
    set {_uniqueStorage()._outputConfig = newValue}
  }
  /// Returns true if `outputConfig` has been explicitly set.
  public var hasOutputConfig: Bool {return _storage._outputConfig != nil}
  /// Clears the value of `outputConfig`. Subsequent reads from it will return its default value.
  public mutating func clearOutputConfig() {_uniqueStorage()._outputConfig = nil}

  /// The config of resources used by the Model during the batch prediction. If
  /// the Model [supports][google.cloud.aiplatform.v1beta1.Model.supported_deployment_resources_types]
  /// DEDICATED_RESOURCES this config may be provided (and the job will use these
  /// resources), if the Model doesn't support AUTOMATIC_RESOURCES, this config
  /// must be provided.
  public var dedicatedResources: Google_Cloud_Aiplatform_V1beta1_BatchDedicatedResources {
    get {return _storage._dedicatedResources ?? Google_Cloud_Aiplatform_V1beta1_BatchDedicatedResources()}
    set {_uniqueStorage()._dedicatedResources = newValue}
  }
  /// Returns true if `dedicatedResources` has been explicitly set.
  public var hasDedicatedResources: Bool {return _storage._dedicatedResources != nil}
  /// Clears the value of `dedicatedResources`. Subsequent reads from it will return its default value.
  public mutating func clearDedicatedResources() {_uniqueStorage()._dedicatedResources = nil}

  /// Immutable. Parameters configuring the batch behavior. Currently only applicable when
  /// [dedicated_resources][google.cloud.aiplatform.v1beta1.BatchPredictionJob.dedicated_resources] are used (in other cases AI Platform does
  /// the tuning itself).
  public var manualBatchTuningParameters: Google_Cloud_Aiplatform_V1beta1_ManualBatchTuningParameters {
    get {return _storage._manualBatchTuningParameters ?? Google_Cloud_Aiplatform_V1beta1_ManualBatchTuningParameters()}
    set {_uniqueStorage()._manualBatchTuningParameters = newValue}
  }
  /// Returns true if `manualBatchTuningParameters` has been explicitly set.
  public var hasManualBatchTuningParameters: Bool {return _storage._manualBatchTuningParameters != nil}
  /// Clears the value of `manualBatchTuningParameters`. Subsequent reads from it will return its default value.
  public mutating func clearManualBatchTuningParameters() {_uniqueStorage()._manualBatchTuningParameters = nil}

  /// Generate explanation along with the batch prediction results.
  ///
  /// When it's true, the batch prediction output will change based on the
  /// [output format][BatchPredictionJob.output_config.predictions_format]:
  ///
  ///  * `bigquery`: output will include a column named `explanation`. The value
  ///    is a struct that conforms to the [Explanation][google.cloud.aiplatform.v1beta1.Explanation] object.
  ///  * `jsonl`: The JSON objects on each line will include an additional entry
  ///    keyed `explanation`. The value of the entry is a JSON object that
  ///    conforms to the [Explanation][google.cloud.aiplatform.v1beta1.Explanation] object.
  ///  * `csv`: Generating explanations for CSV format is not supported.
  public var generateExplanation: Bool {
    get {return _storage._generateExplanation}
    set {_uniqueStorage()._generateExplanation = newValue}
  }

  /// Explanation configuration for this BatchPredictionJob. Can only be
  /// specified if [generate_explanation][google.cloud.aiplatform.v1beta1.BatchPredictionJob.generate_explanation] is set to `true`. It's invalid to
  /// specified it with generate_explanation set to false or unset.
  ///
  /// This value overrides the value of [Model.explanation_spec][google.cloud.aiplatform.v1beta1.Model.explanation_spec]. All fields of
  /// [explanation_spec][google.cloud.aiplatform.v1beta1.BatchPredictionJob.explanation_spec] are optional in the request. If a field of
  /// [explanation_spec][google.cloud.aiplatform.v1beta1.BatchPredictionJob.explanation_spec] is not populated, the value of the same field of
  /// [Model.explanation_spec][google.cloud.aiplatform.v1beta1.Model.explanation_spec] is inherited. The corresponding
  /// [Model.explanation_spec][google.cloud.aiplatform.v1beta1.Model.explanation_spec] must be populated, otherwise explanation for
  /// this Model is not allowed.
  public var explanationSpec: Google_Cloud_Aiplatform_V1beta1_ExplanationSpec {
    get {return _storage._explanationSpec ?? Google_Cloud_Aiplatform_V1beta1_ExplanationSpec()}
    set {_uniqueStorage()._explanationSpec = newValue}
  }
  /// Returns true if `explanationSpec` has been explicitly set.
  public var hasExplanationSpec: Bool {return _storage._explanationSpec != nil}
  /// Clears the value of `explanationSpec`. Subsequent reads from it will return its default value.
  public mutating func clearExplanationSpec() {_uniqueStorage()._explanationSpec = nil}

  /// Output only. Information further describing the output of this job.
  public var outputInfo: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputInfo {
    get {return _storage._outputInfo ?? Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputInfo()}
    set {_uniqueStorage()._outputInfo = newValue}
  }
  /// Returns true if `outputInfo` has been explicitly set.
  public var hasOutputInfo: Bool {return _storage._outputInfo != nil}
  /// Clears the value of `outputInfo`. Subsequent reads from it will return its default value.
  public mutating func clearOutputInfo() {_uniqueStorage()._outputInfo = nil}

  /// Output only. The detailed state of the job.
  public var state: Google_Cloud_Aiplatform_V1beta1_JobState {
    get {return _storage._state}
    set {_uniqueStorage()._state = newValue}
  }

  /// Output only. Only populated when the job's state is JOB_STATE_FAILED or
  /// JOB_STATE_CANCELLED.
  public var error: Google_Rpc_Status {
    get {return _storage._error ?? Google_Rpc_Status()}
    set {_uniqueStorage()._error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return _storage._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {_uniqueStorage()._error = nil}

  /// Output only. Partial failures encountered.
  /// For example, single files that can't be read.
  /// This field never exceeds 20 entries.
  /// Status details fields contain standard GCP error details.
  public var partialFailures: [Google_Rpc_Status] {
    get {return _storage._partialFailures}
    set {_uniqueStorage()._partialFailures = newValue}
  }

  /// Output only. Information about resources that had been consumed by this job.
  /// Provided in real time at best effort basis, as well as a final value
  /// once the job completes.
  ///
  /// Note: This field currently may be not populated for batch predictions that
  /// use AutoML Models.
  public var resourcesConsumed: Google_Cloud_Aiplatform_V1beta1_ResourcesConsumed {
    get {return _storage._resourcesConsumed ?? Google_Cloud_Aiplatform_V1beta1_ResourcesConsumed()}
    set {_uniqueStorage()._resourcesConsumed = newValue}
  }
  /// Returns true if `resourcesConsumed` has been explicitly set.
  public var hasResourcesConsumed: Bool {return _storage._resourcesConsumed != nil}
  /// Clears the value of `resourcesConsumed`. Subsequent reads from it will return its default value.
  public mutating func clearResourcesConsumed() {_uniqueStorage()._resourcesConsumed = nil}

  /// Output only. Statistics on completed and failed prediction instances.
  public var completionStats: Google_Cloud_Aiplatform_V1beta1_CompletionStats {
    get {return _storage._completionStats ?? Google_Cloud_Aiplatform_V1beta1_CompletionStats()}
    set {_uniqueStorage()._completionStats = newValue}
  }
  /// Returns true if `completionStats` has been explicitly set.
  public var hasCompletionStats: Bool {return _storage._completionStats != nil}
  /// Clears the value of `completionStats`. Subsequent reads from it will return its default value.
  public mutating func clearCompletionStats() {_uniqueStorage()._completionStats = nil}

  /// Output only. Time when the BatchPredictionJob was created.
  public var createTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._createTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._createTime = newValue}
  }
  /// Returns true if `createTime` has been explicitly set.
  public var hasCreateTime: Bool {return _storage._createTime != nil}
  /// Clears the value of `createTime`. Subsequent reads from it will return its default value.
  public mutating func clearCreateTime() {_uniqueStorage()._createTime = nil}

  /// Output only. Time when the BatchPredictionJob for the first time entered the
  /// `JOB_STATE_RUNNING` state.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return _storage._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {_uniqueStorage()._startTime = nil}

  /// Output only. Time when the BatchPredictionJob entered any of the following states:
  /// `JOB_STATE_SUCCEEDED`, `JOB_STATE_FAILED`, `JOB_STATE_CANCELLED`.
  public var endTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._endTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._endTime = newValue}
  }
  /// Returns true if `endTime` has been explicitly set.
  public var hasEndTime: Bool {return _storage._endTime != nil}
  /// Clears the value of `endTime`. Subsequent reads from it will return its default value.
  public mutating func clearEndTime() {_uniqueStorage()._endTime = nil}

  /// Output only. Time when the BatchPredictionJob was most recently updated.
  public var updateTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._updateTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._updateTime = newValue}
  }
  /// Returns true if `updateTime` has been explicitly set.
  public var hasUpdateTime: Bool {return _storage._updateTime != nil}
  /// Clears the value of `updateTime`. Subsequent reads from it will return its default value.
  public mutating func clearUpdateTime() {_uniqueStorage()._updateTime = nil}

  /// The labels with user-defined metadata to organize BatchPredictionJobs.
  ///
  /// Label keys and values can be no longer than 64 characters
  /// (Unicode codepoints), can only contain lowercase letters, numeric
  /// characters, underscores and dashes. International characters are allowed.
  ///
  /// See https://goo.gl/xmQnxf for more information and examples of labels.
  public var labels: Dictionary<String,String> {
    get {return _storage._labels}
    set {_uniqueStorage()._labels = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Configures the input to [BatchPredictionJob][google.cloud.aiplatform.v1beta1.BatchPredictionJob].
  /// See [Model.supported_input_storage_formats][google.cloud.aiplatform.v1beta1.Model.supported_input_storage_formats] for Model's supported input
  /// formats, and how instances should be expressed via any of them.
  public struct InputConfig {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Required. The source of the input.
    public var source: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.InputConfig.OneOf_Source? = nil

    /// The Google Cloud Storage location for the input instances.
    public var gcsSource: Google_Cloud_Aiplatform_V1beta1_GcsSource {
      get {
        if case .gcsSource(let v)? = source {return v}
        return Google_Cloud_Aiplatform_V1beta1_GcsSource()
      }
      set {source = .gcsSource(newValue)}
    }

    /// The BigQuery location of the input table.
    /// The schema of the table should be in the format described by the given
    /// context OpenAPI Schema, if one is provided. The table may contain
    /// additional columns that are not described by the schema, and they will
    /// be ignored.
    public var bigquerySource: Google_Cloud_Aiplatform_V1beta1_BigQuerySource {
      get {
        if case .bigquerySource(let v)? = source {return v}
        return Google_Cloud_Aiplatform_V1beta1_BigQuerySource()
      }
      set {source = .bigquerySource(newValue)}
    }

    /// Required. The format in which instances are given, must be one of the
    /// [Model's][google.cloud.aiplatform.v1beta1.BatchPredictionJob.model]
    /// [supported_input_storage_formats][google.cloud.aiplatform.v1beta1.Model.supported_input_storage_formats].
    public var instancesFormat: String = String()

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    /// Required. The source of the input.
    public enum OneOf_Source: Equatable {
      /// The Google Cloud Storage location for the input instances.
      case gcsSource(Google_Cloud_Aiplatform_V1beta1_GcsSource)
      /// The BigQuery location of the input table.
      /// The schema of the table should be in the format described by the given
      /// context OpenAPI Schema, if one is provided. The table may contain
      /// additional columns that are not described by the schema, and they will
      /// be ignored.
      case bigquerySource(Google_Cloud_Aiplatform_V1beta1_BigQuerySource)

    #if !swift(>=4.1)
      public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.InputConfig.OneOf_Source, rhs: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.InputConfig.OneOf_Source) -> Bool {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch (lhs, rhs) {
        case (.gcsSource, .gcsSource): return {
          guard case .gcsSource(let l) = lhs, case .gcsSource(let r) = rhs else { preconditionFailure() }
          return l == r
        }()
        case (.bigquerySource, .bigquerySource): return {
          guard case .bigquerySource(let l) = lhs, case .bigquerySource(let r) = rhs else { preconditionFailure() }
          return l == r
        }()
        default: return false
        }
      }
    #endif
    }

    public init() {}
  }

  /// Configures the output of [BatchPredictionJob][google.cloud.aiplatform.v1beta1.BatchPredictionJob].
  /// See [Model.supported_output_storage_formats][google.cloud.aiplatform.v1beta1.Model.supported_output_storage_formats] for supported output
  /// formats, and how predictions are expressed via any of them.
  public struct OutputConfig {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Required. The destination of the output.
    public var destination: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputConfig.OneOf_Destination? = nil

    /// The Google Cloud Storage location of the directory where the output is
    /// to be written to. In the given directory a new directory is created.
    /// Its name is `prediction-<model-display-name>-<job-create-time>`,
    /// where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
    /// Inside of it files `predictions_0001.<extension>`,
    /// `predictions_0002.<extension>`, ..., `predictions_N.<extension>`
    /// are created where `<extension>` depends on chosen
    /// [predictions_format][google.cloud.aiplatform.v1beta1.BatchPredictionJob.OutputConfig.predictions_format], and N may equal 0001 and depends on the total
    /// number of successfully predicted instances.
    /// If the Model has both [instance][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri]
    /// and [prediction][google.cloud.aiplatform.v1beta1.PredictSchemata.parameters_schema_uri] schemata
    /// defined then each such file contains predictions as per the
    /// [predictions_format][google.cloud.aiplatform.v1beta1.BatchPredictionJob.OutputConfig.predictions_format].
    /// If prediction for any instance failed (partially or completely), then
    /// an additional `errors_0001.<extension>`, `errors_0002.<extension>`,...,
    /// `errors_N.<extension>` files are created (N depends on total number
    /// of failed predictions). These files contain the failed instances,
    /// as per their schema, followed by an additional `error` field which as
    /// value has
    /// [`google.rpc.Status`](Status)
    /// containing only `code` and `message` fields.
    public var gcsDestination: Google_Cloud_Aiplatform_V1beta1_GcsDestination {
      get {
        if case .gcsDestination(let v)? = destination {return v}
        return Google_Cloud_Aiplatform_V1beta1_GcsDestination()
      }
      set {destination = .gcsDestination(newValue)}
    }

    /// The BigQuery project location where the output is to be written to.
    /// In the given project a new dataset is created with name
    /// `prediction_<model-display-name>_<job-create-time>`
    /// where <model-display-name> is made
    /// BigQuery-dataset-name compatible (for example, most special characters
    /// become underscores), and timestamp is in
    /// YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset
    /// two tables will be created, `predictions`, and `errors`.
    /// If the Model has both [instance][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri]
    /// and [prediction][google.cloud.aiplatform.v1beta1.PredictSchemata.parameters_schema_uri] schemata
    /// defined then the tables have columns as follows: The `predictions`
    /// table contains instances for which the prediction succeeded, it
    /// has columns as per a concatenation of the Model's instance and
    /// prediction schemata. The `errors` table contains rows for which the
    /// prediction has failed, it has instance columns, as per the
    /// instance schema, followed by a single "errors" column, which as values
    /// has [`google.rpc.Status`](Status)
    /// represented as a STRUCT, and containing only `code` and `message`.
    public var bigqueryDestination: Google_Cloud_Aiplatform_V1beta1_BigQueryDestination {
      get {
        if case .bigqueryDestination(let v)? = destination {return v}
        return Google_Cloud_Aiplatform_V1beta1_BigQueryDestination()
      }
      set {destination = .bigqueryDestination(newValue)}
    }

    /// Required. The format in which AI Platform gives the predictions, must be one of the
    /// [Model's][google.cloud.aiplatform.v1beta1.BatchPredictionJob.model]
    ///
    /// [supported_output_storage_formats][google.cloud.aiplatform.v1beta1.Model.supported_output_storage_formats].
    public var predictionsFormat: String = String()

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    /// Required. The destination of the output.
    public enum OneOf_Destination: Equatable {
      /// The Google Cloud Storage location of the directory where the output is
      /// to be written to. In the given directory a new directory is created.
      /// Its name is `prediction-<model-display-name>-<job-create-time>`,
      /// where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
      /// Inside of it files `predictions_0001.<extension>`,
      /// `predictions_0002.<extension>`, ..., `predictions_N.<extension>`
      /// are created where `<extension>` depends on chosen
      /// [predictions_format][google.cloud.aiplatform.v1beta1.BatchPredictionJob.OutputConfig.predictions_format], and N may equal 0001 and depends on the total
      /// number of successfully predicted instances.
      /// If the Model has both [instance][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri]
      /// and [prediction][google.cloud.aiplatform.v1beta1.PredictSchemata.parameters_schema_uri] schemata
      /// defined then each such file contains predictions as per the
      /// [predictions_format][google.cloud.aiplatform.v1beta1.BatchPredictionJob.OutputConfig.predictions_format].
      /// If prediction for any instance failed (partially or completely), then
      /// an additional `errors_0001.<extension>`, `errors_0002.<extension>`,...,
      /// `errors_N.<extension>` files are created (N depends on total number
      /// of failed predictions). These files contain the failed instances,
      /// as per their schema, followed by an additional `error` field which as
      /// value has
      /// [`google.rpc.Status`](Status)
      /// containing only `code` and `message` fields.
      case gcsDestination(Google_Cloud_Aiplatform_V1beta1_GcsDestination)
      /// The BigQuery project location where the output is to be written to.
      /// In the given project a new dataset is created with name
      /// `prediction_<model-display-name>_<job-create-time>`
      /// where <model-display-name> is made
      /// BigQuery-dataset-name compatible (for example, most special characters
      /// become underscores), and timestamp is in
      /// YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset
      /// two tables will be created, `predictions`, and `errors`.
      /// If the Model has both [instance][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri]
      /// and [prediction][google.cloud.aiplatform.v1beta1.PredictSchemata.parameters_schema_uri] schemata
      /// defined then the tables have columns as follows: The `predictions`
      /// table contains instances for which the prediction succeeded, it
      /// has columns as per a concatenation of the Model's instance and
      /// prediction schemata. The `errors` table contains rows for which the
      /// prediction has failed, it has instance columns, as per the
      /// instance schema, followed by a single "errors" column, which as values
      /// has [`google.rpc.Status`](Status)
      /// represented as a STRUCT, and containing only `code` and `message`.
      case bigqueryDestination(Google_Cloud_Aiplatform_V1beta1_BigQueryDestination)

    #if !swift(>=4.1)
      public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputConfig.OneOf_Destination, rhs: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputConfig.OneOf_Destination) -> Bool {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch (lhs, rhs) {
        case (.gcsDestination, .gcsDestination): return {
          guard case .gcsDestination(let l) = lhs, case .gcsDestination(let r) = rhs else { preconditionFailure() }
          return l == r
        }()
        case (.bigqueryDestination, .bigqueryDestination): return {
          guard case .bigqueryDestination(let l) = lhs, case .bigqueryDestination(let r) = rhs else { preconditionFailure() }
          return l == r
        }()
        default: return false
        }
      }
    #endif
    }

    public init() {}
  }

  /// Further describes this job's output.
  /// Supplements [output_config][google.cloud.aiplatform.v1beta1.BatchPredictionJob.output_config].
  public struct OutputInfo {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// The output location into which prediction output is written.
    public var outputLocation: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputInfo.OneOf_OutputLocation? = nil

    /// Output only. The full path of the Google Cloud Storage directory created, into which
    /// the prediction output is written.
    public var gcsOutputDirectory: String {
      get {
        if case .gcsOutputDirectory(let v)? = outputLocation {return v}
        return String()
      }
      set {outputLocation = .gcsOutputDirectory(newValue)}
    }

    /// Output only. The path of the BigQuery dataset created, in
    /// `bq://projectId.bqDatasetId`
    /// format, into which the prediction output is written.
    public var bigqueryOutputDataset: String {
      get {
        if case .bigqueryOutputDataset(let v)? = outputLocation {return v}
        return String()
      }
      set {outputLocation = .bigqueryOutputDataset(newValue)}
    }

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    /// The output location into which prediction output is written.
    public enum OneOf_OutputLocation: Equatable {
      /// Output only. The full path of the Google Cloud Storage directory created, into which
      /// the prediction output is written.
      case gcsOutputDirectory(String)
      /// Output only. The path of the BigQuery dataset created, in
      /// `bq://projectId.bqDatasetId`
      /// format, into which the prediction output is written.
      case bigqueryOutputDataset(String)

    #if !swift(>=4.1)
      public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputInfo.OneOf_OutputLocation, rhs: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputInfo.OneOf_OutputLocation) -> Bool {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch (lhs, rhs) {
        case (.gcsOutputDirectory, .gcsOutputDirectory): return {
          guard case .gcsOutputDirectory(let l) = lhs, case .gcsOutputDirectory(let r) = rhs else { preconditionFailure() }
          return l == r
        }()
        case (.bigqueryOutputDataset, .bigqueryOutputDataset): return {
          guard case .bigqueryOutputDataset(let l) = lhs, case .bigqueryOutputDataset(let r) = rhs else { preconditionFailure() }
          return l == r
        }()
        default: return false
        }
      }
    #endif
    }

    public init() {}
  }

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.aiplatform.v1beta1"

extension Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".BatchPredictionJob"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .standard(proto: "display_name"),
    3: .same(proto: "model"),
    4: .standard(proto: "input_config"),
    5: .standard(proto: "model_parameters"),
    6: .standard(proto: "output_config"),
    7: .standard(proto: "dedicated_resources"),
    8: .standard(proto: "manual_batch_tuning_parameters"),
    23: .standard(proto: "generate_explanation"),
    25: .standard(proto: "explanation_spec"),
    9: .standard(proto: "output_info"),
    10: .same(proto: "state"),
    11: .same(proto: "error"),
    12: .standard(proto: "partial_failures"),
    13: .standard(proto: "resources_consumed"),
    14: .standard(proto: "completion_stats"),
    15: .standard(proto: "create_time"),
    16: .standard(proto: "start_time"),
    17: .standard(proto: "end_time"),
    18: .standard(proto: "update_time"),
    19: .same(proto: "labels"),
  ]

  fileprivate class _StorageClass {
    var _name: String = String()
    var _displayName: String = String()
    var _model: String = String()
    var _inputConfig: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.InputConfig? = nil
    var _modelParameters: SwiftProtobuf.Google_Protobuf_Value? = nil
    var _outputConfig: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputConfig? = nil
    var _dedicatedResources: Google_Cloud_Aiplatform_V1beta1_BatchDedicatedResources? = nil
    var _manualBatchTuningParameters: Google_Cloud_Aiplatform_V1beta1_ManualBatchTuningParameters? = nil
    var _generateExplanation: Bool = false
    var _explanationSpec: Google_Cloud_Aiplatform_V1beta1_ExplanationSpec? = nil
    var _outputInfo: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputInfo? = nil
    var _state: Google_Cloud_Aiplatform_V1beta1_JobState = .unspecified
    var _error: Google_Rpc_Status? = nil
    var _partialFailures: [Google_Rpc_Status] = []
    var _resourcesConsumed: Google_Cloud_Aiplatform_V1beta1_ResourcesConsumed? = nil
    var _completionStats: Google_Cloud_Aiplatform_V1beta1_CompletionStats? = nil
    var _createTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _endTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _updateTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _labels: Dictionary<String,String> = [:]

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _name = source._name
      _displayName = source._displayName
      _model = source._model
      _inputConfig = source._inputConfig
      _modelParameters = source._modelParameters
      _outputConfig = source._outputConfig
      _dedicatedResources = source._dedicatedResources
      _manualBatchTuningParameters = source._manualBatchTuningParameters
      _generateExplanation = source._generateExplanation
      _explanationSpec = source._explanationSpec
      _outputInfo = source._outputInfo
      _state = source._state
      _error = source._error
      _partialFailures = source._partialFailures
      _resourcesConsumed = source._resourcesConsumed
      _completionStats = source._completionStats
      _createTime = source._createTime
      _startTime = source._startTime
      _endTime = source._endTime
      _updateTime = source._updateTime
      _labels = source._labels
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularStringField(value: &_storage._name) }()
        case 2: try { try decoder.decodeSingularStringField(value: &_storage._displayName) }()
        case 3: try { try decoder.decodeSingularStringField(value: &_storage._model) }()
        case 4: try { try decoder.decodeSingularMessageField(value: &_storage._inputConfig) }()
        case 5: try { try decoder.decodeSingularMessageField(value: &_storage._modelParameters) }()
        case 6: try { try decoder.decodeSingularMessageField(value: &_storage._outputConfig) }()
        case 7: try { try decoder.decodeSingularMessageField(value: &_storage._dedicatedResources) }()
        case 8: try { try decoder.decodeSingularMessageField(value: &_storage._manualBatchTuningParameters) }()
        case 9: try { try decoder.decodeSingularMessageField(value: &_storage._outputInfo) }()
        case 10: try { try decoder.decodeSingularEnumField(value: &_storage._state) }()
        case 11: try { try decoder.decodeSingularMessageField(value: &_storage._error) }()
        case 12: try { try decoder.decodeRepeatedMessageField(value: &_storage._partialFailures) }()
        case 13: try { try decoder.decodeSingularMessageField(value: &_storage._resourcesConsumed) }()
        case 14: try { try decoder.decodeSingularMessageField(value: &_storage._completionStats) }()
        case 15: try { try decoder.decodeSingularMessageField(value: &_storage._createTime) }()
        case 16: try { try decoder.decodeSingularMessageField(value: &_storage._startTime) }()
        case 17: try { try decoder.decodeSingularMessageField(value: &_storage._endTime) }()
        case 18: try { try decoder.decodeSingularMessageField(value: &_storage._updateTime) }()
        case 19: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &_storage._labels) }()
        case 23: try { try decoder.decodeSingularBoolField(value: &_storage._generateExplanation) }()
        case 25: try { try decoder.decodeSingularMessageField(value: &_storage._explanationSpec) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if !_storage._name.isEmpty {
        try visitor.visitSingularStringField(value: _storage._name, fieldNumber: 1)
      }
      if !_storage._displayName.isEmpty {
        try visitor.visitSingularStringField(value: _storage._displayName, fieldNumber: 2)
      }
      if !_storage._model.isEmpty {
        try visitor.visitSingularStringField(value: _storage._model, fieldNumber: 3)
      }
      if let v = _storage._inputConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
      }
      if let v = _storage._modelParameters {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      }
      if let v = _storage._outputConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
      }
      if let v = _storage._dedicatedResources {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
      }
      if let v = _storage._manualBatchTuningParameters {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
      }
      if let v = _storage._outputInfo {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
      }
      if _storage._state != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._state, fieldNumber: 10)
      }
      if let v = _storage._error {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 11)
      }
      if !_storage._partialFailures.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._partialFailures, fieldNumber: 12)
      }
      if let v = _storage._resourcesConsumed {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 13)
      }
      if let v = _storage._completionStats {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 14)
      }
      if let v = _storage._createTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 15)
      }
      if let v = _storage._startTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 16)
      }
      if let v = _storage._endTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 17)
      }
      if let v = _storage._updateTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 18)
      }
      if !_storage._labels.isEmpty {
        try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: _storage._labels, fieldNumber: 19)
      }
      if _storage._generateExplanation != false {
        try visitor.visitSingularBoolField(value: _storage._generateExplanation, fieldNumber: 23)
      }
      if let v = _storage._explanationSpec {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 25)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob, rhs: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._name != rhs_storage._name {return false}
        if _storage._displayName != rhs_storage._displayName {return false}
        if _storage._model != rhs_storage._model {return false}
        if _storage._inputConfig != rhs_storage._inputConfig {return false}
        if _storage._modelParameters != rhs_storage._modelParameters {return false}
        if _storage._outputConfig != rhs_storage._outputConfig {return false}
        if _storage._dedicatedResources != rhs_storage._dedicatedResources {return false}
        if _storage._manualBatchTuningParameters != rhs_storage._manualBatchTuningParameters {return false}
        if _storage._generateExplanation != rhs_storage._generateExplanation {return false}
        if _storage._explanationSpec != rhs_storage._explanationSpec {return false}
        if _storage._outputInfo != rhs_storage._outputInfo {return false}
        if _storage._state != rhs_storage._state {return false}
        if _storage._error != rhs_storage._error {return false}
        if _storage._partialFailures != rhs_storage._partialFailures {return false}
        if _storage._resourcesConsumed != rhs_storage._resourcesConsumed {return false}
        if _storage._completionStats != rhs_storage._completionStats {return false}
        if _storage._createTime != rhs_storage._createTime {return false}
        if _storage._startTime != rhs_storage._startTime {return false}
        if _storage._endTime != rhs_storage._endTime {return false}
        if _storage._updateTime != rhs_storage._updateTime {return false}
        if _storage._labels != rhs_storage._labels {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.InputConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.protoMessageName + ".InputConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .standard(proto: "gcs_source"),
    3: .standard(proto: "bigquery_source"),
    1: .standard(proto: "instances_format"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.instancesFormat) }()
      case 2: try {
        var v: Google_Cloud_Aiplatform_V1beta1_GcsSource?
        if let current = self.source {
          try decoder.handleConflictingOneOf()
          if case .gcsSource(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.source = .gcsSource(v)}
      }()
      case 3: try {
        var v: Google_Cloud_Aiplatform_V1beta1_BigQuerySource?
        if let current = self.source {
          try decoder.handleConflictingOneOf()
          if case .bigquerySource(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.source = .bigquerySource(v)}
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.instancesFormat.isEmpty {
      try visitor.visitSingularStringField(value: self.instancesFormat, fieldNumber: 1)
    }
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.source {
    case .gcsSource?: try {
      guard case .gcsSource(let v)? = self.source else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case .bigquerySource?: try {
      guard case .bigquerySource(let v)? = self.source else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }()
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.InputConfig, rhs: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.InputConfig) -> Bool {
    if lhs.source != rhs.source {return false}
    if lhs.instancesFormat != rhs.instancesFormat {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.protoMessageName + ".OutputConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .standard(proto: "gcs_destination"),
    3: .standard(proto: "bigquery_destination"),
    1: .standard(proto: "predictions_format"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.predictionsFormat) }()
      case 2: try {
        var v: Google_Cloud_Aiplatform_V1beta1_GcsDestination?
        if let current = self.destination {
          try decoder.handleConflictingOneOf()
          if case .gcsDestination(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.destination = .gcsDestination(v)}
      }()
      case 3: try {
        var v: Google_Cloud_Aiplatform_V1beta1_BigQueryDestination?
        if let current = self.destination {
          try decoder.handleConflictingOneOf()
          if case .bigqueryDestination(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.destination = .bigqueryDestination(v)}
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.predictionsFormat.isEmpty {
      try visitor.visitSingularStringField(value: self.predictionsFormat, fieldNumber: 1)
    }
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.destination {
    case .gcsDestination?: try {
      guard case .gcsDestination(let v)? = self.destination else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case .bigqueryDestination?: try {
      guard case .bigqueryDestination(let v)? = self.destination else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }()
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputConfig, rhs: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputConfig) -> Bool {
    if lhs.destination != rhs.destination {return false}
    if lhs.predictionsFormat != rhs.predictionsFormat {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputInfo: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.protoMessageName + ".OutputInfo"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "gcs_output_directory"),
    2: .standard(proto: "bigquery_output_dataset"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        if self.outputLocation != nil {try decoder.handleConflictingOneOf()}
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {self.outputLocation = .gcsOutputDirectory(v)}
      }()
      case 2: try {
        if self.outputLocation != nil {try decoder.handleConflictingOneOf()}
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {self.outputLocation = .bigqueryOutputDataset(v)}
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.outputLocation {
    case .gcsOutputDirectory?: try {
      guard case .gcsOutputDirectory(let v)? = self.outputLocation else { preconditionFailure() }
      try visitor.visitSingularStringField(value: v, fieldNumber: 1)
    }()
    case .bigqueryOutputDataset?: try {
      guard case .bigqueryOutputDataset(let v)? = self.outputLocation else { preconditionFailure() }
      try visitor.visitSingularStringField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputInfo, rhs: Google_Cloud_Aiplatform_V1beta1_BatchPredictionJob.OutputInfo) -> Bool {
    if lhs.outputLocation != rhs.outputLocation {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
