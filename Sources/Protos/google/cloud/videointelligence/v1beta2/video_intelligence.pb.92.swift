// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/videointelligence/v1beta2/video_intelligence.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2019 Google LLC.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Video annotation feature.
public enum Google_Cloud_Videointelligence_V1beta2_Feature: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Unspecified.
  case unspecified // = 0

  /// Label detection. Detect objects, such as dog or flower.
  case labelDetection // = 1

  /// Shot change detection.
  case shotChangeDetection // = 2

  /// Explicit content detection.
  case explicitContentDetection // = 3

  /// Human face detection and tracking.
  case faceDetection // = 4
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .labelDetection
    case 2: self = .shotChangeDetection
    case 3: self = .explicitContentDetection
    case 4: self = .faceDetection
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .labelDetection: return 1
    case .shotChangeDetection: return 2
    case .explicitContentDetection: return 3
    case .faceDetection: return 4
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Videointelligence_V1beta2_Feature: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Videointelligence_V1beta2_Feature] = [
    .unspecified,
    .labelDetection,
    .shotChangeDetection,
    .explicitContentDetection,
    .faceDetection,
  ]
}

#endif  // swift(>=4.2)

/// Label detection mode.
public enum Google_Cloud_Videointelligence_V1beta2_LabelDetectionMode: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Unspecified.
  case unspecified // = 0

  /// Detect shot-level labels.
  case shotMode // = 1

  /// Detect frame-level labels.
  case frameMode // = 2

  /// Detect both shot-level and frame-level labels.
  case shotAndFrameMode // = 3
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .shotMode
    case 2: self = .frameMode
    case 3: self = .shotAndFrameMode
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .shotMode: return 1
    case .frameMode: return 2
    case .shotAndFrameMode: return 3
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Videointelligence_V1beta2_LabelDetectionMode: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Videointelligence_V1beta2_LabelDetectionMode] = [
    .unspecified,
    .shotMode,
    .frameMode,
    .shotAndFrameMode,
  ]
}

#endif  // swift(>=4.2)

/// Bucketized representation of likelihood.
public enum Google_Cloud_Videointelligence_V1beta2_Likelihood: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Unspecified likelihood.
  case unspecified // = 0

  /// Very unlikely.
  case veryUnlikely // = 1

  /// Unlikely.
  case unlikely // = 2

  /// Possible.
  case possible // = 3

  /// Likely.
  case likely // = 4

  /// Very likely.
  case veryLikely // = 5
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .veryUnlikely
    case 2: self = .unlikely
    case 3: self = .possible
    case 4: self = .likely
    case 5: self = .veryLikely
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .veryUnlikely: return 1
    case .unlikely: return 2
    case .possible: return 3
    case .likely: return 4
    case .veryLikely: return 5
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Videointelligence_V1beta2_Likelihood: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Videointelligence_V1beta2_Likelihood] = [
    .unspecified,
    .veryUnlikely,
    .unlikely,
    .possible,
    .likely,
    .veryLikely,
  ]
}

#endif  // swift(>=4.2)

/// Video annotation request.
public struct Google_Cloud_Videointelligence_V1beta2_AnnotateVideoRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Input video location. Currently, only
  /// [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
  /// supported, which must be specified in the following format:
  /// `gs://bucket-id/object-id` (other URI formats return
  /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
  /// more information, see [Request URIs](https://cloud.google.com/storage/docs/request-endpoints). A video
  /// URI may include wildcards in `object-id`, and thus identify multiple
  /// videos. Supported wildcards: '*' to match 0 or more characters;
  /// '?' to match 1 character. If unset, the input video should be embedded
  /// in the request as `input_content`. If set, `input_content` should be unset.
  public var inputUri: String = String()

  /// The video data bytes.
  /// If unset, the input video(s) should be specified via `input_uri`.
  /// If set, `input_uri` should be unset.
  public var inputContent: Data = Data()

  /// Required. Requested video annotation features.
  public var features: [Google_Cloud_Videointelligence_V1beta2_Feature] = []

  /// Additional video context and/or feature-specific parameters.
  public var videoContext: Google_Cloud_Videointelligence_V1beta2_VideoContext {
    get {return _videoContext ?? Google_Cloud_Videointelligence_V1beta2_VideoContext()}
    set {_videoContext = newValue}
  }
  /// Returns true if `videoContext` has been explicitly set.
  public var hasVideoContext: Bool {return self._videoContext != nil}
  /// Clears the value of `videoContext`. Subsequent reads from it will return its default value.
  public mutating func clearVideoContext() {self._videoContext = nil}

  /// Optional. Location where the output (in JSON format) should be stored.
  /// Currently, only [Google Cloud Storage](https://cloud.google.com/storage/)
  /// URIs are supported, which must be specified in the following format:
  /// `gs://bucket-id/object-id` (other URI formats return
  /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
  /// more information, see [Request URIs](https://cloud.google.com/storage/docs/request-endpoints).
  public var outputUri: String = String()

  /// Optional. Cloud region where annotation should take place. Supported cloud
  /// regions: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no region
  /// is specified, a region will be determined based on video file location.
  public var locationID: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _videoContext: Google_Cloud_Videointelligence_V1beta2_VideoContext? = nil
}

/// Video context and/or feature-specific parameters.
public struct Google_Cloud_Videointelligence_V1beta2_VideoContext {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video segments to annotate. The segments may overlap and are not required
  /// to be contiguous or span the whole video. If unspecified, each video is
  /// treated as a single segment.
  public var segments: [Google_Cloud_Videointelligence_V1beta2_VideoSegment] = []

  /// Config for LABEL_DETECTION.
  public var labelDetectionConfig: Google_Cloud_Videointelligence_V1beta2_LabelDetectionConfig {
    get {return _labelDetectionConfig ?? Google_Cloud_Videointelligence_V1beta2_LabelDetectionConfig()}
    set {_labelDetectionConfig = newValue}
  }
  /// Returns true if `labelDetectionConfig` has been explicitly set.
  public var hasLabelDetectionConfig: Bool {return self._labelDetectionConfig != nil}
  /// Clears the value of `labelDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearLabelDetectionConfig() {self._labelDetectionConfig = nil}

  /// Config for SHOT_CHANGE_DETECTION.
  public var shotChangeDetectionConfig: Google_Cloud_Videointelligence_V1beta2_ShotChangeDetectionConfig {
    get {return _shotChangeDetectionConfig ?? Google_Cloud_Videointelligence_V1beta2_ShotChangeDetectionConfig()}
    set {_shotChangeDetectionConfig = newValue}
  }
  /// Returns true if `shotChangeDetectionConfig` has been explicitly set.
  public var hasShotChangeDetectionConfig: Bool {return self._shotChangeDetectionConfig != nil}
  /// Clears the value of `shotChangeDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearShotChangeDetectionConfig() {self._shotChangeDetectionConfig = nil}

  /// Config for EXPLICIT_CONTENT_DETECTION.
  public var explicitContentDetectionConfig: Google_Cloud_Videointelligence_V1beta2_ExplicitContentDetectionConfig {
    get {return _explicitContentDetectionConfig ?? Google_Cloud_Videointelligence_V1beta2_ExplicitContentDetectionConfig()}
    set {_explicitContentDetectionConfig = newValue}
  }
  /// Returns true if `explicitContentDetectionConfig` has been explicitly set.
  public var hasExplicitContentDetectionConfig: Bool {return self._explicitContentDetectionConfig != nil}
  /// Clears the value of `explicitContentDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearExplicitContentDetectionConfig() {self._explicitContentDetectionConfig = nil}

  /// Config for FACE_DETECTION.
  public var faceDetectionConfig: Google_Cloud_Videointelligence_V1beta2_FaceDetectionConfig {
    get {return _faceDetectionConfig ?? Google_Cloud_Videointelligence_V1beta2_FaceDetectionConfig()}
    set {_faceDetectionConfig = newValue}
  }
  /// Returns true if `faceDetectionConfig` has been explicitly set.
  public var hasFaceDetectionConfig: Bool {return self._faceDetectionConfig != nil}
  /// Clears the value of `faceDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearFaceDetectionConfig() {self._faceDetectionConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _labelDetectionConfig: Google_Cloud_Videointelligence_V1beta2_LabelDetectionConfig? = nil
  fileprivate var _shotChangeDetectionConfig: Google_Cloud_Videointelligence_V1beta2_ShotChangeDetectionConfig? = nil
  fileprivate var _explicitContentDetectionConfig: Google_Cloud_Videointelligence_V1beta2_ExplicitContentDetectionConfig? = nil
  fileprivate var _faceDetectionConfig: Google_Cloud_Videointelligence_V1beta2_FaceDetectionConfig? = nil
}

/// Config for LABEL_DETECTION.
public struct Google_Cloud_Videointelligence_V1beta2_LabelDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// What labels should be detected with LABEL_DETECTION, in addition to
  /// video-level labels or segment-level labels.
  /// If unspecified, defaults to `SHOT_MODE`.
  public var labelDetectionMode: Google_Cloud_Videointelligence_V1beta2_LabelDetectionMode = .unspecified

  /// Whether the video has been shot from a stationary (i.e. non-moving) camera.
  /// When set to true, might improve detection accuracy for moving objects.
  /// Should be used with `SHOT_AND_FRAME_MODE` enabled.
  public var stationaryCamera: Bool = false

  /// Model to use for label detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for SHOT_CHANGE_DETECTION.
public struct Google_Cloud_Videointelligence_V1beta2_ShotChangeDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Model to use for shot change detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for EXPLICIT_CONTENT_DETECTION.
public struct Google_Cloud_Videointelligence_V1beta2_ExplicitContentDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Model to use for explicit content detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for FACE_DETECTION.
public struct Google_Cloud_Videointelligence_V1beta2_FaceDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Model to use for face detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  /// Whether bounding boxes be included in the face annotation output.
  public var includeBoundingBoxes: Bool = false

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Video segment.
public struct Google_Cloud_Videointelligence_V1beta2_VideoSegment {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Time-offset, relative to the beginning of the video,
  /// corresponding to the start of the segment (inclusive).
  public var startTimeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _startTimeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_startTimeOffset = newValue}
  }
  /// Returns true if `startTimeOffset` has been explicitly set.
  public var hasStartTimeOffset: Bool {return self._startTimeOffset != nil}
  /// Clears the value of `startTimeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearStartTimeOffset() {self._startTimeOffset = nil}

  /// Time-offset, relative to the beginning of the video,
  /// corresponding to the end of the segment (inclusive).
  public var endTimeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _endTimeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_endTimeOffset = newValue}
  }
  /// Returns true if `endTimeOffset` has been explicitly set.
  public var hasEndTimeOffset: Bool {return self._endTimeOffset != nil}
  /// Clears the value of `endTimeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearEndTimeOffset() {self._endTimeOffset = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _startTimeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
  fileprivate var _endTimeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Video segment level annotation results for label detection.
public struct Google_Cloud_Videointelligence_V1beta2_LabelSegment {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video segment where a label was detected.
  public var segment: Google_Cloud_Videointelligence_V1beta2_VideoSegment {
    get {return _segment ?? Google_Cloud_Videointelligence_V1beta2_VideoSegment()}
    set {_segment = newValue}
  }
  /// Returns true if `segment` has been explicitly set.
  public var hasSegment: Bool {return self._segment != nil}
  /// Clears the value of `segment`. Subsequent reads from it will return its default value.
  public mutating func clearSegment() {self._segment = nil}

  /// Confidence that the label is accurate. Range: [0, 1].
  public var confidence: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _segment: Google_Cloud_Videointelligence_V1beta2_VideoSegment? = nil
}

/// Video frame level annotation results for label detection.
public struct Google_Cloud_Videointelligence_V1beta2_LabelFrame {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Time-offset, relative to the beginning of the video, corresponding to the
  /// video frame for this location.
  public var timeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _timeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_timeOffset = newValue}
  }
  /// Returns true if `timeOffset` has been explicitly set.
  public var hasTimeOffset: Bool {return self._timeOffset != nil}
  /// Clears the value of `timeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearTimeOffset() {self._timeOffset = nil}

  /// Confidence that the label is accurate. Range: [0, 1].
  public var confidence: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _timeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Detected entity from video analysis.
public struct Google_Cloud_Videointelligence_V1beta2_Entity {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Opaque entity ID. Some IDs may be available in
  /// [Google Knowledge Graph Search
  /// API](https://developers.google.com/knowledge-graph/).
  public var entityID: String = String()

  /// Textual description, e.g. `Fixed-gear bicycle`.
  public var description_p: String = String()

  /// Language code for `description` in BCP-47 format.
  public var languageCode: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Label annotation.
public struct Google_Cloud_Videointelligence_V1beta2_LabelAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Detected entity.
  public var entity: Google_Cloud_Videointelligence_V1beta2_Entity {
    get {return _entity ?? Google_Cloud_Videointelligence_V1beta2_Entity()}
    set {_entity = newValue}
  }
  /// Returns true if `entity` has been explicitly set.
  public var hasEntity: Bool {return self._entity != nil}
  /// Clears the value of `entity`. Subsequent reads from it will return its default value.
  public mutating func clearEntity() {self._entity = nil}

  /// Common categories for the detected entity.
  /// E.g. when the label is `Terrier` the category is likely `dog`. And in some
  /// cases there might be more than one categories e.g. `Terrier` could also be
  /// a `pet`.
  public var categoryEntities: [Google_Cloud_Videointelligence_V1beta2_Entity] = []

  /// All video segments where a label was detected.
  public var segments: [Google_Cloud_Videointelligence_V1beta2_LabelSegment] = []

  /// All video frames where a label was detected.
  public var frames: [Google_Cloud_Videointelligence_V1beta2_LabelFrame] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _entity: Google_Cloud_Videointelligence_V1beta2_Entity? = nil
}

/// Video frame level annotation results for explicit content.
public struct Google_Cloud_Videointelligence_V1beta2_ExplicitContentFrame {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Time-offset, relative to the beginning of the video, corresponding to the
  /// video frame for this location.
  public var timeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _timeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_timeOffset = newValue}
  }
  /// Returns true if `timeOffset` has been explicitly set.
  public var hasTimeOffset: Bool {return self._timeOffset != nil}
  /// Clears the value of `timeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearTimeOffset() {self._timeOffset = nil}

  /// Likelihood of the pornography content..
  public var pornographyLikelihood: Google_Cloud_Videointelligence_V1beta2_Likelihood = .unspecified

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _timeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Explicit content annotation (based on per-frame visual signals only).
/// If no explicit content has been detected in a frame, no annotations are
/// present for that frame.
public struct Google_Cloud_Videointelligence_V1beta2_ExplicitContentAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// All video frames where explicit content was detected.
  public var frames: [Google_Cloud_Videointelligence_V1beta2_ExplicitContentFrame] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Normalized bounding box.
/// The normalized vertex coordinates are relative to the original image.
/// Range: [0, 1].
public struct Google_Cloud_Videointelligence_V1beta2_NormalizedBoundingBox {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Left X coordinate.
  public var left: Float = 0

  /// Top Y coordinate.
  public var top: Float = 0

  /// Right X coordinate.
  public var right: Float = 0

  /// Bottom Y coordinate.
  public var bottom: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Video segment level annotation results for face detection.
public struct Google_Cloud_Videointelligence_V1beta2_FaceSegment {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video segment where a face was detected.
  public var segment: Google_Cloud_Videointelligence_V1beta2_VideoSegment {
    get {return _segment ?? Google_Cloud_Videointelligence_V1beta2_VideoSegment()}
    set {_segment = newValue}
  }
  /// Returns true if `segment` has been explicitly set.
  public var hasSegment: Bool {return self._segment != nil}
  /// Clears the value of `segment`. Subsequent reads from it will return its default value.
  public mutating func clearSegment() {self._segment = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _segment: Google_Cloud_Videointelligence_V1beta2_VideoSegment? = nil
}

/// Video frame level annotation results for face detection.
public struct Google_Cloud_Videointelligence_V1beta2_FaceFrame {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Normalized Bounding boxes in a frame.
  /// There can be more than one boxes if the same face is detected in multiple
  /// locations within the current frame.
  public var normalizedBoundingBoxes: [Google_Cloud_Videointelligence_V1beta2_NormalizedBoundingBox] = []

  /// Time-offset, relative to the beginning of the video,
  /// corresponding to the video frame for this location.
  public var timeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _timeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_timeOffset = newValue}
  }
  /// Returns true if `timeOffset` has been explicitly set.
  public var hasTimeOffset: Bool {return self._timeOffset != nil}
  /// Clears the value of `timeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearTimeOffset() {self._timeOffset = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _timeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Face annotation.
public struct Google_Cloud_Videointelligence_V1beta2_FaceAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Thumbnail of a representative face view (in JPEG format).
  public var thumbnail: Data = Data()

  /// All video segments where a face was detected.
  public var segments: [Google_Cloud_Videointelligence_V1beta2_FaceSegment] = []

  /// All video frames where a face was detected.
  public var frames: [Google_Cloud_Videointelligence_V1beta2_FaceFrame] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Annotation results for a single video.
public struct Google_Cloud_Videointelligence_V1beta2_VideoAnnotationResults {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video file location in
  /// [Google Cloud Storage](https://cloud.google.com/storage/).
  public var inputUri: String = String()

  /// Label annotations on video level or user specified segment level.
  /// There is exactly one element for each unique label.
  public var segmentLabelAnnotations: [Google_Cloud_Videointelligence_V1beta2_LabelAnnotation] = []

  /// Label annotations on shot level.
  /// There is exactly one element for each unique label.
  public var shotLabelAnnotations: [Google_Cloud_Videointelligence_V1beta2_LabelAnnotation] = []

  /// Label annotations on frame level.
  /// There is exactly one element for each unique label.
  public var frameLabelAnnotations: [Google_Cloud_Videointelligence_V1beta2_LabelAnnotation] = []

  /// Face annotations. There is exactly one element for each unique face.
  public var faceAnnotations: [Google_Cloud_Videointelligence_V1beta2_FaceAnnotation] = []

  /// Shot annotations. Each shot is represented as a video segment.
  public var shotAnnotations: [Google_Cloud_Videointelligence_V1beta2_VideoSegment] = []

  /// Explicit content annotation.
  public var explicitAnnotation: Google_Cloud_Videointelligence_V1beta2_ExplicitContentAnnotation {
    get {return _explicitAnnotation ?? Google_Cloud_Videointelligence_V1beta2_ExplicitContentAnnotation()}
    set {_explicitAnnotation = newValue}
  }
  /// Returns true if `explicitAnnotation` has been explicitly set.
  public var hasExplicitAnnotation: Bool {return self._explicitAnnotation != nil}
  /// Clears the value of `explicitAnnotation`. Subsequent reads from it will return its default value.
  public mutating func clearExplicitAnnotation() {self._explicitAnnotation = nil}

  /// If set, indicates an error. Note that for a single `AnnotateVideoRequest`
  /// some videos may succeed and some may fail.
  public var error: Google_Rpc_Status {
    get {return _error ?? Google_Rpc_Status()}
    set {_error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return self._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {self._error = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _explicitAnnotation: Google_Cloud_Videointelligence_V1beta2_ExplicitContentAnnotation? = nil
  fileprivate var _error: Google_Rpc_Status? = nil
}

/// Video annotation response. Included in the `response`
/// field of the `Operation` returned by the `GetOperation`
/// call of the `google::longrunning::Operations` service.
public struct Google_Cloud_Videointelligence_V1beta2_AnnotateVideoResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Annotation results for all videos specified in `AnnotateVideoRequest`.
  public var annotationResults: [Google_Cloud_Videointelligence_V1beta2_VideoAnnotationResults] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Annotation progress for a single video.
public struct Google_Cloud_Videointelligence_V1beta2_VideoAnnotationProgress {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video file location in
  /// [Google Cloud Storage](https://cloud.google.com/storage/).
  public var inputUri: String = String()

  /// Approximate percentage processed thus far.
  /// Guaranteed to be 100 when fully processed.
  public var progressPercent: Int32 = 0

  /// Time when the request was received.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return self._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {self._startTime = nil}

  /// Time of the most recent update.
  public var updateTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _updateTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_updateTime = newValue}
  }
  /// Returns true if `updateTime` has been explicitly set.
  public var hasUpdateTime: Bool {return self._updateTime != nil}
  /// Clears the value of `updateTime`. Subsequent reads from it will return its default value.
  public mutating func clearUpdateTime() {self._updateTime = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
  fileprivate var _updateTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
}

/// Video annotation progress. Included in the `metadata`
/// field of the `Operation` returned by the `GetOperation`
/// call of the `google::longrunning::Operations` service.
public struct Google_Cloud_Videointelligence_V1beta2_AnnotateVideoProgress {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Progress metadata for all videos specified in `AnnotateVideoRequest`.
  public var annotationProgress: [Google_Cloud_Videointelligence_V1beta2_VideoAnnotationProgress] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.videointelligence.v1beta2"

extension Google_Cloud_Videointelligence_V1beta2_Feature: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "FEATURE_UNSPECIFIED"),
    1: .same(proto: "LABEL_DETECTION"),
    2: .same(proto: "SHOT_CHANGE_DETECTION"),
    3: .same(proto: "EXPLICIT_CONTENT_DETECTION"),
    4: .same(proto: "FACE_DETECTION"),
  ]
}

extension Google_Cloud_Videointelligence_V1beta2_LabelDetectionMode: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "LABEL_DETECTION_MODE_UNSPECIFIED"),
    1: .same(proto: "SHOT_MODE"),
    2: .same(proto: "FRAME_MODE"),
    3: .same(proto: "SHOT_AND_FRAME_MODE"),
  ]
}

extension Google_Cloud_Videointelligence_V1beta2_Likelihood: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "LIKELIHOOD_UNSPECIFIED"),
    1: .same(proto: "VERY_UNLIKELY"),
    2: .same(proto: "UNLIKELY"),
    3: .same(proto: "POSSIBLE"),
    4: .same(proto: "LIKELY"),
    5: .same(proto: "VERY_LIKELY"),
  ]
}

extension Google_Cloud_Videointelligence_V1beta2_AnnotateVideoRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AnnotateVideoRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "input_uri"),
    6: .standard(proto: "input_content"),
    2: .same(proto: "features"),
    3: .standard(proto: "video_context"),
    4: .standard(proto: "output_uri"),
    5: .standard(proto: "location_id"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.inputUri) }()
      case 2: try { try decoder.decodeRepeatedEnumField(value: &self.features) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._videoContext) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.outputUri) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.locationID) }()
      case 6: try { try decoder.decodeSingularBytesField(value: &self.inputContent) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.inputUri.isEmpty {
      try visitor.visitSingularStringField(value: self.inputUri, fieldNumber: 1)
    }
    if !self.features.isEmpty {
      try visitor.visitPackedEnumField(value: self.features, fieldNumber: 2)
    }
    if let v = self._videoContext {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if !self.outputUri.isEmpty {
      try visitor.visitSingularStringField(value: self.outputUri, fieldNumber: 4)
    }
    if !self.locationID.isEmpty {
      try visitor.visitSingularStringField(value: self.locationID, fieldNumber: 5)
    }
    if !self.inputContent.isEmpty {
      try visitor.visitSingularBytesField(value: self.inputContent, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_AnnotateVideoRequest, rhs: Google_Cloud_Videointelligence_V1beta2_AnnotateVideoRequest) -> Bool {
    if lhs.inputUri != rhs.inputUri {return false}
    if lhs.inputContent != rhs.inputContent {return false}
    if lhs.features != rhs.features {return false}
    if lhs._videoContext != rhs._videoContext {return false}
    if lhs.outputUri != rhs.outputUri {return false}
    if lhs.locationID != rhs.locationID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_VideoContext: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoContext"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "segments"),
    2: .standard(proto: "label_detection_config"),
    3: .standard(proto: "shot_change_detection_config"),
    4: .standard(proto: "explicit_content_detection_config"),
    5: .standard(proto: "face_detection_config"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.segments) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._labelDetectionConfig) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._shotChangeDetectionConfig) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._explicitContentDetectionConfig) }()
      case 5: try { try decoder.decodeSingularMessageField(value: &self._faceDetectionConfig) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.segments.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.segments, fieldNumber: 1)
    }
    if let v = self._labelDetectionConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if let v = self._shotChangeDetectionConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if let v = self._explicitContentDetectionConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    if let v = self._faceDetectionConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_VideoContext, rhs: Google_Cloud_Videointelligence_V1beta2_VideoContext) -> Bool {
    if lhs.segments != rhs.segments {return false}
    if lhs._labelDetectionConfig != rhs._labelDetectionConfig {return false}
    if lhs._shotChangeDetectionConfig != rhs._shotChangeDetectionConfig {return false}
    if lhs._explicitContentDetectionConfig != rhs._explicitContentDetectionConfig {return false}
    if lhs._faceDetectionConfig != rhs._faceDetectionConfig {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_LabelDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "label_detection_mode"),
    2: .standard(proto: "stationary_camera"),
    3: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.labelDetectionMode) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.stationaryCamera) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.model) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.labelDetectionMode != .unspecified {
      try visitor.visitSingularEnumField(value: self.labelDetectionMode, fieldNumber: 1)
    }
    if self.stationaryCamera != false {
      try visitor.visitSingularBoolField(value: self.stationaryCamera, fieldNumber: 2)
    }
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_LabelDetectionConfig, rhs: Google_Cloud_Videointelligence_V1beta2_LabelDetectionConfig) -> Bool {
    if lhs.labelDetectionMode != rhs.labelDetectionMode {return false}
    if lhs.stationaryCamera != rhs.stationaryCamera {return false}
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_ShotChangeDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ShotChangeDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.model) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_ShotChangeDetectionConfig, rhs: Google_Cloud_Videointelligence_V1beta2_ShotChangeDetectionConfig) -> Bool {
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_ExplicitContentDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ExplicitContentDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.model) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_ExplicitContentDetectionConfig, rhs: Google_Cloud_Videointelligence_V1beta2_ExplicitContentDetectionConfig) -> Bool {
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_FaceDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".FaceDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "model"),
    2: .standard(proto: "include_bounding_boxes"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.model) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.includeBoundingBoxes) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 1)
    }
    if self.includeBoundingBoxes != false {
      try visitor.visitSingularBoolField(value: self.includeBoundingBoxes, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_FaceDetectionConfig, rhs: Google_Cloud_Videointelligence_V1beta2_FaceDetectionConfig) -> Bool {
    if lhs.model != rhs.model {return false}
    if lhs.includeBoundingBoxes != rhs.includeBoundingBoxes {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_VideoSegment: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoSegment"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "start_time_offset"),
    2: .standard(proto: "end_time_offset"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._startTimeOffset) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._endTimeOffset) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._startTimeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._endTimeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_VideoSegment, rhs: Google_Cloud_Videointelligence_V1beta2_VideoSegment) -> Bool {
    if lhs._startTimeOffset != rhs._startTimeOffset {return false}
    if lhs._endTimeOffset != rhs._endTimeOffset {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_LabelSegment: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelSegment"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "segment"),
    2: .same(proto: "confidence"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._segment) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._segment {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_LabelSegment, rhs: Google_Cloud_Videointelligence_V1beta2_LabelSegment) -> Bool {
    if lhs._segment != rhs._segment {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_LabelFrame: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelFrame"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "time_offset"),
    2: .same(proto: "confidence"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._timeOffset) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._timeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_LabelFrame, rhs: Google_Cloud_Videointelligence_V1beta2_LabelFrame) -> Bool {
    if lhs._timeOffset != rhs._timeOffset {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_Entity: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".Entity"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "entity_id"),
    2: .same(proto: "description"),
    3: .standard(proto: "language_code"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.entityID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.description_p) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.languageCode) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.entityID.isEmpty {
      try visitor.visitSingularStringField(value: self.entityID, fieldNumber: 1)
    }
    if !self.description_p.isEmpty {
      try visitor.visitSingularStringField(value: self.description_p, fieldNumber: 2)
    }
    if !self.languageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.languageCode, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_Entity, rhs: Google_Cloud_Videointelligence_V1beta2_Entity) -> Bool {
    if lhs.entityID != rhs.entityID {return false}
    if lhs.description_p != rhs.description_p {return false}
    if lhs.languageCode != rhs.languageCode {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_LabelAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "entity"),
    2: .standard(proto: "category_entities"),
    3: .same(proto: "segments"),
    4: .same(proto: "frames"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._entity) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.categoryEntities) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.segments) }()
      case 4: try { try decoder.decodeRepeatedMessageField(value: &self.frames) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._entity {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if !self.categoryEntities.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.categoryEntities, fieldNumber: 2)
    }
    if !self.segments.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.segments, fieldNumber: 3)
    }
    if !self.frames.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frames, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_LabelAnnotation, rhs: Google_Cloud_Videointelligence_V1beta2_LabelAnnotation) -> Bool {
    if lhs._entity != rhs._entity {return false}
    if lhs.categoryEntities != rhs.categoryEntities {return false}
    if lhs.segments != rhs.segments {return false}
    if lhs.frames != rhs.frames {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_ExplicitContentFrame: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ExplicitContentFrame"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "time_offset"),
    2: .standard(proto: "pornography_likelihood"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._timeOffset) }()
      case 2: try { try decoder.decodeSingularEnumField(value: &self.pornographyLikelihood) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._timeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.pornographyLikelihood != .unspecified {
      try visitor.visitSingularEnumField(value: self.pornographyLikelihood, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_ExplicitContentFrame, rhs: Google_Cloud_Videointelligence_V1beta2_ExplicitContentFrame) -> Bool {
    if lhs._timeOffset != rhs._timeOffset {return false}
    if lhs.pornographyLikelihood != rhs.pornographyLikelihood {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_ExplicitContentAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ExplicitContentAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "frames"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.frames) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.frames.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frames, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_ExplicitContentAnnotation, rhs: Google_Cloud_Videointelligence_V1beta2_ExplicitContentAnnotation) -> Bool {
    if lhs.frames != rhs.frames {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_NormalizedBoundingBox: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".NormalizedBoundingBox"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "left"),
    2: .same(proto: "top"),
    3: .same(proto: "right"),
    4: .same(proto: "bottom"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularFloatField(value: &self.left) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.top) }()
      case 3: try { try decoder.decodeSingularFloatField(value: &self.right) }()
      case 4: try { try decoder.decodeSingularFloatField(value: &self.bottom) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.left != 0 {
      try visitor.visitSingularFloatField(value: self.left, fieldNumber: 1)
    }
    if self.top != 0 {
      try visitor.visitSingularFloatField(value: self.top, fieldNumber: 2)
    }
    if self.right != 0 {
      try visitor.visitSingularFloatField(value: self.right, fieldNumber: 3)
    }
    if self.bottom != 0 {
      try visitor.visitSingularFloatField(value: self.bottom, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_NormalizedBoundingBox, rhs: Google_Cloud_Videointelligence_V1beta2_NormalizedBoundingBox) -> Bool {
    if lhs.left != rhs.left {return false}
    if lhs.top != rhs.top {return false}
    if lhs.right != rhs.right {return false}
    if lhs.bottom != rhs.bottom {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_FaceSegment: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".FaceSegment"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "segment"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._segment) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._segment {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_FaceSegment, rhs: Google_Cloud_Videointelligence_V1beta2_FaceSegment) -> Bool {
    if lhs._segment != rhs._segment {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_FaceFrame: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".FaceFrame"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "normalized_bounding_boxes"),
    2: .standard(proto: "time_offset"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.normalizedBoundingBoxes) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._timeOffset) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.normalizedBoundingBoxes.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.normalizedBoundingBoxes, fieldNumber: 1)
    }
    if let v = self._timeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_FaceFrame, rhs: Google_Cloud_Videointelligence_V1beta2_FaceFrame) -> Bool {
    if lhs.normalizedBoundingBoxes != rhs.normalizedBoundingBoxes {return false}
    if lhs._timeOffset != rhs._timeOffset {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_FaceAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".FaceAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "thumbnail"),
    2: .same(proto: "segments"),
    3: .same(proto: "frames"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularBytesField(value: &self.thumbnail) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.segments) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.frames) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.thumbnail.isEmpty {
      try visitor.visitSingularBytesField(value: self.thumbnail, fieldNumber: 1)
    }
    if !self.segments.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.segments, fieldNumber: 2)
    }
    if !self.frames.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frames, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_FaceAnnotation, rhs: Google_Cloud_Videointelligence_V1beta2_FaceAnnotation) -> Bool {
    if lhs.thumbnail != rhs.thumbnail {return false}
    if lhs.segments != rhs.segments {return false}
    if lhs.frames != rhs.frames {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_VideoAnnotationResults: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoAnnotationResults"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "input_uri"),
    2: .standard(proto: "segment_label_annotations"),
    3: .standard(proto: "shot_label_annotations"),
    4: .standard(proto: "frame_label_annotations"),
    5: .standard(proto: "face_annotations"),
    6: .standard(proto: "shot_annotations"),
    7: .standard(proto: "explicit_annotation"),
    9: .same(proto: "error"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.inputUri) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.segmentLabelAnnotations) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.shotLabelAnnotations) }()
      case 4: try { try decoder.decodeRepeatedMessageField(value: &self.frameLabelAnnotations) }()
      case 5: try { try decoder.decodeRepeatedMessageField(value: &self.faceAnnotations) }()
      case 6: try { try decoder.decodeRepeatedMessageField(value: &self.shotAnnotations) }()
      case 7: try { try decoder.decodeSingularMessageField(value: &self._explicitAnnotation) }()
      case 9: try { try decoder.decodeSingularMessageField(value: &self._error) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.inputUri.isEmpty {
      try visitor.visitSingularStringField(value: self.inputUri, fieldNumber: 1)
    }
    if !self.segmentLabelAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.segmentLabelAnnotations, fieldNumber: 2)
    }
    if !self.shotLabelAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.shotLabelAnnotations, fieldNumber: 3)
    }
    if !self.frameLabelAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frameLabelAnnotations, fieldNumber: 4)
    }
    if !self.faceAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.faceAnnotations, fieldNumber: 5)
    }
    if !self.shotAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.shotAnnotations, fieldNumber: 6)
    }
    if let v = self._explicitAnnotation {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
    }
    if let v = self._error {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_VideoAnnotationResults, rhs: Google_Cloud_Videointelligence_V1beta2_VideoAnnotationResults) -> Bool {
    if lhs.inputUri != rhs.inputUri {return false}
    if lhs.segmentLabelAnnotations != rhs.segmentLabelAnnotations {return false}
    if lhs.shotLabelAnnotations != rhs.shotLabelAnnotations {return false}
    if lhs.frameLabelAnnotations != rhs.frameLabelAnnotations {return false}
    if lhs.faceAnnotations != rhs.faceAnnotations {return false}
    if lhs.shotAnnotations != rhs.shotAnnotations {return false}
    if lhs._explicitAnnotation != rhs._explicitAnnotation {return false}
    if lhs._error != rhs._error {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_AnnotateVideoResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AnnotateVideoResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "annotation_results"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.annotationResults) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.annotationResults.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.annotationResults, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_AnnotateVideoResponse, rhs: Google_Cloud_Videointelligence_V1beta2_AnnotateVideoResponse) -> Bool {
    if lhs.annotationResults != rhs.annotationResults {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_VideoAnnotationProgress: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoAnnotationProgress"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "input_uri"),
    2: .standard(proto: "progress_percent"),
    3: .standard(proto: "start_time"),
    4: .standard(proto: "update_time"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.inputUri) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.progressPercent) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._startTime) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._updateTime) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.inputUri.isEmpty {
      try visitor.visitSingularStringField(value: self.inputUri, fieldNumber: 1)
    }
    if self.progressPercent != 0 {
      try visitor.visitSingularInt32Field(value: self.progressPercent, fieldNumber: 2)
    }
    if let v = self._startTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if let v = self._updateTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_VideoAnnotationProgress, rhs: Google_Cloud_Videointelligence_V1beta2_VideoAnnotationProgress) -> Bool {
    if lhs.inputUri != rhs.inputUri {return false}
    if lhs.progressPercent != rhs.progressPercent {return false}
    if lhs._startTime != rhs._startTime {return false}
    if lhs._updateTime != rhs._updateTime {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1beta2_AnnotateVideoProgress: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AnnotateVideoProgress"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "annotation_progress"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.annotationProgress) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.annotationProgress.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.annotationProgress, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1beta2_AnnotateVideoProgress, rhs: Google_Cloud_Videointelligence_V1beta2_AnnotateVideoProgress) -> Bool {
    if lhs.annotationProgress != rhs.annotationProgress {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
