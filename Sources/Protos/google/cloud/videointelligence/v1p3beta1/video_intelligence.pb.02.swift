// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/videointelligence/v1p3beta1/video_intelligence.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2020 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Label detection mode.
public enum Google_Cloud_Videointelligence_V1p3beta1_LabelDetectionMode: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Unspecified.
  case unspecified // = 0

  /// Detect shot-level labels.
  case shotMode // = 1

  /// Detect frame-level labels.
  case frameMode // = 2

  /// Detect both shot-level and frame-level labels.
  case shotAndFrameMode // = 3
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .shotMode
    case 2: self = .frameMode
    case 3: self = .shotAndFrameMode
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .shotMode: return 1
    case .frameMode: return 2
    case .shotAndFrameMode: return 3
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Videointelligence_V1p3beta1_LabelDetectionMode: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Videointelligence_V1p3beta1_LabelDetectionMode] = [
    .unspecified,
    .shotMode,
    .frameMode,
    .shotAndFrameMode,
  ]
}

#endif  // swift(>=4.2)

/// Bucketized representation of likelihood.
public enum Google_Cloud_Videointelligence_V1p3beta1_Likelihood: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Unspecified likelihood.
  case unspecified // = 0

  /// Very unlikely.
  case veryUnlikely // = 1

  /// Unlikely.
  case unlikely // = 2

  /// Possible.
  case possible // = 3

  /// Likely.
  case likely // = 4

  /// Very likely.
  case veryLikely // = 5
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .veryUnlikely
    case 2: self = .unlikely
    case 3: self = .possible
    case 4: self = .likely
    case 5: self = .veryLikely
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .veryUnlikely: return 1
    case .unlikely: return 2
    case .possible: return 3
    case .likely: return 4
    case .veryLikely: return 5
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Videointelligence_V1p3beta1_Likelihood: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Videointelligence_V1p3beta1_Likelihood] = [
    .unspecified,
    .veryUnlikely,
    .unlikely,
    .possible,
    .likely,
    .veryLikely,
  ]
}

#endif  // swift(>=4.2)

/// Streaming video annotation feature.
public enum Google_Cloud_Videointelligence_V1p3beta1_StreamingFeature: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Unspecified.
  case unspecified // = 0

  /// Label detection. Detect objects, such as dog or flower.
  case streamingLabelDetection // = 1

  /// Shot change detection.
  case streamingShotChangeDetection // = 2

  /// Explicit content detection.
  case streamingExplicitContentDetection // = 3

  /// Object detection and tracking.
  case streamingObjectTracking // = 4

  /// Action recognition based on AutoML model.
  case streamingAutomlActionRecognition // = 23

  /// Video classification based on AutoML model.
  case streamingAutomlClassification // = 21

  /// Object detection and tracking based on AutoML model.
  case streamingAutomlObjectTracking // = 22
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .streamingLabelDetection
    case 2: self = .streamingShotChangeDetection
    case 3: self = .streamingExplicitContentDetection
    case 4: self = .streamingObjectTracking
    case 21: self = .streamingAutomlClassification
    case 22: self = .streamingAutomlObjectTracking
    case 23: self = .streamingAutomlActionRecognition
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .streamingLabelDetection: return 1
    case .streamingShotChangeDetection: return 2
    case .streamingExplicitContentDetection: return 3
    case .streamingObjectTracking: return 4
    case .streamingAutomlClassification: return 21
    case .streamingAutomlObjectTracking: return 22
    case .streamingAutomlActionRecognition: return 23
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Videointelligence_V1p3beta1_StreamingFeature: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Videointelligence_V1p3beta1_StreamingFeature] = [
    .unspecified,
    .streamingLabelDetection,
    .streamingShotChangeDetection,
    .streamingExplicitContentDetection,
    .streamingObjectTracking,
    .streamingAutomlActionRecognition,
    .streamingAutomlClassification,
    .streamingAutomlObjectTracking,
  ]
}

#endif  // swift(>=4.2)

/// Video annotation feature.
public enum Google_Cloud_Videointelligence_V1p3beta1_Feature: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Unspecified.
  case unspecified // = 0

  /// Label detection. Detect objects, such as dog or flower.
  case labelDetection // = 1

  /// Shot change detection.
  case shotChangeDetection // = 2

  /// Explicit content detection.
  case explicitContentDetection // = 3

  /// Human face detection.
  case faceDetection // = 4

  /// Speech transcription.
  case speechTranscription // = 6

  /// OCR text detection and tracking.
  case textDetection // = 7

  /// Object detection and tracking.
  case objectTracking // = 9

  /// Logo detection, tracking, and recognition.
  case logoRecognition // = 12

  /// Celebrity recognition.
  case celebrityRecognition // = 13

  /// Person detection.
  case personDetection // = 14
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .labelDetection
    case 2: self = .shotChangeDetection
    case 3: self = .explicitContentDetection
    case 4: self = .faceDetection
    case 6: self = .speechTranscription
    case 7: self = .textDetection
    case 9: self = .objectTracking
    case 12: self = .logoRecognition
    case 13: self = .celebrityRecognition
    case 14: self = .personDetection
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .labelDetection: return 1
    case .shotChangeDetection: return 2
    case .explicitContentDetection: return 3
    case .faceDetection: return 4
    case .speechTranscription: return 6
    case .textDetection: return 7
    case .objectTracking: return 9
    case .logoRecognition: return 12
    case .celebrityRecognition: return 13
    case .personDetection: return 14
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Videointelligence_V1p3beta1_Feature: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Videointelligence_V1p3beta1_Feature] = [
    .unspecified,
    .labelDetection,
    .shotChangeDetection,
    .explicitContentDetection,
    .faceDetection,
    .speechTranscription,
    .textDetection,
    .objectTracking,
    .logoRecognition,
    .celebrityRecognition,
    .personDetection,
  ]
}

#endif  // swift(>=4.2)

/// Video annotation request.
public struct Google_Cloud_Videointelligence_V1p3beta1_AnnotateVideoRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Input video location. Currently, only
  /// [Cloud Storage](https://cloud.google.com/storage/) URIs are
  /// supported. URIs must be specified in the following format:
  /// `gs://bucket-id/object-id` (other URI formats return
  /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
  /// more information, see [Request
  /// URIs](https://cloud.google.com/storage/docs/request-endpoints). To identify
  /// multiple videos, a video URI may include wildcards in the `object-id`.
  /// Supported wildcards: '*' to match 0 or more characters;
  /// '?' to match 1 character. If unset, the input video should be embedded
  /// in the request as `input_content`. If set, `input_content` must be unset.
  public var inputUri: String = String()

  /// The video data bytes.
  /// If unset, the input video(s) should be specified via the `input_uri`.
  /// If set, `input_uri` must be unset.
  public var inputContent: Data = Data()

  /// Required. Requested video annotation features.
  public var features: [Google_Cloud_Videointelligence_V1p3beta1_Feature] = []

  /// Additional video context and/or feature-specific parameters.
  public var videoContext: Google_Cloud_Videointelligence_V1p3beta1_VideoContext {
    get {return _videoContext ?? Google_Cloud_Videointelligence_V1p3beta1_VideoContext()}
    set {_videoContext = newValue}
  }
  /// Returns true if `videoContext` has been explicitly set.
  public var hasVideoContext: Bool {return self._videoContext != nil}
  /// Clears the value of `videoContext`. Subsequent reads from it will return its default value.
  public mutating func clearVideoContext() {self._videoContext = nil}

  /// Optional. Location where the output (in JSON format) should be stored.
  /// Currently, only [Cloud Storage](https://cloud.google.com/storage/)
  /// URIs are supported. These must be specified in the following format:
  /// `gs://bucket-id/object-id` (other URI formats return
  /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
  /// more information, see [Request
  /// URIs](https://cloud.google.com/storage/docs/request-endpoints).
  public var outputUri: String = String()

  /// Optional. Cloud region where annotation should take place. Supported cloud
  /// regions are: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no
  /// region is specified, the region will be determined based on video file
  /// location.
  public var locationID: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _videoContext: Google_Cloud_Videointelligence_V1p3beta1_VideoContext? = nil
}

/// Video context and/or feature-specific parameters.
public struct Google_Cloud_Videointelligence_V1p3beta1_VideoContext {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video segments to annotate. The segments may overlap and are not required
  /// to be contiguous or span the whole video. If unspecified, each video is
  /// treated as a single segment.
  public var segments: [Google_Cloud_Videointelligence_V1p3beta1_VideoSegment] {
    get {return _storage._segments}
    set {_uniqueStorage()._segments = newValue}
  }

  /// Config for LABEL_DETECTION.
  public var labelDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_LabelDetectionConfig {
    get {return _storage._labelDetectionConfig ?? Google_Cloud_Videointelligence_V1p3beta1_LabelDetectionConfig()}
    set {_uniqueStorage()._labelDetectionConfig = newValue}
  }
  /// Returns true if `labelDetectionConfig` has been explicitly set.
  public var hasLabelDetectionConfig: Bool {return _storage._labelDetectionConfig != nil}
  /// Clears the value of `labelDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearLabelDetectionConfig() {_uniqueStorage()._labelDetectionConfig = nil}

  /// Config for SHOT_CHANGE_DETECTION.
  public var shotChangeDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_ShotChangeDetectionConfig {
    get {return _storage._shotChangeDetectionConfig ?? Google_Cloud_Videointelligence_V1p3beta1_ShotChangeDetectionConfig()}
    set {_uniqueStorage()._shotChangeDetectionConfig = newValue}
  }
  /// Returns true if `shotChangeDetectionConfig` has been explicitly set.
  public var hasShotChangeDetectionConfig: Bool {return _storage._shotChangeDetectionConfig != nil}
  /// Clears the value of `shotChangeDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearShotChangeDetectionConfig() {_uniqueStorage()._shotChangeDetectionConfig = nil}

  /// Config for EXPLICIT_CONTENT_DETECTION.
  public var explicitContentDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentDetectionConfig {
    get {return _storage._explicitContentDetectionConfig ?? Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentDetectionConfig()}
    set {_uniqueStorage()._explicitContentDetectionConfig = newValue}
  }
  /// Returns true if `explicitContentDetectionConfig` has been explicitly set.
  public var hasExplicitContentDetectionConfig: Bool {return _storage._explicitContentDetectionConfig != nil}
  /// Clears the value of `explicitContentDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearExplicitContentDetectionConfig() {_uniqueStorage()._explicitContentDetectionConfig = nil}

  /// Config for FACE_DETECTION.
  public var faceDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_FaceDetectionConfig {
    get {return _storage._faceDetectionConfig ?? Google_Cloud_Videointelligence_V1p3beta1_FaceDetectionConfig()}
    set {_uniqueStorage()._faceDetectionConfig = newValue}
  }
  /// Returns true if `faceDetectionConfig` has been explicitly set.
  public var hasFaceDetectionConfig: Bool {return _storage._faceDetectionConfig != nil}
  /// Clears the value of `faceDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearFaceDetectionConfig() {_uniqueStorage()._faceDetectionConfig = nil}

  /// Config for SPEECH_TRANSCRIPTION.
  public var speechTranscriptionConfig: Google_Cloud_Videointelligence_V1p3beta1_SpeechTranscriptionConfig {
    get {return _storage._speechTranscriptionConfig ?? Google_Cloud_Videointelligence_V1p3beta1_SpeechTranscriptionConfig()}
    set {_uniqueStorage()._speechTranscriptionConfig = newValue}
  }
  /// Returns true if `speechTranscriptionConfig` has been explicitly set.
  public var hasSpeechTranscriptionConfig: Bool {return _storage._speechTranscriptionConfig != nil}
  /// Clears the value of `speechTranscriptionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearSpeechTranscriptionConfig() {_uniqueStorage()._speechTranscriptionConfig = nil}

  /// Config for TEXT_DETECTION.
  public var textDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_TextDetectionConfig {
    get {return _storage._textDetectionConfig ?? Google_Cloud_Videointelligence_V1p3beta1_TextDetectionConfig()}
    set {_uniqueStorage()._textDetectionConfig = newValue}
  }
  /// Returns true if `textDetectionConfig` has been explicitly set.
  public var hasTextDetectionConfig: Bool {return _storage._textDetectionConfig != nil}
  /// Clears the value of `textDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearTextDetectionConfig() {_uniqueStorage()._textDetectionConfig = nil}

  /// Config for PERSON_DETECTION.
  public var personDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_PersonDetectionConfig {
    get {return _storage._personDetectionConfig ?? Google_Cloud_Videointelligence_V1p3beta1_PersonDetectionConfig()}
    set {_uniqueStorage()._personDetectionConfig = newValue}
  }
  /// Returns true if `personDetectionConfig` has been explicitly set.
  public var hasPersonDetectionConfig: Bool {return _storage._personDetectionConfig != nil}
  /// Clears the value of `personDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearPersonDetectionConfig() {_uniqueStorage()._personDetectionConfig = nil}

  /// Config for OBJECT_TRACKING.
  public var objectTrackingConfig: Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingConfig {
    get {return _storage._objectTrackingConfig ?? Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingConfig()}
    set {_uniqueStorage()._objectTrackingConfig = newValue}
  }
  /// Returns true if `objectTrackingConfig` has been explicitly set.
  public var hasObjectTrackingConfig: Bool {return _storage._objectTrackingConfig != nil}
  /// Clears the value of `objectTrackingConfig`. Subsequent reads from it will return its default value.
  public mutating func clearObjectTrackingConfig() {_uniqueStorage()._objectTrackingConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Config for LABEL_DETECTION.
public struct Google_Cloud_Videointelligence_V1p3beta1_LabelDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// What labels should be detected with LABEL_DETECTION, in addition to
  /// video-level labels or segment-level labels.
  /// If unspecified, defaults to `SHOT_MODE`.
  public var labelDetectionMode: Google_Cloud_Videointelligence_V1p3beta1_LabelDetectionMode = .unspecified

  /// Whether the video has been shot from a stationary (i.e., non-moving)
  /// camera. When set to true, might improve detection accuracy for moving
  /// objects. Should be used with `SHOT_AND_FRAME_MODE` enabled.
  public var stationaryCamera: Bool = false

  /// Model to use for label detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  /// The confidence threshold we perform filtering on the labels from
  /// frame-level detection. If not set, it is set to 0.4 by default. The valid
  /// range for this threshold is [0.1, 0.9]. Any value set outside of this
  /// range will be clipped.
  /// Note: For best results, follow the default threshold. We will update
  /// the default threshold everytime when we release a new model.
  public var frameConfidenceThreshold: Float = 0

  /// The confidence threshold we perform filtering on the labels from
  /// video-level and shot-level detections. If not set, it's set to 0.3 by
  /// default. The valid range for this threshold is [0.1, 0.9]. Any value set
  /// outside of this range will be clipped.
  /// Note: For best results, follow the default threshold. We will update
  /// the default threshold everytime when we release a new model.
  public var videoConfidenceThreshold: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for SHOT_CHANGE_DETECTION.
public struct Google_Cloud_Videointelligence_V1p3beta1_ShotChangeDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Model to use for shot change detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for OBJECT_TRACKING.
public struct Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Model to use for object tracking.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for EXPLICIT_CONTENT_DETECTION.
public struct Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Model to use for explicit content detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for FACE_DETECTION.
public struct Google_Cloud_Videointelligence_V1p3beta1_FaceDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Model to use for face detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  /// Whether bounding boxes are included in the face annotation output.
  public var includeBoundingBoxes: Bool = false

  /// Whether to enable face attributes detection, such as glasses, dark_glasses,
  /// mouth_open etc. Ignored if 'include_bounding_boxes' is set to false.
  public var includeAttributes: Bool = false

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for PERSON_DETECTION.
public struct Google_Cloud_Videointelligence_V1p3beta1_PersonDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Whether bounding boxes are included in the person detection annotation
  /// output.
  public var includeBoundingBoxes: Bool = false

  /// Whether to enable pose landmarks detection. Ignored if
  /// 'include_bounding_boxes' is set to false.
  public var includePoseLandmarks: Bool = false

  /// Whether to enable person attributes detection, such as cloth color (black,
  /// blue, etc), type (coat, dress, etc), pattern (plain, floral, etc), hair,
  /// etc.
  /// Ignored if 'include_bounding_boxes' is set to false.
  public var includeAttributes: Bool = false

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for TEXT_DETECTION.
public struct Google_Cloud_Videointelligence_V1p3beta1_TextDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Language hint can be specified if the language to be detected is known a
  /// priori. It can increase the accuracy of the detection. Language hint must
  /// be language code in BCP-47 format.
  ///
  /// Automatic language detection is performed if no hint is provided.
  public var languageHints: [String] = []

  /// Model to use for text detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Video segment.
public struct Google_Cloud_Videointelligence_V1p3beta1_VideoSegment {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Time-offset, relative to the beginning of the video,
  /// corresponding to the start of the segment (inclusive).
  public var startTimeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _startTimeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_startTimeOffset = newValue}
  }
  /// Returns true if `startTimeOffset` has been explicitly set.
  public var hasStartTimeOffset: Bool {return self._startTimeOffset != nil}
  /// Clears the value of `startTimeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearStartTimeOffset() {self._startTimeOffset = nil}

  /// Time-offset, relative to the beginning of the video,
  /// corresponding to the end of the segment (inclusive).
  public var endTimeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _endTimeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_endTimeOffset = newValue}
  }
  /// Returns true if `endTimeOffset` has been explicitly set.
  public var hasEndTimeOffset: Bool {return self._endTimeOffset != nil}
  /// Clears the value of `endTimeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearEndTimeOffset() {self._endTimeOffset = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _startTimeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
  fileprivate var _endTimeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Video segment level annotation results for label detection.
public struct Google_Cloud_Videointelligence_V1p3beta1_LabelSegment {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video segment where a label was detected.
  public var segment: Google_Cloud_Videointelligence_V1p3beta1_VideoSegment {
    get {return _segment ?? Google_Cloud_Videointelligence_V1p3beta1_VideoSegment()}
    set {_segment = newValue}
  }
  /// Returns true if `segment` has been explicitly set.
  public var hasSegment: Bool {return self._segment != nil}
  /// Clears the value of `segment`. Subsequent reads from it will return its default value.
  public mutating func clearSegment() {self._segment = nil}

  /// Confidence that the label is accurate. Range: [0, 1].
  public var confidence: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _segment: Google_Cloud_Videointelligence_V1p3beta1_VideoSegment? = nil
}

/// Video frame level annotation results for label detection.
public struct Google_Cloud_Videointelligence_V1p3beta1_LabelFrame {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Time-offset, relative to the beginning of the video, corresponding to the
  /// video frame for this location.
  public var timeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _timeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_timeOffset = newValue}
  }
  /// Returns true if `timeOffset` has been explicitly set.
  public var hasTimeOffset: Bool {return self._timeOffset != nil}
  /// Clears the value of `timeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearTimeOffset() {self._timeOffset = nil}

  /// Confidence that the label is accurate. Range: [0, 1].
  public var confidence: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _timeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Detected entity from video analysis.
public struct Google_Cloud_Videointelligence_V1p3beta1_Entity {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Opaque entity ID. Some IDs may be available in
  /// [Google Knowledge Graph Search
  /// API](https://developers.google.com/knowledge-graph/).
  public var entityID: String = String()

  /// Textual description, e.g., `Fixed-gear bicycle`.
  public var description_p: String = String()

  /// Language code for `description` in BCP-47 format.
  public var languageCode: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Label annotation.
public struct Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Detected entity.
  public var entity: Google_Cloud_Videointelligence_V1p3beta1_Entity {
    get {return _entity ?? Google_Cloud_Videointelligence_V1p3beta1_Entity()}
    set {_entity = newValue}
  }
  /// Returns true if `entity` has been explicitly set.
  public var hasEntity: Bool {return self._entity != nil}
  /// Clears the value of `entity`. Subsequent reads from it will return its default value.
  public mutating func clearEntity() {self._entity = nil}

  /// Common categories for the detected entity.
  /// For example, when the label is `Terrier`, the category is likely `dog`. And
  /// in some cases there might be more than one categories e.g., `Terrier` could
  /// also be a `pet`.
  public var categoryEntities: [Google_Cloud_Videointelligence_V1p3beta1_Entity] = []

  /// All video segments where a label was detected.
  public var segments: [Google_Cloud_Videointelligence_V1p3beta1_LabelSegment] = []

  /// All video frames where a label was detected.
  public var frames: [Google_Cloud_Videointelligence_V1p3beta1_LabelFrame] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _entity: Google_Cloud_Videointelligence_V1p3beta1_Entity? = nil
}

/// Video frame level annotation results for explicit content.
public struct Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentFrame {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Time-offset, relative to the beginning of the video, corresponding to the
  /// video frame for this location.
  public var timeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _timeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_timeOffset = newValue}
  }
  /// Returns true if `timeOffset` has been explicitly set.
  public var hasTimeOffset: Bool {return self._timeOffset != nil}
  /// Clears the value of `timeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearTimeOffset() {self._timeOffset = nil}

  /// Likelihood of the pornography content..
  public var pornographyLikelihood: Google_Cloud_Videointelligence_V1p3beta1_Likelihood = .unspecified

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _timeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Explicit content annotation (based on per-frame visual signals only).
/// If no explicit content has been detected in a frame, no annotations are
/// present for that frame.
public struct Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// All video frames where explicit content was detected.
  public var frames: [Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentFrame] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Normalized bounding box.
/// The normalized vertex coordinates are relative to the original image.
/// Range: [0, 1].
public struct Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingBox {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Left X coordinate.
  public var left: Float = 0

  /// Top Y coordinate.
  public var top: Float = 0

  /// Right X coordinate.
  public var right: Float = 0

  /// Bottom Y coordinate.
  public var bottom: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// For tracking related features.
/// An object at time_offset with attributes, and located with
/// normalized_bounding_box.
public struct Google_Cloud_Videointelligence_V1p3beta1_TimestampedObject {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Normalized Bounding box in a frame, where the object is located.
  public var normalizedBoundingBox: Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingBox {
    get {return _normalizedBoundingBox ?? Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingBox()}
    set {_normalizedBoundingBox = newValue}
  }
  /// Returns true if `normalizedBoundingBox` has been explicitly set.
  public var hasNormalizedBoundingBox: Bool {return self._normalizedBoundingBox != nil}
  /// Clears the value of `normalizedBoundingBox`. Subsequent reads from it will return its default value.
  public mutating func clearNormalizedBoundingBox() {self._normalizedBoundingBox = nil}

  /// Time-offset, relative to the beginning of the video,
  /// corresponding to the video frame for this object.
  public var timeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _timeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_timeOffset = newValue}
  }
  /// Returns true if `timeOffset` has been explicitly set.
  public var hasTimeOffset: Bool {return self._timeOffset != nil}
  /// Clears the value of `timeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearTimeOffset() {self._timeOffset = nil}

  /// Optional. The attributes of the object in the bounding box.
  public var attributes: [Google_Cloud_Videointelligence_V1p3beta1_DetectedAttribute] = []

  /// Optional. The detected landmarks.
  public var landmarks: [Google_Cloud_Videointelligence_V1p3beta1_DetectedLandmark] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _normalizedBoundingBox: Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingBox? = nil
  fileprivate var _timeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// A track of an object instance.
public struct Google_Cloud_Videointelligence_V1p3beta1_Track {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video segment of a track.
  public var segment: Google_Cloud_Videointelligence_V1p3beta1_VideoSegment {
    get {return _segment ?? Google_Cloud_Videointelligence_V1p3beta1_VideoSegment()}
    set {_segment = newValue}
  }
  /// Returns true if `segment` has been explicitly set.
  public var hasSegment: Bool {return self._segment != nil}
  /// Clears the value of `segment`. Subsequent reads from it will return its default value.
  public mutating func clearSegment() {self._segment = nil}

  /// The object with timestamp and attributes per frame in the track.
  public var timestampedObjects: [Google_Cloud_Videointelligence_V1p3beta1_TimestampedObject] = []

  /// Optional. Attributes in the track level.
  public var attributes: [Google_Cloud_Videointelligence_V1p3beta1_DetectedAttribute] = []

  /// Optional. The confidence score of the tracked object.
  public var confidence: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _segment: Google_Cloud_Videointelligence_V1p3beta1_VideoSegment? = nil
}

/// A generic detected attribute represented by name in string format.
public struct Google_Cloud_Videointelligence_V1p3beta1_DetectedAttribute {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The name of the attribute, for example, glasses, dark_glasses, mouth_open.
  /// A full list of supported type names will be provided in the document.
  public var name: String = String()

  /// Detected attribute confidence. Range [0, 1].
  public var confidence: Float = 0

  /// Text value of the detection result. For example, the value for "HairColor"
  /// can be "black", "blonde", etc.
  public var value: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Celebrity definition.
public struct Google_Cloud_Videointelligence_V1p3beta1_Celebrity {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The resource name of the celebrity. Have the format
  /// `video-intelligence/kg-mid` indicates a celebrity from preloaded gallery.
  /// kg-mid is the id in Google knowledge graph, which is unique for the
  /// celebrity.
  public var name: String = String()

  /// The celebrity name.
  public var displayName: String = String()

  /// Textual description of additional information about the celebrity, if
  /// applicable.
  public var description_p: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// The annotation result of a celebrity face track. RecognizedCelebrity field
/// could be empty if the face track does not have any matched celebrities.
public struct Google_Cloud_Videointelligence_V1p3beta1_CelebrityTrack {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Top N match of the celebrities for the face in this track.
  public var celebrities: [Google_Cloud_Videointelligence_V1p3beta1_CelebrityTrack.RecognizedCelebrity] = []

  /// A track of a person's face.
  public var faceTrack: Google_Cloud_Videointelligence_V1p3beta1_Track {
    get {return _faceTrack ?? Google_Cloud_Videointelligence_V1p3beta1_Track()}
    set {_faceTrack = newValue}
  }
  /// Returns true if `faceTrack` has been explicitly set.
  public var hasFaceTrack: Bool {return self._faceTrack != nil}
  /// Clears the value of `faceTrack`. Subsequent reads from it will return its default value.
  public mutating func clearFaceTrack() {self._faceTrack = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The recognized celebrity with confidence score.
  public struct RecognizedCelebrity {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// The recognized celebrity.
    public var celebrity: Google_Cloud_Videointelligence_V1p3beta1_Celebrity {
      get {return _celebrity ?? Google_Cloud_Videointelligence_V1p3beta1_Celebrity()}
      set {_celebrity = newValue}
    }
    /// Returns true if `celebrity` has been explicitly set.
    public var hasCelebrity: Bool {return self._celebrity != nil}
    /// Clears the value of `celebrity`. Subsequent reads from it will return its default value.
    public mutating func clearCelebrity() {self._celebrity = nil}

    /// Recognition confidence. Range [0, 1].
    public var confidence: Float = 0

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    public init() {}

    fileprivate var _celebrity: Google_Cloud_Videointelligence_V1p3beta1_Celebrity? = nil
  }

  public init() {}

  fileprivate var _faceTrack: Google_Cloud_Videointelligence_V1p3beta1_Track? = nil
}

/// Celebrity recognition annotation per video.
public struct Google_Cloud_Videointelligence_V1p3beta1_CelebrityRecognitionAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The tracks detected from the input video, including recognized celebrities
  /// and other detected faces in the video.
  public var celebrityTracks: [Google_Cloud_Videointelligence_V1p3beta1_CelebrityTrack] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A generic detected landmark represented by name in string format and a 2D
/// location.
public struct Google_Cloud_Videointelligence_V1p3beta1_DetectedLandmark {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The name of this landmark, for example, left_hand, right_shoulder.
  public var name: String = String()

  /// The 2D point of the detected landmark using the normalized image
  /// coordindate system. The normalized coordinates have the range from 0 to 1.
  public var point: Google_Cloud_Videointelligence_V1p3beta1_NormalizedVertex {
    get {return _point ?? Google_Cloud_Videointelligence_V1p3beta1_NormalizedVertex()}
    set {_point = newValue}
  }
  /// Returns true if `point` has been explicitly set.
  public var hasPoint: Bool {return self._point != nil}
  /// Clears the value of `point`. Subsequent reads from it will return its default value.
  public mutating func clearPoint() {self._point = nil}

  /// The confidence score of the detected landmark. Range [0, 1].
  public var confidence: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _point: Google_Cloud_Videointelligence_V1p3beta1_NormalizedVertex? = nil
}

/// Face detection annotation.
public struct Google_Cloud_Videointelligence_V1p3beta1_FaceDetectionAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The face tracks with attributes.
  public var tracks: [Google_Cloud_Videointelligence_V1p3beta1_Track] = []

  /// The thumbnail of a person's face.
  public var thumbnail: Data = Data()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Person detection annotation per video.
public struct Google_Cloud_Videointelligence_V1p3beta1_PersonDetectionAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The detected tracks of a person.
  public var tracks: [Google_Cloud_Videointelligence_V1p3beta1_Track] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Annotation results for a single video.
public struct Google_Cloud_Videointelligence_V1p3beta1_VideoAnnotationResults {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video file location in
  /// [Cloud Storage](https://cloud.google.com/storage/).
  public var inputUri: String {
    get {return _storage._inputUri}
    set {_uniqueStorage()._inputUri = newValue}
  }

  /// Video segment on which the annotation is run.
  public var segment: Google_Cloud_Videointelligence_V1p3beta1_VideoSegment {
    get {return _storage._segment ?? Google_Cloud_Videointelligence_V1p3beta1_VideoSegment()}
    set {_uniqueStorage()._segment = newValue}
  }
  /// Returns true if `segment` has been explicitly set.
  public var hasSegment: Bool {return _storage._segment != nil}
  /// Clears the value of `segment`. Subsequent reads from it will return its default value.
  public mutating func clearSegment() {_uniqueStorage()._segment = nil}

  /// Topical label annotations on video level or user-specified segment level.
  /// There is exactly one element for each unique label.
  public var segmentLabelAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation] {
    get {return _storage._segmentLabelAnnotations}
    set {_uniqueStorage()._segmentLabelAnnotations = newValue}
  }

  /// Presence label annotations on video level or user-specified segment level.
  /// There is exactly one element for each unique label. Compared to the
  /// existing topical `segment_label_annotations`, this field presents more
  /// fine-grained, segment-level labels detected in video content and is made
  /// available only when the client sets `LabelDetectionConfig.model` to
  /// "builtin/latest" in the request.
  public var segmentPresenceLabelAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation] {
    get {return _storage._segmentPresenceLabelAnnotations}
    set {_uniqueStorage()._segmentPresenceLabelAnnotations = newValue}
  }

  /// Topical label annotations on shot level.
  /// There is exactly one element for each unique label.
  public var shotLabelAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation] {
    get {return _storage._shotLabelAnnotations}
    set {_uniqueStorage()._shotLabelAnnotations = newValue}
  }

  /// Presence label annotations on shot level. There is exactly one element for
  /// each unique label. Compared to the existing topical
  /// `shot_label_annotations`, this field presents more fine-grained, shot-level
  /// labels detected in video content and is made available only when the client
  /// sets `LabelDetectionConfig.model` to "builtin/latest" in the request.
  public var shotPresenceLabelAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation] {
    get {return _storage._shotPresenceLabelAnnotations}
    set {_uniqueStorage()._shotPresenceLabelAnnotations = newValue}
  }

  /// Label annotations on frame level.
  /// There is exactly one element for each unique label.
  public var frameLabelAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation] {
    get {return _storage._frameLabelAnnotations}
    set {_uniqueStorage()._frameLabelAnnotations = newValue}
  }

  /// Face detection annotations.
  public var faceDetectionAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_FaceDetectionAnnotation] {
    get {return _storage._faceDetectionAnnotations}
    set {_uniqueStorage()._faceDetectionAnnotations = newValue}
  }

  /// Shot annotations. Each shot is represented as a video segment.
  public var shotAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_VideoSegment] {
    get {return _storage._shotAnnotations}
    set {_uniqueStorage()._shotAnnotations = newValue}
  }

  /// Explicit content annotation.
  public var explicitAnnotation: Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentAnnotation {
    get {return _storage._explicitAnnotation ?? Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentAnnotation()}
    set {_uniqueStorage()._explicitAnnotation = newValue}
  }
  /// Returns true if `explicitAnnotation` has been explicitly set.
  public var hasExplicitAnnotation: Bool {return _storage._explicitAnnotation != nil}
  /// Clears the value of `explicitAnnotation`. Subsequent reads from it will return its default value.
  public mutating func clearExplicitAnnotation() {_uniqueStorage()._explicitAnnotation = nil}

  /// Speech transcription.
  public var speechTranscriptions: [Google_Cloud_Videointelligence_V1p3beta1_SpeechTranscription] {
    get {return _storage._speechTranscriptions}
    set {_uniqueStorage()._speechTranscriptions = newValue}
  }

  /// OCR text detection and tracking.
  /// Annotations for list of detected text snippets. Each will have list of
  /// frame information associated with it.
  public var textAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_TextAnnotation] {
    get {return _storage._textAnnotations}
    set {_uniqueStorage()._textAnnotations = newValue}
  }

  /// Annotations for list of objects detected and tracked in video.
  public var objectAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingAnnotation] {
    get {return _storage._objectAnnotations}
    set {_uniqueStorage()._objectAnnotations = newValue}
  }

  /// Annotations for list of logos detected, tracked and recognized in video.
  public var logoRecognitionAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_LogoRecognitionAnnotation] {
    get {return _storage._logoRecognitionAnnotations}
    set {_uniqueStorage()._logoRecognitionAnnotations = newValue}
  }

  /// Person detection annotations.
  public var personDetectionAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_PersonDetectionAnnotation] {
    get {return _storage._personDetectionAnnotations}
    set {_uniqueStorage()._personDetectionAnnotations = newValue}
  }

  /// Celebrity recognition annotations.
  public var celebrityRecognitionAnnotations: Google_Cloud_Videointelligence_V1p3beta1_CelebrityRecognitionAnnotation {
    get {return _storage._celebrityRecognitionAnnotations ?? Google_Cloud_Videointelligence_V1p3beta1_CelebrityRecognitionAnnotation()}
    set {_uniqueStorage()._celebrityRecognitionAnnotations = newValue}
  }
  /// Returns true if `celebrityRecognitionAnnotations` has been explicitly set.
  public var hasCelebrityRecognitionAnnotations: Bool {return _storage._celebrityRecognitionAnnotations != nil}
  /// Clears the value of `celebrityRecognitionAnnotations`. Subsequent reads from it will return its default value.
  public mutating func clearCelebrityRecognitionAnnotations() {_uniqueStorage()._celebrityRecognitionAnnotations = nil}

  /// If set, indicates an error. Note that for a single `AnnotateVideoRequest`
  /// some videos may succeed and some may fail.
  public var error: Google_Rpc_Status {
    get {return _storage._error ?? Google_Rpc_Status()}
    set {_uniqueStorage()._error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return _storage._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {_uniqueStorage()._error = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Video annotation response. Included in the `response`
/// field of the `Operation` returned by the `GetOperation`
/// call of the `google::longrunning::Operations` service.
public struct Google_Cloud_Videointelligence_V1p3beta1_AnnotateVideoResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Annotation results for all videos specified in `AnnotateVideoRequest`.
  public var annotationResults: [Google_Cloud_Videointelligence_V1p3beta1_VideoAnnotationResults] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Annotation progress for a single video.
public struct Google_Cloud_Videointelligence_V1p3beta1_VideoAnnotationProgress {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video file location in
  /// [Cloud Storage](https://cloud.google.com/storage/).
  public var inputUri: String = String()

  /// Approximate percentage processed thus far. Guaranteed to be
  /// 100 when fully processed.
  public var progressPercent: Int32 = 0

  /// Time when the request was received.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return self._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {self._startTime = nil}

  /// Time of the most recent update.
  public var updateTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _updateTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_updateTime = newValue}
  }
  /// Returns true if `updateTime` has been explicitly set.
  public var hasUpdateTime: Bool {return self._updateTime != nil}
  /// Clears the value of `updateTime`. Subsequent reads from it will return its default value.
  public mutating func clearUpdateTime() {self._updateTime = nil}

  /// Specifies which feature is being tracked if the request contains more than
  /// one feature.
  public var feature: Google_Cloud_Videointelligence_V1p3beta1_Feature = .unspecified

  /// Specifies which segment is being tracked if the request contains more than
  /// one segment.
  public var segment: Google_Cloud_Videointelligence_V1p3beta1_VideoSegment {
    get {return _segment ?? Google_Cloud_Videointelligence_V1p3beta1_VideoSegment()}
    set {_segment = newValue}
  }
  /// Returns true if `segment` has been explicitly set.
  public var hasSegment: Bool {return self._segment != nil}
  /// Clears the value of `segment`. Subsequent reads from it will return its default value.
  public mutating func clearSegment() {self._segment = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
  fileprivate var _updateTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
  fileprivate var _segment: Google_Cloud_Videointelligence_V1p3beta1_VideoSegment? = nil
}

/// Video annotation progress. Included in the `metadata`
/// field of the `Operation` returned by the `GetOperation`
/// call of the `google::longrunning::Operations` service.
public struct Google_Cloud_Videointelligence_V1p3beta1_AnnotateVideoProgress {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Progress metadata for all videos specified in `AnnotateVideoRequest`.
  public var annotationProgress: [Google_Cloud_Videointelligence_V1p3beta1_VideoAnnotationProgress] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for SPEECH_TRANSCRIPTION.
public struct Google_Cloud_Videointelligence_V1p3beta1_SpeechTranscriptionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. *Required* The language of the supplied audio as a
  /// [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
  /// Example: "en-US".
  /// See [Language Support](https://cloud.google.com/speech/docs/languages)
  /// for a list of the currently supported language codes.
  public var languageCode: String = String()

  /// Optional. Maximum number of recognition hypotheses to be returned.
  /// Specifically, the maximum number of `SpeechRecognitionAlternative` messages
  /// within each `SpeechTranscription`. The server may return fewer than
  /// `max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will
  /// return a maximum of one. If omitted, will return a maximum of one.
  public var maxAlternatives: Int32 = 0

  /// Optional. If set to `true`, the server will attempt to filter out
  /// profanities, replacing all but the initial character in each filtered word
  /// with asterisks, e.g. "f***". If set to `false` or omitted, profanities
  /// won't be filtered out.
  public var filterProfanity: Bool = false

  /// Optional. A means to provide context to assist the speech recognition.
  public var speechContexts: [Google_Cloud_Videointelligence_V1p3beta1_SpeechContext] = []

  /// Optional. If 'true', adds punctuation to recognition result hypotheses.
  /// This feature is only available in select languages. Setting this for
  /// requests in other languages has no effect at all. The default 'false' value
  /// does not add punctuation to result hypotheses. NOTE: "This is currently
  /// offered as an experimental service, complimentary to all users. In the
  /// future this may be exclusively available as a premium feature."
  public var enableAutomaticPunctuation: Bool = false

  /// Optional. For file formats, such as MXF or MKV, supporting multiple audio
  /// tracks, specify up to two tracks. Default: track 0.
  public var audioTracks: [Int32] = []

  /// Optional. If 'true', enables speaker detection for each recognized word in
  /// the top alternative of the recognition result using a speaker_tag provided
  /// in the WordInfo.
  /// Note: When this is true, we send all the words from the beginning of the
  /// audio for the top alternative in every consecutive response.
  /// This is done in order to improve our speaker tags as our models learn to
  /// identify the speakers in the conversation over time.
  public var enableSpeakerDiarization: Bool = false

  /// Optional. If set, specifies the estimated number of speakers in the
  /// conversation. If not set, defaults to '2'. Ignored unless
  /// enable_speaker_diarization is set to true.
  public var diarizationSpeakerCount: Int32 = 0

  /// Optional. If `true`, the top result includes a list of words and the
  /// confidence for those words. If `false`, no word-level confidence
  /// information is returned. The default is `false`.
  public var enableWordConfidence: Bool = false

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Provides "hints" to the speech recognizer to favor specific words and phrases
/// in the results.
public struct Google_Cloud_Videointelligence_V1p3beta1_SpeechContext {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. A list of strings containing words and phrases "hints" so that
  /// the speech recognition is more likely to recognize them. This can be used
  /// to improve the accuracy for specific words and phrases, for example, if
  /// specific commands are typically spoken by the user. This can also be used
  /// to add additional words to the vocabulary of the recognizer. See
  /// [usage limits](https://cloud.google.com/speech/limits#content).
  public var phrases: [String] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A speech recognition result corresponding to a portion of the audio.
public struct Google_Cloud_Videointelligence_V1p3beta1_SpeechTranscription {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// May contain one or more recognition hypotheses (up to the maximum specified
  /// in `max_alternatives`).  These alternatives are ordered in terms of
  /// accuracy, with the top (first) alternative being the most probable, as
  /// ranked by the recognizer.
  public var alternatives: [Google_Cloud_Videointelligence_V1p3beta1_SpeechRecognitionAlternative] = []

  /// Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
  /// language tag of the language in this result. This language code was
  /// detected to have the most likelihood of being spoken in the audio.
  public var languageCode: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Alternative hypotheses (a.k.a. n-best list).
public struct Google_Cloud_Videointelligence_V1p3beta1_SpeechRecognitionAlternative {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Transcript text representing the words that the user spoke.
  public var transcript: String = String()

  /// Output only. The confidence estimate between 0.0 and 1.0. A higher number
  /// indicates an estimated greater likelihood that the recognized words are
  /// correct. This field is set only for the top alternative.
  /// This field is not guaranteed to be accurate and users should not rely on it
  /// to be always provided.
  /// The default of 0.0 is a sentinel value indicating `confidence` was not set.
  public var confidence: Float = 0

  /// Output only. A list of word-specific information for each recognized word.
  /// Note: When `enable_speaker_diarization` is set to true, you will see all
  /// the words from the beginning of the audio.
  public var words: [Google_Cloud_Videointelligence_V1p3beta1_WordInfo] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Word-specific information for recognized words. Word information is only
/// included in the response when certain request parameters are set, such
/// as `enable_word_time_offsets`.
public struct Google_Cloud_Videointelligence_V1p3beta1_WordInfo {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Time offset relative to the beginning of the audio, and
  /// corresponding to the start of the spoken word. This field is only set if
  /// `enable_word_time_offsets=true` and only in the top hypothesis. This is an
  /// experimental feature and the accuracy of the time offset can vary.
  public var startTime: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _startTime ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return self._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {self._startTime = nil}

  /// Time offset relative to the beginning of the audio, and
  /// corresponding to the end of the spoken word. This field is only set if
  /// `enable_word_time_offsets=true` and only in the top hypothesis. This is an
  /// experimental feature and the accuracy of the time offset can vary.
  public var endTime: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _endTime ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_endTime = newValue}
  }
  /// Returns true if `endTime` has been explicitly set.
  public var hasEndTime: Bool {return self._endTime != nil}
  /// Clears the value of `endTime`. Subsequent reads from it will return its default value.
  public mutating func clearEndTime() {self._endTime = nil}

  /// The word corresponding to this set of information.
  public var word: String = String()

  /// Output only. The confidence estimate between 0.0 and 1.0. A higher number
  /// indicates an estimated greater likelihood that the recognized words are
  /// correct. This field is set only for the top alternative.
  /// This field is not guaranteed to be accurate and users should not rely on it
  /// to be always provided.
  /// The default of 0.0 is a sentinel value indicating `confidence` was not set.
  public var confidence: Float = 0

  /// Output only. A distinct integer value is assigned for every speaker within
  /// the audio. This field specifies which one of those speakers was detected to
  /// have spoken this word. Value ranges from 1 up to diarization_speaker_count,
  /// and is only set if speaker diarization is enabled.
  public var speakerTag: Int32 = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _startTime: SwiftProtobuf.Google_Protobuf_Duration? = nil
  fileprivate var _endTime: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// A vertex represents a 2D point in the image.
/// NOTE: the normalized vertex coordinates are relative to the original image
/// and range from 0 to 1.
public struct Google_Cloud_Videointelligence_V1p3beta1_NormalizedVertex {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// X coordinate.
  public var x: Float = 0

  /// Y coordinate.
  public var y: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Normalized bounding polygon for text (that might not be aligned with axis).
/// Contains list of the corner points in clockwise order starting from
/// top-left corner. For example, for a rectangular bounding box:
/// When the text is horizontal it might look like:
///         0----1
///         |    |
///         3----2
///
/// When it's clockwise rotated 180 degrees around the top-left corner it
/// becomes:
///         2----3
///         |    |
///         1----0
///
/// and the vertex order will still be (0, 1, 2, 3). Note that values can be less
/// than 0, or greater than 1 due to trignometric calculations for location of
/// the box.
public struct Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingPoly {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Normalized vertices of the bounding polygon.
  public var vertices: [Google_Cloud_Videointelligence_V1p3beta1_NormalizedVertex] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Video segment level annotation results for text detection.
public struct Google_Cloud_Videointelligence_V1p3beta1_TextSegment {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video segment where a text snippet was detected.
  public var segment: Google_Cloud_Videointelligence_V1p3beta1_VideoSegment {
    get {return _segment ?? Google_Cloud_Videointelligence_V1p3beta1_VideoSegment()}
    set {_segment = newValue}
  }
  /// Returns true if `segment` has been explicitly set.
  public var hasSegment: Bool {return self._segment != nil}
  /// Clears the value of `segment`. Subsequent reads from it will return its default value.
  public mutating func clearSegment() {self._segment = nil}

  /// Confidence for the track of detected text. It is calculated as the highest
  /// over all frames where OCR detected text appears.
  public var confidence: Float = 0

  /// Information related to the frames where OCR detected text appears.
  public var frames: [Google_Cloud_Videointelligence_V1p3beta1_TextFrame] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _segment: Google_Cloud_Videointelligence_V1p3beta1_VideoSegment? = nil
}

/// Video frame level annotation results for text annotation (OCR).
/// Contains information regarding timestamp and bounding box locations for the
/// frames containing detected OCR text snippets.
public struct Google_Cloud_Videointelligence_V1p3beta1_TextFrame {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Bounding polygon of the detected text for this frame.
  public var rotatedBoundingBox: Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingPoly {
    get {return _rotatedBoundingBox ?? Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingPoly()}
    set {_rotatedBoundingBox = newValue}
  }
  /// Returns true if `rotatedBoundingBox` has been explicitly set.
  public var hasRotatedBoundingBox: Bool {return self._rotatedBoundingBox != nil}
  /// Clears the value of `rotatedBoundingBox`. Subsequent reads from it will return its default value.
  public mutating func clearRotatedBoundingBox() {self._rotatedBoundingBox = nil}

  /// Timestamp of this frame.
  public var timeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _timeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_timeOffset = newValue}
  }
  /// Returns true if `timeOffset` has been explicitly set.
  public var hasTimeOffset: Bool {return self._timeOffset != nil}
  /// Clears the value of `timeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearTimeOffset() {self._timeOffset = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _rotatedBoundingBox: Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingPoly? = nil
  fileprivate var _timeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Annotations related to one detected OCR text snippet. This will contain the
/// corresponding text, confidence value, and frame level information for each
/// detection.
public struct Google_Cloud_Videointelligence_V1p3beta1_TextAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The detected text.
  public var text: String = String()

  /// All video segments where OCR detected text appears.
  public var segments: [Google_Cloud_Videointelligence_V1p3beta1_TextSegment] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Video frame level annotations for object detection and tracking. This field
/// stores per frame location, time offset, and confidence.
public struct Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingFrame {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The normalized bounding box location of this object track for the frame.
  public var normalizedBoundingBox: Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingBox {
    get {return _normalizedBoundingBox ?? Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingBox()}
    set {_normalizedBoundingBox = newValue}
  }
  /// Returns true if `normalizedBoundingBox` has been explicitly set.
  public var hasNormalizedBoundingBox: Bool {return self._normalizedBoundingBox != nil}
  /// Clears the value of `normalizedBoundingBox`. Subsequent reads from it will return its default value.
  public mutating func clearNormalizedBoundingBox() {self._normalizedBoundingBox = nil}

  /// The timestamp of the frame in microseconds.
  public var timeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _timeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_timeOffset = newValue}
  }
  /// Returns true if `timeOffset` has been explicitly set.
  public var hasTimeOffset: Bool {return self._timeOffset != nil}
  /// Clears the value of `timeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearTimeOffset() {self._timeOffset = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _normalizedBoundingBox: Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingBox? = nil
  fileprivate var _timeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Annotations corresponding to one tracked object.
public struct Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Different representation of tracking info in non-streaming batch
  /// and streaming modes.
  public var trackInfo: Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingAnnotation.OneOf_TrackInfo? = nil

  /// Non-streaming batch mode ONLY.
  /// Each object track corresponds to one video segment where it appears.
  public var segment: Google_Cloud_Videointelligence_V1p3beta1_VideoSegment {
    get {
      if case .segment(let v)? = trackInfo {return v}
      return Google_Cloud_Videointelligence_V1p3beta1_VideoSegment()
    }
    set {trackInfo = .segment(newValue)}
  }

  /// Streaming mode ONLY.
  /// In streaming mode, we do not know the end time of a tracked object
  /// before it is completed. Hence, there is no VideoSegment info returned.
  /// Instead, we provide a unique identifiable integer track_id so that
  /// the customers can correlate the results of the ongoing
  /// ObjectTrackAnnotation of the same track_id over time.
  public var trackID: Int64 {
    get {
      if case .trackID(let v)? = trackInfo {return v}
      return 0
    }
    set {trackInfo = .trackID(newValue)}
  }

  /// Entity to specify the object category that this track is labeled as.
  public var entity: Google_Cloud_Videointelligence_V1p3beta1_Entity {
    get {return _entity ?? Google_Cloud_Videointelligence_V1p3beta1_Entity()}
    set {_entity = newValue}
  }
  /// Returns true if `entity` has been explicitly set.
  public var hasEntity: Bool {return self._entity != nil}
  /// Clears the value of `entity`. Subsequent reads from it will return its default value.
  public mutating func clearEntity() {self._entity = nil}

  /// Object category's labeling confidence of this track.
  public var confidence: Float = 0

  /// Information corresponding to all frames where this object track appears.
  /// Non-streaming batch mode: it may be one or multiple ObjectTrackingFrame
  /// messages in frames.
  /// Streaming mode: it can only be one ObjectTrackingFrame message in frames.
  public var frames: [Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingFrame] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Different representation of tracking info in non-streaming batch
  /// and streaming modes.
  public enum OneOf_TrackInfo: Equatable {
    /// Non-streaming batch mode ONLY.
    /// Each object track corresponds to one video segment where it appears.
    case segment(Google_Cloud_Videointelligence_V1p3beta1_VideoSegment)
    /// Streaming mode ONLY.
    /// In streaming mode, we do not know the end time of a tracked object
    /// before it is completed. Hence, there is no VideoSegment info returned.
    /// Instead, we provide a unique identifiable integer track_id so that
    /// the customers can correlate the results of the ongoing
    /// ObjectTrackAnnotation of the same track_id over time.
    case trackID(Int64)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingAnnotation.OneOf_TrackInfo, rhs: Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingAnnotation.OneOf_TrackInfo) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.segment, .segment): return {
        guard case .segment(let l) = lhs, case .segment(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.trackID, .trackID): return {
        guard case .trackID(let l) = lhs, case .trackID(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}

  fileprivate var _entity: Google_Cloud_Videointelligence_V1p3beta1_Entity? = nil
}

/// Annotation corresponding to one detected, tracked and recognized logo class.
public struct Google_Cloud_Videointelligence_V1p3beta1_LogoRecognitionAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Entity category information to specify the logo class that all the logo
  /// tracks within this LogoRecognitionAnnotation are recognized as.
  public var entity: Google_Cloud_Videointelligence_V1p3beta1_Entity {
    get {return _entity ?? Google_Cloud_Videointelligence_V1p3beta1_Entity()}
    set {_entity = newValue}
  }
  /// Returns true if `entity` has been explicitly set.
  public var hasEntity: Bool {return self._entity != nil}
  /// Clears the value of `entity`. Subsequent reads from it will return its default value.
  public mutating func clearEntity() {self._entity = nil}

  /// All logo tracks where the recognized logo appears. Each track corresponds
  /// to one logo instance appearing in consecutive frames.
  public var tracks: [Google_Cloud_Videointelligence_V1p3beta1_Track] = []

  /// All video segments where the recognized logo appears. There might be
  /// multiple instances of the same logo class appearing in one VideoSegment.
  public var segments: [Google_Cloud_Videointelligence_V1p3beta1_VideoSegment] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _entity: Google_Cloud_Videointelligence_V1p3beta1_Entity? = nil
}

/// The top-level message sent by the client for the `StreamingAnnotateVideo`
/// method. Multiple `StreamingAnnotateVideoRequest` messages are sent.
/// The first message must only contain a `StreamingVideoConfig` message.
/// All subsequent messages must only contain `input_content` data.
public struct Google_Cloud_Videointelligence_V1p3beta1_StreamingAnnotateVideoRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Required* The streaming request, which is either a streaming config or
  /// video content.
  public var streamingRequest: Google_Cloud_Videointelligence_V1p3beta1_StreamingAnnotateVideoRequest.OneOf_StreamingRequest? = nil

  /// Provides information to the annotator, specifing how to process the
  /// request. The first `AnnotateStreamingVideoRequest` message must only
  /// contain a `video_config` message.
  public var videoConfig: Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoConfig {
    get {
      if case .videoConfig(let v)? = streamingRequest {return v}
      return Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoConfig()
    }
    set {streamingRequest = .videoConfig(newValue)}
  }

  /// The video data to be annotated. Chunks of video data are sequentially
  /// sent in `StreamingAnnotateVideoRequest` messages. Except the initial
  /// `StreamingAnnotateVideoRequest` message containing only
  /// `video_config`, all subsequent `AnnotateStreamingVideoRequest`
  /// messages must only contain `input_content` field.
  /// Note: as with all bytes fields, protobuffers use a pure binary
  /// representation (not base64).
  public var inputContent: Data {
    get {
      if case .inputContent(let v)? = streamingRequest {return v}
      return Data()
    }
    set {streamingRequest = .inputContent(newValue)}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// *Required* The streaming request, which is either a streaming config or
  /// video content.
  public enum OneOf_StreamingRequest: Equatable {
    /// Provides information to the annotator, specifing how to process the
    /// request. The first `AnnotateStreamingVideoRequest` message must only
    /// contain a `video_config` message.
    case videoConfig(Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoConfig)
    /// The video data to be annotated. Chunks of video data are sequentially
    /// sent in `StreamingAnnotateVideoRequest` messages. Except the initial
    /// `StreamingAnnotateVideoRequest` message containing only
    /// `video_config`, all subsequent `AnnotateStreamingVideoRequest`
    /// messages must only contain `input_content` field.
    /// Note: as with all bytes fields, protobuffers use a pure binary
    /// representation (not base64).
    case inputContent(Data)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingAnnotateVideoRequest.OneOf_StreamingRequest, rhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingAnnotateVideoRequest.OneOf_StreamingRequest) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.videoConfig, .videoConfig): return {
        guard case .videoConfig(let l) = lhs, case .videoConfig(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.inputContent, .inputContent): return {
        guard case .inputContent(let l) = lhs, case .inputContent(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}
}

/// Provides information to the annotator that specifies how to process the
/// request.
public struct Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Config for requested annotation feature.
  public var streamingConfig: Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoConfig.OneOf_StreamingConfig? = nil

  /// Config for STREAMING_SHOT_CHANGE_DETECTION.
  public var shotChangeDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_StreamingShotChangeDetectionConfig {
    get {
      if case .shotChangeDetectionConfig(let v)? = streamingConfig {return v}
      return Google_Cloud_Videointelligence_V1p3beta1_StreamingShotChangeDetectionConfig()
    }
    set {streamingConfig = .shotChangeDetectionConfig(newValue)}
  }

  /// Config for STREAMING_LABEL_DETECTION.
  public var labelDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_StreamingLabelDetectionConfig {
    get {
      if case .labelDetectionConfig(let v)? = streamingConfig {return v}
      return Google_Cloud_Videointelligence_V1p3beta1_StreamingLabelDetectionConfig()
    }
    set {streamingConfig = .labelDetectionConfig(newValue)}
  }

  /// Config for STREAMING_EXPLICIT_CONTENT_DETECTION.
  public var explicitContentDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_StreamingExplicitContentDetectionConfig {
    get {
      if case .explicitContentDetectionConfig(let v)? = streamingConfig {return v}
      return Google_Cloud_Videointelligence_V1p3beta1_StreamingExplicitContentDetectionConfig()
    }
    set {streamingConfig = .explicitContentDetectionConfig(newValue)}
  }

  /// Config for STREAMING_OBJECT_TRACKING.
  public var objectTrackingConfig: Google_Cloud_Videointelligence_V1p3beta1_StreamingObjectTrackingConfig {
    get {
      if case .objectTrackingConfig(let v)? = streamingConfig {return v}
      return Google_Cloud_Videointelligence_V1p3beta1_StreamingObjectTrackingConfig()
    }
    set {streamingConfig = .objectTrackingConfig(newValue)}
  }

  /// Config for STREAMING_AUTOML_ACTION_RECOGNITION.
  public var automlActionRecognitionConfig: Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlActionRecognitionConfig {
    get {
      if case .automlActionRecognitionConfig(let v)? = streamingConfig {return v}
      return Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlActionRecognitionConfig()
    }
    set {streamingConfig = .automlActionRecognitionConfig(newValue)}
  }

  /// Config for STREAMING_AUTOML_CLASSIFICATION.
  public var automlClassificationConfig: Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlClassificationConfig {
    get {
      if case .automlClassificationConfig(let v)? = streamingConfig {return v}
      return Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlClassificationConfig()
    }
    set {streamingConfig = .automlClassificationConfig(newValue)}
  }

  /// Config for STREAMING_AUTOML_OBJECT_TRACKING.
  public var automlObjectTrackingConfig: Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlObjectTrackingConfig {
    get {
      if case .automlObjectTrackingConfig(let v)? = streamingConfig {return v}
      return Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlObjectTrackingConfig()
    }
    set {streamingConfig = .automlObjectTrackingConfig(newValue)}
  }

  /// Requested annotation feature.
  public var feature: Google_Cloud_Videointelligence_V1p3beta1_StreamingFeature = .unspecified

  /// Streaming storage option. By default: storage is disabled.
  public var storageConfig: Google_Cloud_Videointelligence_V1p3beta1_StreamingStorageConfig {
    get {return _storageConfig ?? Google_Cloud_Videointelligence_V1p3beta1_StreamingStorageConfig()}
    set {_storageConfig = newValue}
  }
  /// Returns true if `storageConfig` has been explicitly set.
  public var hasStorageConfig: Bool {return self._storageConfig != nil}
  /// Clears the value of `storageConfig`. Subsequent reads from it will return its default value.
  public mutating func clearStorageConfig() {self._storageConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Config for requested annotation feature.
  public enum OneOf_StreamingConfig: Equatable {
    /// Config for STREAMING_SHOT_CHANGE_DETECTION.
    case shotChangeDetectionConfig(Google_Cloud_Videointelligence_V1p3beta1_StreamingShotChangeDetectionConfig)
    /// Config for STREAMING_LABEL_DETECTION.
    case labelDetectionConfig(Google_Cloud_Videointelligence_V1p3beta1_StreamingLabelDetectionConfig)
    /// Config for STREAMING_EXPLICIT_CONTENT_DETECTION.
    case explicitContentDetectionConfig(Google_Cloud_Videointelligence_V1p3beta1_StreamingExplicitContentDetectionConfig)
    /// Config for STREAMING_OBJECT_TRACKING.
    case objectTrackingConfig(Google_Cloud_Videointelligence_V1p3beta1_StreamingObjectTrackingConfig)
    /// Config for STREAMING_AUTOML_ACTION_RECOGNITION.
    case automlActionRecognitionConfig(Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlActionRecognitionConfig)
    /// Config for STREAMING_AUTOML_CLASSIFICATION.
    case automlClassificationConfig(Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlClassificationConfig)
    /// Config for STREAMING_AUTOML_OBJECT_TRACKING.
    case automlObjectTrackingConfig(Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlObjectTrackingConfig)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoConfig.OneOf_StreamingConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoConfig.OneOf_StreamingConfig) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.shotChangeDetectionConfig, .shotChangeDetectionConfig): return {
        guard case .shotChangeDetectionConfig(let l) = lhs, case .shotChangeDetectionConfig(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.labelDetectionConfig, .labelDetectionConfig): return {
        guard case .labelDetectionConfig(let l) = lhs, case .labelDetectionConfig(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.explicitContentDetectionConfig, .explicitContentDetectionConfig): return {
        guard case .explicitContentDetectionConfig(let l) = lhs, case .explicitContentDetectionConfig(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.objectTrackingConfig, .objectTrackingConfig): return {
        guard case .objectTrackingConfig(let l) = lhs, case .objectTrackingConfig(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.automlActionRecognitionConfig, .automlActionRecognitionConfig): return {
        guard case .automlActionRecognitionConfig(let l) = lhs, case .automlActionRecognitionConfig(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.automlClassificationConfig, .automlClassificationConfig): return {
        guard case .automlClassificationConfig(let l) = lhs, case .automlClassificationConfig(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.automlObjectTrackingConfig, .automlObjectTrackingConfig): return {
        guard case .automlObjectTrackingConfig(let l) = lhs, case .automlObjectTrackingConfig(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}

  fileprivate var _storageConfig: Google_Cloud_Videointelligence_V1p3beta1_StreamingStorageConfig? = nil
}

/// `StreamingAnnotateVideoResponse` is the only message returned to the client
/// by `StreamingAnnotateVideo`. A series of zero or more
/// `StreamingAnnotateVideoResponse` messages are streamed back to the client.
public struct Google_Cloud_Videointelligence_V1p3beta1_StreamingAnnotateVideoResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// If set, returns a [google.rpc.Status][google.rpc.Status] message that
  /// specifies the error for the operation.
  public var error: Google_Rpc_Status {
    get {return _error ?? Google_Rpc_Status()}
    set {_error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return self._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {self._error = nil}

  /// Streaming annotation results.
  public var annotationResults: Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoAnnotationResults {
    get {return _annotationResults ?? Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoAnnotationResults()}
    set {_annotationResults = newValue}
  }
  /// Returns true if `annotationResults` has been explicitly set.
  public var hasAnnotationResults: Bool {return self._annotationResults != nil}
  /// Clears the value of `annotationResults`. Subsequent reads from it will return its default value.
  public mutating func clearAnnotationResults() {self._annotationResults = nil}

  /// Google Cloud Storage(GCS) URI that stores annotation results of one
  /// streaming session in JSON format.
  /// It is the annotation_result_storage_directory
  /// from the request followed by '/cloud_project_number-session_id'.
  public var annotationResultsUri: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _error: Google_Rpc_Status? = nil
  fileprivate var _annotationResults: Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoAnnotationResults? = nil
}

/// Streaming annotation results corresponding to a portion of the video
/// that is currently being processed.
public struct Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoAnnotationResults {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Shot annotation results. Each shot is represented as a video segment.
  public var shotAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_VideoSegment] = []

  /// Label annotation results.
  public var labelAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation] = []

  /// Explicit content annotation results.
  public var explicitAnnotation: Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentAnnotation {
    get {return _explicitAnnotation ?? Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentAnnotation()}
    set {_explicitAnnotation = newValue}
  }
  /// Returns true if `explicitAnnotation` has been explicitly set.
  public var hasExplicitAnnotation: Bool {return self._explicitAnnotation != nil}
  /// Clears the value of `explicitAnnotation`. Subsequent reads from it will return its default value.
  public mutating func clearExplicitAnnotation() {self._explicitAnnotation = nil}

  /// Object tracking results.
  public var objectAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingAnnotation] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _explicitAnnotation: Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentAnnotation? = nil
}

/// Config for STREAMING_SHOT_CHANGE_DETECTION.
public struct Google_Cloud_Videointelligence_V1p3beta1_StreamingShotChangeDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for STREAMING_LABEL_DETECTION.
public struct Google_Cloud_Videointelligence_V1p3beta1_StreamingLabelDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Whether the video has been captured from a stationary (i.e. non-moving)
  /// camera. When set to true, might improve detection accuracy for moving
  /// objects. Default: false.
  public var stationaryCamera: Bool = false

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for STREAMING_EXPLICIT_CONTENT_DETECTION.
public struct Google_Cloud_Videointelligence_V1p3beta1_StreamingExplicitContentDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for STREAMING_OBJECT_TRACKING.
public struct Google_Cloud_Videointelligence_V1p3beta1_StreamingObjectTrackingConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for STREAMING_AUTOML_ACTION_RECOGNITION.
public struct Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlActionRecognitionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Resource name of AutoML model.
  /// Format: `projects/{project_id}/locations/{location_id}/models/{model_id}`
  public var modelName: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for STREAMING_AUTOML_CLASSIFICATION.
public struct Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlClassificationConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Resource name of AutoML model.
  /// Format:
  /// `projects/{project_number}/locations/{location_id}/models/{model_id}`
  public var modelName: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for STREAMING_AUTOML_OBJECT_TRACKING.
public struct Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlObjectTrackingConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Resource name of AutoML model.
  /// Format: `projects/{project_id}/locations/{location_id}/models/{model_id}`
  public var modelName: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for streaming storage option.
public struct Google_Cloud_Videointelligence_V1p3beta1_StreamingStorageConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Enable streaming storage. Default: false.
  public var enableStorageAnnotationResult: Bool = false

  /// Cloud Storage URI to store all annotation results for one client. Client
  /// should specify this field as the top-level storage directory. Annotation
  /// results of different sessions will be put into different sub-directories
  /// denoted by project_name and session_id. All sub-directories will be auto
  /// generated by program and will be made accessible to client in response
  /// proto. URIs must be specified in the following format:
  /// `gs://bucket-id/object-id` `bucket-id` should be a valid Cloud Storage
  /// bucket created by client and bucket permission shall also be configured
  /// properly. `object-id` can be arbitrary string that make sense to client.
  /// Other URI formats will return error and cause Cloud Storage write failure.
  public var annotationResultStorageDirectory: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.videointelligence.v1p3beta1"

extension Google_Cloud_Videointelligence_V1p3beta1_LabelDetectionMode: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "LABEL_DETECTION_MODE_UNSPECIFIED"),
    1: .same(proto: "SHOT_MODE"),
    2: .same(proto: "FRAME_MODE"),
    3: .same(proto: "SHOT_AND_FRAME_MODE"),
  ]
}

extension Google_Cloud_Videointelligence_V1p3beta1_Likelihood: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "LIKELIHOOD_UNSPECIFIED"),
    1: .same(proto: "VERY_UNLIKELY"),
    2: .same(proto: "UNLIKELY"),
    3: .same(proto: "POSSIBLE"),
    4: .same(proto: "LIKELY"),
    5: .same(proto: "VERY_LIKELY"),
  ]
}

extension Google_Cloud_Videointelligence_V1p3beta1_StreamingFeature: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "STREAMING_FEATURE_UNSPECIFIED"),
    1: .same(proto: "STREAMING_LABEL_DETECTION"),
    2: .same(proto: "STREAMING_SHOT_CHANGE_DETECTION"),
    3: .same(proto: "STREAMING_EXPLICIT_CONTENT_DETECTION"),
    4: .same(proto: "STREAMING_OBJECT_TRACKING"),
    21: .same(proto: "STREAMING_AUTOML_CLASSIFICATION"),
    22: .same(proto: "STREAMING_AUTOML_OBJECT_TRACKING"),
    23: .same(proto: "STREAMING_AUTOML_ACTION_RECOGNITION"),
  ]
}

extension Google_Cloud_Videointelligence_V1p3beta1_Feature: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "FEATURE_UNSPECIFIED"),
    1: .same(proto: "LABEL_DETECTION"),
    2: .same(proto: "SHOT_CHANGE_DETECTION"),
    3: .same(proto: "EXPLICIT_CONTENT_DETECTION"),
    4: .same(proto: "FACE_DETECTION"),
    6: .same(proto: "SPEECH_TRANSCRIPTION"),
    7: .same(proto: "TEXT_DETECTION"),
    9: .same(proto: "OBJECT_TRACKING"),
    12: .same(proto: "LOGO_RECOGNITION"),
    13: .same(proto: "CELEBRITY_RECOGNITION"),
    14: .same(proto: "PERSON_DETECTION"),
  ]
}

extension Google_Cloud_Videointelligence_V1p3beta1_AnnotateVideoRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AnnotateVideoRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "input_uri"),
    6: .standard(proto: "input_content"),
    2: .same(proto: "features"),
    3: .standard(proto: "video_context"),
    4: .standard(proto: "output_uri"),
    5: .standard(proto: "location_id"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.inputUri) }()
      case 2: try { try decoder.decodeRepeatedEnumField(value: &self.features) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._videoContext) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.outputUri) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.locationID) }()
      case 6: try { try decoder.decodeSingularBytesField(value: &self.inputContent) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.inputUri.isEmpty {
      try visitor.visitSingularStringField(value: self.inputUri, fieldNumber: 1)
    }
    if !self.features.isEmpty {
      try visitor.visitPackedEnumField(value: self.features, fieldNumber: 2)
    }
    if let v = self._videoContext {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if !self.outputUri.isEmpty {
      try visitor.visitSingularStringField(value: self.outputUri, fieldNumber: 4)
    }
    if !self.locationID.isEmpty {
      try visitor.visitSingularStringField(value: self.locationID, fieldNumber: 5)
    }
    if !self.inputContent.isEmpty {
      try visitor.visitSingularBytesField(value: self.inputContent, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_AnnotateVideoRequest, rhs: Google_Cloud_Videointelligence_V1p3beta1_AnnotateVideoRequest) -> Bool {
    if lhs.inputUri != rhs.inputUri {return false}
    if lhs.inputContent != rhs.inputContent {return false}
    if lhs.features != rhs.features {return false}
    if lhs._videoContext != rhs._videoContext {return false}
    if lhs.outputUri != rhs.outputUri {return false}
    if lhs.locationID != rhs.locationID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_VideoContext: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoContext"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "segments"),
    2: .standard(proto: "label_detection_config"),
    3: .standard(proto: "shot_change_detection_config"),
    4: .standard(proto: "explicit_content_detection_config"),
    5: .standard(proto: "face_detection_config"),
    6: .standard(proto: "speech_transcription_config"),
    8: .standard(proto: "text_detection_config"),
    11: .standard(proto: "person_detection_config"),
    13: .standard(proto: "object_tracking_config"),
  ]

  fileprivate class _StorageClass {
    var _segments: [Google_Cloud_Videointelligence_V1p3beta1_VideoSegment] = []
    var _labelDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_LabelDetectionConfig? = nil
    var _shotChangeDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_ShotChangeDetectionConfig? = nil
    var _explicitContentDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentDetectionConfig? = nil
    var _faceDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_FaceDetectionConfig? = nil
    var _speechTranscriptionConfig: Google_Cloud_Videointelligence_V1p3beta1_SpeechTranscriptionConfig? = nil
    var _textDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_TextDetectionConfig? = nil
    var _personDetectionConfig: Google_Cloud_Videointelligence_V1p3beta1_PersonDetectionConfig? = nil
    var _objectTrackingConfig: Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingConfig? = nil

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _segments = source._segments
      _labelDetectionConfig = source._labelDetectionConfig
      _shotChangeDetectionConfig = source._shotChangeDetectionConfig
      _explicitContentDetectionConfig = source._explicitContentDetectionConfig
      _faceDetectionConfig = source._faceDetectionConfig
      _speechTranscriptionConfig = source._speechTranscriptionConfig
      _textDetectionConfig = source._textDetectionConfig
      _personDetectionConfig = source._personDetectionConfig
      _objectTrackingConfig = source._objectTrackingConfig
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeRepeatedMessageField(value: &_storage._segments) }()
        case 2: try { try decoder.decodeSingularMessageField(value: &_storage._labelDetectionConfig) }()
        case 3: try { try decoder.decodeSingularMessageField(value: &_storage._shotChangeDetectionConfig) }()
        case 4: try { try decoder.decodeSingularMessageField(value: &_storage._explicitContentDetectionConfig) }()
        case 5: try { try decoder.decodeSingularMessageField(value: &_storage._faceDetectionConfig) }()
        case 6: try { try decoder.decodeSingularMessageField(value: &_storage._speechTranscriptionConfig) }()
        case 8: try { try decoder.decodeSingularMessageField(value: &_storage._textDetectionConfig) }()
        case 11: try { try decoder.decodeSingularMessageField(value: &_storage._personDetectionConfig) }()
        case 13: try { try decoder.decodeSingularMessageField(value: &_storage._objectTrackingConfig) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if !_storage._segments.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._segments, fieldNumber: 1)
      }
      if let v = _storage._labelDetectionConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      }
      if let v = _storage._shotChangeDetectionConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      }
      if let v = _storage._explicitContentDetectionConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
      }
      if let v = _storage._faceDetectionConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      }
      if let v = _storage._speechTranscriptionConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
      }
      if let v = _storage._textDetectionConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
      }
      if let v = _storage._personDetectionConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 11)
      }
      if let v = _storage._objectTrackingConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 13)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_VideoContext, rhs: Google_Cloud_Videointelligence_V1p3beta1_VideoContext) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._segments != rhs_storage._segments {return false}
        if _storage._labelDetectionConfig != rhs_storage._labelDetectionConfig {return false}
        if _storage._shotChangeDetectionConfig != rhs_storage._shotChangeDetectionConfig {return false}
        if _storage._explicitContentDetectionConfig != rhs_storage._explicitContentDetectionConfig {return false}
        if _storage._faceDetectionConfig != rhs_storage._faceDetectionConfig {return false}
        if _storage._speechTranscriptionConfig != rhs_storage._speechTranscriptionConfig {return false}
        if _storage._textDetectionConfig != rhs_storage._textDetectionConfig {return false}
        if _storage._personDetectionConfig != rhs_storage._personDetectionConfig {return false}
        if _storage._objectTrackingConfig != rhs_storage._objectTrackingConfig {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_LabelDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "label_detection_mode"),
    2: .standard(proto: "stationary_camera"),
    3: .same(proto: "model"),
    4: .standard(proto: "frame_confidence_threshold"),
    5: .standard(proto: "video_confidence_threshold"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.labelDetectionMode) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.stationaryCamera) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.model) }()
      case 4: try { try decoder.decodeSingularFloatField(value: &self.frameConfidenceThreshold) }()
      case 5: try { try decoder.decodeSingularFloatField(value: &self.videoConfidenceThreshold) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.labelDetectionMode != .unspecified {
      try visitor.visitSingularEnumField(value: self.labelDetectionMode, fieldNumber: 1)
    }
    if self.stationaryCamera != false {
      try visitor.visitSingularBoolField(value: self.stationaryCamera, fieldNumber: 2)
    }
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 3)
    }
    if self.frameConfidenceThreshold != 0 {
      try visitor.visitSingularFloatField(value: self.frameConfidenceThreshold, fieldNumber: 4)
    }
    if self.videoConfidenceThreshold != 0 {
      try visitor.visitSingularFloatField(value: self.videoConfidenceThreshold, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_LabelDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_LabelDetectionConfig) -> Bool {
    if lhs.labelDetectionMode != rhs.labelDetectionMode {return false}
    if lhs.stationaryCamera != rhs.stationaryCamera {return false}
    if lhs.model != rhs.model {return false}
    if lhs.frameConfidenceThreshold != rhs.frameConfidenceThreshold {return false}
    if lhs.videoConfidenceThreshold != rhs.videoConfidenceThreshold {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_ShotChangeDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ShotChangeDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.model) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_ShotChangeDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_ShotChangeDetectionConfig) -> Bool {
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ObjectTrackingConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.model) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingConfig) -> Bool {
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ExplicitContentDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.model) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentDetectionConfig) -> Bool {
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_FaceDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".FaceDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "model"),
    2: .standard(proto: "include_bounding_boxes"),
    5: .standard(proto: "include_attributes"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.model) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.includeBoundingBoxes) }()
      case 5: try { try decoder.decodeSingularBoolField(value: &self.includeAttributes) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 1)
    }
    if self.includeBoundingBoxes != false {
      try visitor.visitSingularBoolField(value: self.includeBoundingBoxes, fieldNumber: 2)
    }
    if self.includeAttributes != false {
      try visitor.visitSingularBoolField(value: self.includeAttributes, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_FaceDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_FaceDetectionConfig) -> Bool {
    if lhs.model != rhs.model {return false}
    if lhs.includeBoundingBoxes != rhs.includeBoundingBoxes {return false}
    if lhs.includeAttributes != rhs.includeAttributes {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_PersonDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".PersonDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "include_bounding_boxes"),
    2: .standard(proto: "include_pose_landmarks"),
    3: .standard(proto: "include_attributes"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularBoolField(value: &self.includeBoundingBoxes) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.includePoseLandmarks) }()
      case 3: try { try decoder.decodeSingularBoolField(value: &self.includeAttributes) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.includeBoundingBoxes != false {
      try visitor.visitSingularBoolField(value: self.includeBoundingBoxes, fieldNumber: 1)
    }
    if self.includePoseLandmarks != false {
      try visitor.visitSingularBoolField(value: self.includePoseLandmarks, fieldNumber: 2)
    }
    if self.includeAttributes != false {
      try visitor.visitSingularBoolField(value: self.includeAttributes, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_PersonDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_PersonDetectionConfig) -> Bool {
    if lhs.includeBoundingBoxes != rhs.includeBoundingBoxes {return false}
    if lhs.includePoseLandmarks != rhs.includePoseLandmarks {return false}
    if lhs.includeAttributes != rhs.includeAttributes {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_TextDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TextDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "language_hints"),
    2: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedStringField(value: &self.languageHints) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.model) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.languageHints.isEmpty {
      try visitor.visitRepeatedStringField(value: self.languageHints, fieldNumber: 1)
    }
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_TextDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_TextDetectionConfig) -> Bool {
    if lhs.languageHints != rhs.languageHints {return false}
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_VideoSegment: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoSegment"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "start_time_offset"),
    2: .standard(proto: "end_time_offset"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._startTimeOffset) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._endTimeOffset) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._startTimeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._endTimeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_VideoSegment, rhs: Google_Cloud_Videointelligence_V1p3beta1_VideoSegment) -> Bool {
    if lhs._startTimeOffset != rhs._startTimeOffset {return false}
    if lhs._endTimeOffset != rhs._endTimeOffset {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_LabelSegment: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelSegment"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "segment"),
    2: .same(proto: "confidence"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._segment) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._segment {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_LabelSegment, rhs: Google_Cloud_Videointelligence_V1p3beta1_LabelSegment) -> Bool {
    if lhs._segment != rhs._segment {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_LabelFrame: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelFrame"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "time_offset"),
    2: .same(proto: "confidence"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._timeOffset) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._timeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_LabelFrame, rhs: Google_Cloud_Videointelligence_V1p3beta1_LabelFrame) -> Bool {
    if lhs._timeOffset != rhs._timeOffset {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_Entity: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".Entity"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "entity_id"),
    2: .same(proto: "description"),
    3: .standard(proto: "language_code"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.entityID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.description_p) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.languageCode) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.entityID.isEmpty {
      try visitor.visitSingularStringField(value: self.entityID, fieldNumber: 1)
    }
    if !self.description_p.isEmpty {
      try visitor.visitSingularStringField(value: self.description_p, fieldNumber: 2)
    }
    if !self.languageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.languageCode, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_Entity, rhs: Google_Cloud_Videointelligence_V1p3beta1_Entity) -> Bool {
    if lhs.entityID != rhs.entityID {return false}
    if lhs.description_p != rhs.description_p {return false}
    if lhs.languageCode != rhs.languageCode {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "entity"),
    2: .standard(proto: "category_entities"),
    3: .same(proto: "segments"),
    4: .same(proto: "frames"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._entity) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.categoryEntities) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.segments) }()
      case 4: try { try decoder.decodeRepeatedMessageField(value: &self.frames) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._entity {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if !self.categoryEntities.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.categoryEntities, fieldNumber: 2)
    }
    if !self.segments.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.segments, fieldNumber: 3)
    }
    if !self.frames.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frames, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation, rhs: Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation) -> Bool {
    if lhs._entity != rhs._entity {return false}
    if lhs.categoryEntities != rhs.categoryEntities {return false}
    if lhs.segments != rhs.segments {return false}
    if lhs.frames != rhs.frames {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentFrame: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ExplicitContentFrame"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "time_offset"),
    2: .standard(proto: "pornography_likelihood"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._timeOffset) }()
      case 2: try { try decoder.decodeSingularEnumField(value: &self.pornographyLikelihood) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._timeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.pornographyLikelihood != .unspecified {
      try visitor.visitSingularEnumField(value: self.pornographyLikelihood, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentFrame, rhs: Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentFrame) -> Bool {
    if lhs._timeOffset != rhs._timeOffset {return false}
    if lhs.pornographyLikelihood != rhs.pornographyLikelihood {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ExplicitContentAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "frames"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.frames) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.frames.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frames, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentAnnotation, rhs: Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentAnnotation) -> Bool {
    if lhs.frames != rhs.frames {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingBox: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".NormalizedBoundingBox"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "left"),
    2: .same(proto: "top"),
    3: .same(proto: "right"),
    4: .same(proto: "bottom"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularFloatField(value: &self.left) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.top) }()
      case 3: try { try decoder.decodeSingularFloatField(value: &self.right) }()
      case 4: try { try decoder.decodeSingularFloatField(value: &self.bottom) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.left != 0 {
      try visitor.visitSingularFloatField(value: self.left, fieldNumber: 1)
    }
    if self.top != 0 {
      try visitor.visitSingularFloatField(value: self.top, fieldNumber: 2)
    }
    if self.right != 0 {
      try visitor.visitSingularFloatField(value: self.right, fieldNumber: 3)
    }
    if self.bottom != 0 {
      try visitor.visitSingularFloatField(value: self.bottom, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingBox, rhs: Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingBox) -> Bool {
    if lhs.left != rhs.left {return false}
    if lhs.top != rhs.top {return false}
    if lhs.right != rhs.right {return false}
    if lhs.bottom != rhs.bottom {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_TimestampedObject: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TimestampedObject"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "normalized_bounding_box"),
    2: .standard(proto: "time_offset"),
    3: .same(proto: "attributes"),
    4: .same(proto: "landmarks"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._normalizedBoundingBox) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._timeOffset) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.attributes) }()
      case 4: try { try decoder.decodeRepeatedMessageField(value: &self.landmarks) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._normalizedBoundingBox {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._timeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if !self.attributes.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.attributes, fieldNumber: 3)
    }
    if !self.landmarks.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.landmarks, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_TimestampedObject, rhs: Google_Cloud_Videointelligence_V1p3beta1_TimestampedObject) -> Bool {
    if lhs._normalizedBoundingBox != rhs._normalizedBoundingBox {return false}
    if lhs._timeOffset != rhs._timeOffset {return false}
    if lhs.attributes != rhs.attributes {return false}
    if lhs.landmarks != rhs.landmarks {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_Track: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".Track"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "segment"),
    2: .standard(proto: "timestamped_objects"),
    3: .same(proto: "attributes"),
    4: .same(proto: "confidence"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._segment) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.timestampedObjects) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.attributes) }()
      case 4: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._segment {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if !self.timestampedObjects.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.timestampedObjects, fieldNumber: 2)
    }
    if !self.attributes.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.attributes, fieldNumber: 3)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_Track, rhs: Google_Cloud_Videointelligence_V1p3beta1_Track) -> Bool {
    if lhs._segment != rhs._segment {return false}
    if lhs.timestampedObjects != rhs.timestampedObjects {return false}
    if lhs.attributes != rhs.attributes {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_DetectedAttribute: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".DetectedAttribute"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .same(proto: "confidence"),
    3: .same(proto: "value"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.value) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    if !self.value.isEmpty {
      try visitor.visitSingularStringField(value: self.value, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_DetectedAttribute, rhs: Google_Cloud_Videointelligence_V1p3beta1_DetectedAttribute) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.value != rhs.value {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_Celebrity: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".Celebrity"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .standard(proto: "display_name"),
    3: .same(proto: "description"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.displayName) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.description_p) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if !self.displayName.isEmpty {
      try visitor.visitSingularStringField(value: self.displayName, fieldNumber: 2)
    }
    if !self.description_p.isEmpty {
      try visitor.visitSingularStringField(value: self.description_p, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_Celebrity, rhs: Google_Cloud_Videointelligence_V1p3beta1_Celebrity) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.displayName != rhs.displayName {return false}
    if lhs.description_p != rhs.description_p {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_CelebrityTrack: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".CelebrityTrack"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "celebrities"),
    3: .standard(proto: "face_track"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.celebrities) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._faceTrack) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.celebrities.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.celebrities, fieldNumber: 1)
    }
    if let v = self._faceTrack {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_CelebrityTrack, rhs: Google_Cloud_Videointelligence_V1p3beta1_CelebrityTrack) -> Bool {
    if lhs.celebrities != rhs.celebrities {return false}
    if lhs._faceTrack != rhs._faceTrack {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_CelebrityTrack.RecognizedCelebrity: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Videointelligence_V1p3beta1_CelebrityTrack.protoMessageName + ".RecognizedCelebrity"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "celebrity"),
    2: .same(proto: "confidence"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._celebrity) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._celebrity {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_CelebrityTrack.RecognizedCelebrity, rhs: Google_Cloud_Videointelligence_V1p3beta1_CelebrityTrack.RecognizedCelebrity) -> Bool {
    if lhs._celebrity != rhs._celebrity {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_CelebrityRecognitionAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".CelebrityRecognitionAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "celebrity_tracks"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.celebrityTracks) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.celebrityTracks.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.celebrityTracks, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_CelebrityRecognitionAnnotation, rhs: Google_Cloud_Videointelligence_V1p3beta1_CelebrityRecognitionAnnotation) -> Bool {
    if lhs.celebrityTracks != rhs.celebrityTracks {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_DetectedLandmark: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".DetectedLandmark"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .same(proto: "point"),
    3: .same(proto: "confidence"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._point) }()
      case 3: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if let v = self._point {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_DetectedLandmark, rhs: Google_Cloud_Videointelligence_V1p3beta1_DetectedLandmark) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs._point != rhs._point {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_FaceDetectionAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".FaceDetectionAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    3: .same(proto: "tracks"),
    4: .same(proto: "thumbnail"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.tracks) }()
      case 4: try { try decoder.decodeSingularBytesField(value: &self.thumbnail) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.tracks.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.tracks, fieldNumber: 3)
    }
    if !self.thumbnail.isEmpty {
      try visitor.visitSingularBytesField(value: self.thumbnail, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_FaceDetectionAnnotation, rhs: Google_Cloud_Videointelligence_V1p3beta1_FaceDetectionAnnotation) -> Bool {
    if lhs.tracks != rhs.tracks {return false}
    if lhs.thumbnail != rhs.thumbnail {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_PersonDetectionAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".PersonDetectionAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "tracks"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.tracks) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.tracks.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.tracks, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_PersonDetectionAnnotation, rhs: Google_Cloud_Videointelligence_V1p3beta1_PersonDetectionAnnotation) -> Bool {
    if lhs.tracks != rhs.tracks {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_VideoAnnotationResults: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoAnnotationResults"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "input_uri"),
    10: .same(proto: "segment"),
    2: .standard(proto: "segment_label_annotations"),
    23: .standard(proto: "segment_presence_label_annotations"),
    3: .standard(proto: "shot_label_annotations"),
    24: .standard(proto: "shot_presence_label_annotations"),
    4: .standard(proto: "frame_label_annotations"),
    13: .standard(proto: "face_detection_annotations"),
    6: .standard(proto: "shot_annotations"),
    7: .standard(proto: "explicit_annotation"),
    11: .standard(proto: "speech_transcriptions"),
    12: .standard(proto: "text_annotations"),
    14: .standard(proto: "object_annotations"),
    19: .standard(proto: "logo_recognition_annotations"),
    20: .standard(proto: "person_detection_annotations"),
    21: .standard(proto: "celebrity_recognition_annotations"),
    9: .same(proto: "error"),
  ]

  fileprivate class _StorageClass {
    var _inputUri: String = String()
    var _segment: Google_Cloud_Videointelligence_V1p3beta1_VideoSegment? = nil
    var _segmentLabelAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation] = []
    var _segmentPresenceLabelAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation] = []
    var _shotLabelAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation] = []
    var _shotPresenceLabelAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation] = []
    var _frameLabelAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_LabelAnnotation] = []
    var _faceDetectionAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_FaceDetectionAnnotation] = []
    var _shotAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_VideoSegment] = []
    var _explicitAnnotation: Google_Cloud_Videointelligence_V1p3beta1_ExplicitContentAnnotation? = nil
    var _speechTranscriptions: [Google_Cloud_Videointelligence_V1p3beta1_SpeechTranscription] = []
    var _textAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_TextAnnotation] = []
    var _objectAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingAnnotation] = []
    var _logoRecognitionAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_LogoRecognitionAnnotation] = []
    var _personDetectionAnnotations: [Google_Cloud_Videointelligence_V1p3beta1_PersonDetectionAnnotation] = []
    var _celebrityRecognitionAnnotations: Google_Cloud_Videointelligence_V1p3beta1_CelebrityRecognitionAnnotation? = nil
    var _error: Google_Rpc_Status? = nil

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _inputUri = source._inputUri
      _segment = source._segment
      _segmentLabelAnnotations = source._segmentLabelAnnotations
      _segmentPresenceLabelAnnotations = source._segmentPresenceLabelAnnotations
      _shotLabelAnnotations = source._shotLabelAnnotations
      _shotPresenceLabelAnnotations = source._shotPresenceLabelAnnotations
      _frameLabelAnnotations = source._frameLabelAnnotations
      _faceDetectionAnnotations = source._faceDetectionAnnotations
      _shotAnnotations = source._shotAnnotations
      _explicitAnnotation = source._explicitAnnotation
      _speechTranscriptions = source._speechTranscriptions
      _textAnnotations = source._textAnnotations
      _objectAnnotations = source._objectAnnotations
      _logoRecognitionAnnotations = source._logoRecognitionAnnotations
      _personDetectionAnnotations = source._personDetectionAnnotations
      _celebrityRecognitionAnnotations = source._celebrityRecognitionAnnotations
      _error = source._error
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularStringField(value: &_storage._inputUri) }()
        case 2: try { try decoder.decodeRepeatedMessageField(value: &_storage._segmentLabelAnnotations) }()
        case 3: try { try decoder.decodeRepeatedMessageField(value: &_storage._shotLabelAnnotations) }()
        case 4: try { try decoder.decodeRepeatedMessageField(value: &_storage._frameLabelAnnotations) }()
        case 6: try { try decoder.decodeRepeatedMessageField(value: &_storage._shotAnnotations) }()
        case 7: try { try decoder.decodeSingularMessageField(value: &_storage._explicitAnnotation) }()
        case 9: try { try decoder.decodeSingularMessageField(value: &_storage._error) }()
        case 10: try { try decoder.decodeSingularMessageField(value: &_storage._segment) }()
        case 11: try { try decoder.decodeRepeatedMessageField(value: &_storage._speechTranscriptions) }()
        case 12: try { try decoder.decodeRepeatedMessageField(value: &_storage._textAnnotations) }()
        case 13: try { try decoder.decodeRepeatedMessageField(value: &_storage._faceDetectionAnnotations) }()
        case 14: try { try decoder.decodeRepeatedMessageField(value: &_storage._objectAnnotations) }()
        case 19: try { try decoder.decodeRepeatedMessageField(value: &_storage._logoRecognitionAnnotations) }()
        case 20: try { try decoder.decodeRepeatedMessageField(value: &_storage._personDetectionAnnotations) }()
        case 21: try { try decoder.decodeSingularMessageField(value: &_storage._celebrityRecognitionAnnotations) }()
        case 23: try { try decoder.decodeRepeatedMessageField(value: &_storage._segmentPresenceLabelAnnotations) }()
        case 24: try { try decoder.decodeRepeatedMessageField(value: &_storage._shotPresenceLabelAnnotations) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if !_storage._inputUri.isEmpty {
        try visitor.visitSingularStringField(value: _storage._inputUri, fieldNumber: 1)
      }
      if !_storage._segmentLabelAnnotations.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._segmentLabelAnnotations, fieldNumber: 2)
      }
      if !_storage._shotLabelAnnotations.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._shotLabelAnnotations, fieldNumber: 3)
      }
      if !_storage._frameLabelAnnotations.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._frameLabelAnnotations, fieldNumber: 4)
      }
      if !_storage._shotAnnotations.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._shotAnnotations, fieldNumber: 6)
      }
      if let v = _storage._explicitAnnotation {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
      }
      if let v = _storage._error {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
      }
      if let v = _storage._segment {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 10)
      }
      if !_storage._speechTranscriptions.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._speechTranscriptions, fieldNumber: 11)
      }
      if !_storage._textAnnotations.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._textAnnotations, fieldNumber: 12)
      }
      if !_storage._faceDetectionAnnotations.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._faceDetectionAnnotations, fieldNumber: 13)
      }
      if !_storage._objectAnnotations.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._objectAnnotations, fieldNumber: 14)
      }
      if !_storage._logoRecognitionAnnotations.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._logoRecognitionAnnotations, fieldNumber: 19)
      }
      if !_storage._personDetectionAnnotations.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._personDetectionAnnotations, fieldNumber: 20)
      }
      if let v = _storage._celebrityRecognitionAnnotations {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 21)
      }
      if !_storage._segmentPresenceLabelAnnotations.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._segmentPresenceLabelAnnotations, fieldNumber: 23)
      }
      if !_storage._shotPresenceLabelAnnotations.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._shotPresenceLabelAnnotations, fieldNumber: 24)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_VideoAnnotationResults, rhs: Google_Cloud_Videointelligence_V1p3beta1_VideoAnnotationResults) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._inputUri != rhs_storage._inputUri {return false}
        if _storage._segment != rhs_storage._segment {return false}
        if _storage._segmentLabelAnnotations != rhs_storage._segmentLabelAnnotations {return false}
        if _storage._segmentPresenceLabelAnnotations != rhs_storage._segmentPresenceLabelAnnotations {return false}
        if _storage._shotLabelAnnotations != rhs_storage._shotLabelAnnotations {return false}
        if _storage._shotPresenceLabelAnnotations != rhs_storage._shotPresenceLabelAnnotations {return false}
        if _storage._frameLabelAnnotations != rhs_storage._frameLabelAnnotations {return false}
        if _storage._faceDetectionAnnotations != rhs_storage._faceDetectionAnnotations {return false}
        if _storage._shotAnnotations != rhs_storage._shotAnnotations {return false}
        if _storage._explicitAnnotation != rhs_storage._explicitAnnotation {return false}
        if _storage._speechTranscriptions != rhs_storage._speechTranscriptions {return false}
        if _storage._textAnnotations != rhs_storage._textAnnotations {return false}
        if _storage._objectAnnotations != rhs_storage._objectAnnotations {return false}
        if _storage._logoRecognitionAnnotations != rhs_storage._logoRecognitionAnnotations {return false}
        if _storage._personDetectionAnnotations != rhs_storage._personDetectionAnnotations {return false}
        if _storage._celebrityRecognitionAnnotations != rhs_storage._celebrityRecognitionAnnotations {return false}
        if _storage._error != rhs_storage._error {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_AnnotateVideoResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AnnotateVideoResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "annotation_results"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.annotationResults) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.annotationResults.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.annotationResults, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_AnnotateVideoResponse, rhs: Google_Cloud_Videointelligence_V1p3beta1_AnnotateVideoResponse) -> Bool {
    if lhs.annotationResults != rhs.annotationResults {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_VideoAnnotationProgress: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoAnnotationProgress"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "input_uri"),
    2: .standard(proto: "progress_percent"),
    3: .standard(proto: "start_time"),
    4: .standard(proto: "update_time"),
    5: .same(proto: "feature"),
    6: .same(proto: "segment"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.inputUri) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.progressPercent) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._startTime) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._updateTime) }()
      case 5: try { try decoder.decodeSingularEnumField(value: &self.feature) }()
      case 6: try { try decoder.decodeSingularMessageField(value: &self._segment) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.inputUri.isEmpty {
      try visitor.visitSingularStringField(value: self.inputUri, fieldNumber: 1)
    }
    if self.progressPercent != 0 {
      try visitor.visitSingularInt32Field(value: self.progressPercent, fieldNumber: 2)
    }
    if let v = self._startTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if let v = self._updateTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    if self.feature != .unspecified {
      try visitor.visitSingularEnumField(value: self.feature, fieldNumber: 5)
    }
    if let v = self._segment {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_VideoAnnotationProgress, rhs: Google_Cloud_Videointelligence_V1p3beta1_VideoAnnotationProgress) -> Bool {
    if lhs.inputUri != rhs.inputUri {return false}
    if lhs.progressPercent != rhs.progressPercent {return false}
    if lhs._startTime != rhs._startTime {return false}
    if lhs._updateTime != rhs._updateTime {return false}
    if lhs.feature != rhs.feature {return false}
    if lhs._segment != rhs._segment {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_AnnotateVideoProgress: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AnnotateVideoProgress"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "annotation_progress"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.annotationProgress) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.annotationProgress.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.annotationProgress, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_AnnotateVideoProgress, rhs: Google_Cloud_Videointelligence_V1p3beta1_AnnotateVideoProgress) -> Bool {
    if lhs.annotationProgress != rhs.annotationProgress {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_SpeechTranscriptionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechTranscriptionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "language_code"),
    2: .standard(proto: "max_alternatives"),
    3: .standard(proto: "filter_profanity"),
    4: .standard(proto: "speech_contexts"),
    5: .standard(proto: "enable_automatic_punctuation"),
    6: .standard(proto: "audio_tracks"),
    7: .standard(proto: "enable_speaker_diarization"),
    8: .standard(proto: "diarization_speaker_count"),
    9: .standard(proto: "enable_word_confidence"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.languageCode) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.maxAlternatives) }()
      case 3: try { try decoder.decodeSingularBoolField(value: &self.filterProfanity) }()
      case 4: try { try decoder.decodeRepeatedMessageField(value: &self.speechContexts) }()
      case 5: try { try decoder.decodeSingularBoolField(value: &self.enableAutomaticPunctuation) }()
      case 6: try { try decoder.decodeRepeatedInt32Field(value: &self.audioTracks) }()
      case 7: try { try decoder.decodeSingularBoolField(value: &self.enableSpeakerDiarization) }()
      case 8: try { try decoder.decodeSingularInt32Field(value: &self.diarizationSpeakerCount) }()
      case 9: try { try decoder.decodeSingularBoolField(value: &self.enableWordConfidence) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.languageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.languageCode, fieldNumber: 1)
    }
    if self.maxAlternatives != 0 {
      try visitor.visitSingularInt32Field(value: self.maxAlternatives, fieldNumber: 2)
    }
    if self.filterProfanity != false {
      try visitor.visitSingularBoolField(value: self.filterProfanity, fieldNumber: 3)
    }
    if !self.speechContexts.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.speechContexts, fieldNumber: 4)
    }
    if self.enableAutomaticPunctuation != false {
      try visitor.visitSingularBoolField(value: self.enableAutomaticPunctuation, fieldNumber: 5)
    }
    if !self.audioTracks.isEmpty {
      try visitor.visitPackedInt32Field(value: self.audioTracks, fieldNumber: 6)
    }
    if self.enableSpeakerDiarization != false {
      try visitor.visitSingularBoolField(value: self.enableSpeakerDiarization, fieldNumber: 7)
    }
    if self.diarizationSpeakerCount != 0 {
      try visitor.visitSingularInt32Field(value: self.diarizationSpeakerCount, fieldNumber: 8)
    }
    if self.enableWordConfidence != false {
      try visitor.visitSingularBoolField(value: self.enableWordConfidence, fieldNumber: 9)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_SpeechTranscriptionConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_SpeechTranscriptionConfig) -> Bool {
    if lhs.languageCode != rhs.languageCode {return false}
    if lhs.maxAlternatives != rhs.maxAlternatives {return false}
    if lhs.filterProfanity != rhs.filterProfanity {return false}
    if lhs.speechContexts != rhs.speechContexts {return false}
    if lhs.enableAutomaticPunctuation != rhs.enableAutomaticPunctuation {return false}
    if lhs.audioTracks != rhs.audioTracks {return false}
    if lhs.enableSpeakerDiarization != rhs.enableSpeakerDiarization {return false}
    if lhs.diarizationSpeakerCount != rhs.diarizationSpeakerCount {return false}
    if lhs.enableWordConfidence != rhs.enableWordConfidence {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_SpeechContext: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechContext"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "phrases"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedStringField(value: &self.phrases) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.phrases.isEmpty {
      try visitor.visitRepeatedStringField(value: self.phrases, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_SpeechContext, rhs: Google_Cloud_Videointelligence_V1p3beta1_SpeechContext) -> Bool {
    if lhs.phrases != rhs.phrases {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_SpeechTranscription: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechTranscription"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "alternatives"),
    2: .standard(proto: "language_code"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.alternatives) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.languageCode) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.alternatives.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.alternatives, fieldNumber: 1)
    }
    if !self.languageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.languageCode, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_SpeechTranscription, rhs: Google_Cloud_Videointelligence_V1p3beta1_SpeechTranscription) -> Bool {
    if lhs.alternatives != rhs.alternatives {return false}
    if lhs.languageCode != rhs.languageCode {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_SpeechRecognitionAlternative: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechRecognitionAlternative"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "transcript"),
    2: .same(proto: "confidence"),
    3: .same(proto: "words"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.transcript) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.words) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.transcript.isEmpty {
      try visitor.visitSingularStringField(value: self.transcript, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    if !self.words.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.words, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_SpeechRecognitionAlternative, rhs: Google_Cloud_Videointelligence_V1p3beta1_SpeechRecognitionAlternative) -> Bool {
    if lhs.transcript != rhs.transcript {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.words != rhs.words {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_WordInfo: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".WordInfo"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "start_time"),
    2: .standard(proto: "end_time"),
    3: .same(proto: "word"),
    4: .same(proto: "confidence"),
    5: .standard(proto: "speaker_tag"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._startTime) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._endTime) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.word) }()
      case 4: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      case 5: try { try decoder.decodeSingularInt32Field(value: &self.speakerTag) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._startTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._endTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if !self.word.isEmpty {
      try visitor.visitSingularStringField(value: self.word, fieldNumber: 3)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 4)
    }
    if self.speakerTag != 0 {
      try visitor.visitSingularInt32Field(value: self.speakerTag, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_WordInfo, rhs: Google_Cloud_Videointelligence_V1p3beta1_WordInfo) -> Bool {
    if lhs._startTime != rhs._startTime {return false}
    if lhs._endTime != rhs._endTime {return false}
    if lhs.word != rhs.word {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.speakerTag != rhs.speakerTag {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_NormalizedVertex: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".NormalizedVertex"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "x"),
    2: .same(proto: "y"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularFloatField(value: &self.x) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.y) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.x != 0 {
      try visitor.visitSingularFloatField(value: self.x, fieldNumber: 1)
    }
    if self.y != 0 {
      try visitor.visitSingularFloatField(value: self.y, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_NormalizedVertex, rhs: Google_Cloud_Videointelligence_V1p3beta1_NormalizedVertex) -> Bool {
    if lhs.x != rhs.x {return false}
    if lhs.y != rhs.y {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingPoly: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".NormalizedBoundingPoly"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "vertices"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.vertices) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.vertices.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.vertices, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingPoly, rhs: Google_Cloud_Videointelligence_V1p3beta1_NormalizedBoundingPoly) -> Bool {
    if lhs.vertices != rhs.vertices {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_TextSegment: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TextSegment"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "segment"),
    2: .same(proto: "confidence"),
    3: .same(proto: "frames"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._segment) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.frames) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._segment {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    if !self.frames.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frames, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_TextSegment, rhs: Google_Cloud_Videointelligence_V1p3beta1_TextSegment) -> Bool {
    if lhs._segment != rhs._segment {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.frames != rhs.frames {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_TextFrame: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TextFrame"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "rotated_bounding_box"),
    2: .standard(proto: "time_offset"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._rotatedBoundingBox) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._timeOffset) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._rotatedBoundingBox {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._timeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_TextFrame, rhs: Google_Cloud_Videointelligence_V1p3beta1_TextFrame) -> Bool {
    if lhs._rotatedBoundingBox != rhs._rotatedBoundingBox {return false}
    if lhs._timeOffset != rhs._timeOffset {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_TextAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TextAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "text"),
    2: .same(proto: "segments"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.text) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.segments) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.text.isEmpty {
      try visitor.visitSingularStringField(value: self.text, fieldNumber: 1)
    }
    if !self.segments.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.segments, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_TextAnnotation, rhs: Google_Cloud_Videointelligence_V1p3beta1_TextAnnotation) -> Bool {
    if lhs.text != rhs.text {return false}
    if lhs.segments != rhs.segments {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingFrame: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ObjectTrackingFrame"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "normalized_bounding_box"),
    2: .standard(proto: "time_offset"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._normalizedBoundingBox) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._timeOffset) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._normalizedBoundingBox {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._timeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingFrame, rhs: Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingFrame) -> Bool {
    if lhs._normalizedBoundingBox != rhs._normalizedBoundingBox {return false}
    if lhs._timeOffset != rhs._timeOffset {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ObjectTrackingAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    3: .same(proto: "segment"),
    5: .standard(proto: "track_id"),
    1: .same(proto: "entity"),
    4: .same(proto: "confidence"),
    2: .same(proto: "frames"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._entity) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.frames) }()
      case 3: try {
        var v: Google_Cloud_Videointelligence_V1p3beta1_VideoSegment?
        if let current = self.trackInfo {
          try decoder.handleConflictingOneOf()
          if case .segment(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.trackInfo = .segment(v)}
      }()
      case 4: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      case 5: try {
        if self.trackInfo != nil {try decoder.handleConflictingOneOf()}
        var v: Int64?
        try decoder.decodeSingularInt64Field(value: &v)
        if let v = v {self.trackInfo = .trackID(v)}
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._entity {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if !self.frames.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frames, fieldNumber: 2)
    }
    if case .segment(let v)? = self.trackInfo {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 4)
    }
    if case .trackID(let v)? = self.trackInfo {
      try visitor.visitSingularInt64Field(value: v, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingAnnotation, rhs: Google_Cloud_Videointelligence_V1p3beta1_ObjectTrackingAnnotation) -> Bool {
    if lhs.trackInfo != rhs.trackInfo {return false}
    if lhs._entity != rhs._entity {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.frames != rhs.frames {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_LogoRecognitionAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LogoRecognitionAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "entity"),
    2: .same(proto: "tracks"),
    3: .same(proto: "segments"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._entity) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.tracks) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.segments) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._entity {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if !self.tracks.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.tracks, fieldNumber: 2)
    }
    if !self.segments.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.segments, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_LogoRecognitionAnnotation, rhs: Google_Cloud_Videointelligence_V1p3beta1_LogoRecognitionAnnotation) -> Bool {
    if lhs._entity != rhs._entity {return false}
    if lhs.tracks != rhs.tracks {return false}
    if lhs.segments != rhs.segments {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_StreamingAnnotateVideoRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingAnnotateVideoRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "video_config"),
    2: .standard(proto: "input_content"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        var v: Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoConfig?
        if let current = self.streamingRequest {
          try decoder.handleConflictingOneOf()
          if case .videoConfig(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.streamingRequest = .videoConfig(v)}
      }()
      case 2: try {
        if self.streamingRequest != nil {try decoder.handleConflictingOneOf()}
        var v: Data?
        try decoder.decodeSingularBytesField(value: &v)
        if let v = v {self.streamingRequest = .inputContent(v)}
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.streamingRequest {
    case .videoConfig?: try {
      guard case .videoConfig(let v)? = self.streamingRequest else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }()
    case .inputContent?: try {
      guard case .inputContent(let v)? = self.streamingRequest else { preconditionFailure() }
      try visitor.visitSingularBytesField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingAnnotateVideoRequest, rhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingAnnotateVideoRequest) -> Bool {
    if lhs.streamingRequest != rhs.streamingRequest {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingVideoConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .standard(proto: "shot_change_detection_config"),
    3: .standard(proto: "label_detection_config"),
    4: .standard(proto: "explicit_content_detection_config"),
    5: .standard(proto: "object_tracking_config"),
    23: .standard(proto: "automl_action_recognition_config"),
    21: .standard(proto: "automl_classification_config"),
    22: .standard(proto: "automl_object_tracking_config"),
    1: .same(proto: "feature"),
    30: .standard(proto: "storage_config"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.feature) }()
      case 2: try {
        var v: Google_Cloud_Videointelligence_V1p3beta1_StreamingShotChangeDetectionConfig?
        if let current = self.streamingConfig {
          try decoder.handleConflictingOneOf()
          if case .shotChangeDetectionConfig(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.streamingConfig = .shotChangeDetectionConfig(v)}
      }()
      case 3: try {
        var v: Google_Cloud_Videointelligence_V1p3beta1_StreamingLabelDetectionConfig?
        if let current = self.streamingConfig {
          try decoder.handleConflictingOneOf()
          if case .labelDetectionConfig(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.streamingConfig = .labelDetectionConfig(v)}
      }()
      case 4: try {
        var v: Google_Cloud_Videointelligence_V1p3beta1_StreamingExplicitContentDetectionConfig?
        if let current = self.streamingConfig {
          try decoder.handleConflictingOneOf()
          if case .explicitContentDetectionConfig(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.streamingConfig = .explicitContentDetectionConfig(v)}
      }()
      case 5: try {
        var v: Google_Cloud_Videointelligence_V1p3beta1_StreamingObjectTrackingConfig?
        if let current = self.streamingConfig {
          try decoder.handleConflictingOneOf()
          if case .objectTrackingConfig(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.streamingConfig = .objectTrackingConfig(v)}
      }()
      case 21: try {
        var v: Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlClassificationConfig?
        if let current = self.streamingConfig {
          try decoder.handleConflictingOneOf()
          if case .automlClassificationConfig(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.streamingConfig = .automlClassificationConfig(v)}
      }()
      case 22: try {
        var v: Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlObjectTrackingConfig?
        if let current = self.streamingConfig {
          try decoder.handleConflictingOneOf()
          if case .automlObjectTrackingConfig(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.streamingConfig = .automlObjectTrackingConfig(v)}
      }()
      case 23: try {
        var v: Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlActionRecognitionConfig?
        if let current = self.streamingConfig {
          try decoder.handleConflictingOneOf()
          if case .automlActionRecognitionConfig(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.streamingConfig = .automlActionRecognitionConfig(v)}
      }()
      case 30: try { try decoder.decodeSingularMessageField(value: &self._storageConfig) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.feature != .unspecified {
      try visitor.visitSingularEnumField(value: self.feature, fieldNumber: 1)
    }
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.streamingConfig {
    case .shotChangeDetectionConfig?: try {
      guard case .shotChangeDetectionConfig(let v)? = self.streamingConfig else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case .labelDetectionConfig?: try {
      guard case .labelDetectionConfig(let v)? = self.streamingConfig else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }()
    case .explicitContentDetectionConfig?: try {
      guard case .explicitContentDetectionConfig(let v)? = self.streamingConfig else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }()
    case .objectTrackingConfig?: try {
      guard case .objectTrackingConfig(let v)? = self.streamingConfig else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    }()
    case .automlClassificationConfig?: try {
      guard case .automlClassificationConfig(let v)? = self.streamingConfig else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 21)
    }()
    case .automlObjectTrackingConfig?: try {
      guard case .automlObjectTrackingConfig(let v)? = self.streamingConfig else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 22)
    }()
    case .automlActionRecognitionConfig?: try {
      guard case .automlActionRecognitionConfig(let v)? = self.streamingConfig else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 23)
    }()
    case nil: break
    }
    if let v = self._storageConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 30)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoConfig) -> Bool {
    if lhs.streamingConfig != rhs.streamingConfig {return false}
    if lhs.feature != rhs.feature {return false}
    if lhs._storageConfig != rhs._storageConfig {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_StreamingAnnotateVideoResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingAnnotateVideoResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "error"),
    2: .standard(proto: "annotation_results"),
    3: .standard(proto: "annotation_results_uri"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._error) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._annotationResults) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.annotationResultsUri) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._error {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._annotationResults {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if !self.annotationResultsUri.isEmpty {
      try visitor.visitSingularStringField(value: self.annotationResultsUri, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingAnnotateVideoResponse, rhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingAnnotateVideoResponse) -> Bool {
    if lhs._error != rhs._error {return false}
    if lhs._annotationResults != rhs._annotationResults {return false}
    if lhs.annotationResultsUri != rhs.annotationResultsUri {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoAnnotationResults: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingVideoAnnotationResults"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "shot_annotations"),
    2: .standard(proto: "label_annotations"),
    3: .standard(proto: "explicit_annotation"),
    4: .standard(proto: "object_annotations"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.shotAnnotations) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.labelAnnotations) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._explicitAnnotation) }()
      case 4: try { try decoder.decodeRepeatedMessageField(value: &self.objectAnnotations) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.shotAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.shotAnnotations, fieldNumber: 1)
    }
    if !self.labelAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.labelAnnotations, fieldNumber: 2)
    }
    if let v = self._explicitAnnotation {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if !self.objectAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.objectAnnotations, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoAnnotationResults, rhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingVideoAnnotationResults) -> Bool {
    if lhs.shotAnnotations != rhs.shotAnnotations {return false}
    if lhs.labelAnnotations != rhs.labelAnnotations {return false}
    if lhs._explicitAnnotation != rhs._explicitAnnotation {return false}
    if lhs.objectAnnotations != rhs.objectAnnotations {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_StreamingShotChangeDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingShotChangeDetectionConfig"
  public static let _protobuf_nameMap = SwiftProtobuf._NameMap()

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let _ = try decoder.nextFieldNumber() {
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingShotChangeDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingShotChangeDetectionConfig) -> Bool {
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_StreamingLabelDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingLabelDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "stationary_camera"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularBoolField(value: &self.stationaryCamera) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.stationaryCamera != false {
      try visitor.visitSingularBoolField(value: self.stationaryCamera, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingLabelDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingLabelDetectionConfig) -> Bool {
    if lhs.stationaryCamera != rhs.stationaryCamera {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_StreamingExplicitContentDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingExplicitContentDetectionConfig"
  public static let _protobuf_nameMap = SwiftProtobuf._NameMap()

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let _ = try decoder.nextFieldNumber() {
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingExplicitContentDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingExplicitContentDetectionConfig) -> Bool {
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_StreamingObjectTrackingConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingObjectTrackingConfig"
  public static let _protobuf_nameMap = SwiftProtobuf._NameMap()

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let _ = try decoder.nextFieldNumber() {
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingObjectTrackingConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingObjectTrackingConfig) -> Bool {
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlActionRecognitionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingAutomlActionRecognitionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "model_name"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.modelName) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.modelName.isEmpty {
      try visitor.visitSingularStringField(value: self.modelName, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlActionRecognitionConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlActionRecognitionConfig) -> Bool {
    if lhs.modelName != rhs.modelName {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlClassificationConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingAutomlClassificationConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "model_name"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.modelName) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.modelName.isEmpty {
      try visitor.visitSingularStringField(value: self.modelName, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlClassificationConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlClassificationConfig) -> Bool {
    if lhs.modelName != rhs.modelName {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlObjectTrackingConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingAutomlObjectTrackingConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "model_name"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.modelName) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.modelName.isEmpty {
      try visitor.visitSingularStringField(value: self.modelName, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlObjectTrackingConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingAutomlObjectTrackingConfig) -> Bool {
    if lhs.modelName != rhs.modelName {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p3beta1_StreamingStorageConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingStorageConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "enable_storage_annotation_result"),
    3: .standard(proto: "annotation_result_storage_directory"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularBoolField(value: &self.enableStorageAnnotationResult) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.annotationResultStorageDirectory) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.enableStorageAnnotationResult != false {
      try visitor.visitSingularBoolField(value: self.enableStorageAnnotationResult, fieldNumber: 1)
    }
    if !self.annotationResultStorageDirectory.isEmpty {
      try visitor.visitSingularStringField(value: self.annotationResultStorageDirectory, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingStorageConfig, rhs: Google_Cloud_Videointelligence_V1p3beta1_StreamingStorageConfig) -> Bool {
    if lhs.enableStorageAnnotationResult != rhs.enableStorageAnnotationResult {return false}
    if lhs.annotationResultStorageDirectory != rhs.annotationResultStorageDirectory {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
