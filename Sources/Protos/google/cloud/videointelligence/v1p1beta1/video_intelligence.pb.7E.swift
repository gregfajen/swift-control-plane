// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/videointelligence/v1p1beta1/video_intelligence.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2019 Google LLC.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Video annotation feature.
public enum Google_Cloud_Videointelligence_V1p1beta1_Feature: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Unspecified.
  case unspecified // = 0

  /// Label detection. Detect objects, such as dog or flower.
  case labelDetection // = 1

  /// Shot change detection.
  case shotChangeDetection // = 2

  /// Explicit content detection.
  case explicitContentDetection // = 3

  /// Speech transcription.
  case speechTranscription // = 6
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .labelDetection
    case 2: self = .shotChangeDetection
    case 3: self = .explicitContentDetection
    case 6: self = .speechTranscription
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .labelDetection: return 1
    case .shotChangeDetection: return 2
    case .explicitContentDetection: return 3
    case .speechTranscription: return 6
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Videointelligence_V1p1beta1_Feature: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Videointelligence_V1p1beta1_Feature] = [
    .unspecified,
    .labelDetection,
    .shotChangeDetection,
    .explicitContentDetection,
    .speechTranscription,
  ]
}

#endif  // swift(>=4.2)

/// Label detection mode.
public enum Google_Cloud_Videointelligence_V1p1beta1_LabelDetectionMode: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Unspecified.
  case unspecified // = 0

  /// Detect shot-level labels.
  case shotMode // = 1

  /// Detect frame-level labels.
  case frameMode // = 2

  /// Detect both shot-level and frame-level labels.
  case shotAndFrameMode // = 3
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .shotMode
    case 2: self = .frameMode
    case 3: self = .shotAndFrameMode
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .shotMode: return 1
    case .frameMode: return 2
    case .shotAndFrameMode: return 3
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Videointelligence_V1p1beta1_LabelDetectionMode: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Videointelligence_V1p1beta1_LabelDetectionMode] = [
    .unspecified,
    .shotMode,
    .frameMode,
    .shotAndFrameMode,
  ]
}

#endif  // swift(>=4.2)

/// Bucketized representation of likelihood.
public enum Google_Cloud_Videointelligence_V1p1beta1_Likelihood: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Unspecified likelihood.
  case unspecified // = 0

  /// Very unlikely.
  case veryUnlikely // = 1

  /// Unlikely.
  case unlikely // = 2

  /// Possible.
  case possible // = 3

  /// Likely.
  case likely // = 4

  /// Very likely.
  case veryLikely // = 5
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .veryUnlikely
    case 2: self = .unlikely
    case 3: self = .possible
    case 4: self = .likely
    case 5: self = .veryLikely
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .veryUnlikely: return 1
    case .unlikely: return 2
    case .possible: return 3
    case .likely: return 4
    case .veryLikely: return 5
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Videointelligence_V1p1beta1_Likelihood: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Videointelligence_V1p1beta1_Likelihood] = [
    .unspecified,
    .veryUnlikely,
    .unlikely,
    .possible,
    .likely,
    .veryLikely,
  ]
}

#endif  // swift(>=4.2)

/// Video annotation request.
public struct Google_Cloud_Videointelligence_V1p1beta1_AnnotateVideoRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Input video location. Currently, only
  /// [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
  /// supported, which must be specified in the following format:
  /// `gs://bucket-id/object-id` (other URI formats return
  /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
  /// [Request URIs](https://cloud.google.com/storage/docs/request-endpoints).
  /// A video URI may include wildcards in `object-id`, and thus identify
  /// multiple videos. Supported wildcards: '*' to match 0 or more characters;
  /// '?' to match 1 character. If unset, the input video should be embedded
  /// in the request as `input_content`. If set, `input_content` should be unset.
  public var inputUri: String {
    get {return _storage._inputUri}
    set {_uniqueStorage()._inputUri = newValue}
  }

  /// The video data bytes.
  /// If unset, the input video(s) should be specified via `input_uri`.
  /// If set, `input_uri` should be unset.
  public var inputContent: Data {
    get {return _storage._inputContent}
    set {_uniqueStorage()._inputContent = newValue}
  }

  /// Required. Requested video annotation features.
  public var features: [Google_Cloud_Videointelligence_V1p1beta1_Feature] {
    get {return _storage._features}
    set {_uniqueStorage()._features = newValue}
  }

  /// Additional video context and/or feature-specific parameters.
  public var videoContext: Google_Cloud_Videointelligence_V1p1beta1_VideoContext {
    get {return _storage._videoContext ?? Google_Cloud_Videointelligence_V1p1beta1_VideoContext()}
    set {_uniqueStorage()._videoContext = newValue}
  }
  /// Returns true if `videoContext` has been explicitly set.
  public var hasVideoContext: Bool {return _storage._videoContext != nil}
  /// Clears the value of `videoContext`. Subsequent reads from it will return its default value.
  public mutating func clearVideoContext() {_uniqueStorage()._videoContext = nil}

  /// Optional. Location where the output (in JSON format) should be stored.
  /// Currently, only [Google Cloud Storage](https://cloud.google.com/storage/)
  /// URIs are supported, which must be specified in the following format:
  /// `gs://bucket-id/object-id` (other URI formats return
  /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
  /// [Request URIs](https://cloud.google.com/storage/docs/request-endpoints).
  public var outputUri: String {
    get {return _storage._outputUri}
    set {_uniqueStorage()._outputUri = newValue}
  }

  /// Optional. Cloud region where annotation should take place. Supported cloud
  /// regions: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no region
  /// is specified, a region will be determined based on video file location.
  public var locationID: String {
    get {return _storage._locationID}
    set {_uniqueStorage()._locationID = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Video context and/or feature-specific parameters.
public struct Google_Cloud_Videointelligence_V1p1beta1_VideoContext {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video segments to annotate. The segments may overlap and are not required
  /// to be contiguous or span the whole video. If unspecified, each video is
  /// treated as a single segment.
  public var segments: [Google_Cloud_Videointelligence_V1p1beta1_VideoSegment] = []

  /// Config for LABEL_DETECTION.
  public var labelDetectionConfig: Google_Cloud_Videointelligence_V1p1beta1_LabelDetectionConfig {
    get {return _labelDetectionConfig ?? Google_Cloud_Videointelligence_V1p1beta1_LabelDetectionConfig()}
    set {_labelDetectionConfig = newValue}
  }
  /// Returns true if `labelDetectionConfig` has been explicitly set.
  public var hasLabelDetectionConfig: Bool {return self._labelDetectionConfig != nil}
  /// Clears the value of `labelDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearLabelDetectionConfig() {self._labelDetectionConfig = nil}

  /// Config for SHOT_CHANGE_DETECTION.
  public var shotChangeDetectionConfig: Google_Cloud_Videointelligence_V1p1beta1_ShotChangeDetectionConfig {
    get {return _shotChangeDetectionConfig ?? Google_Cloud_Videointelligence_V1p1beta1_ShotChangeDetectionConfig()}
    set {_shotChangeDetectionConfig = newValue}
  }
  /// Returns true if `shotChangeDetectionConfig` has been explicitly set.
  public var hasShotChangeDetectionConfig: Bool {return self._shotChangeDetectionConfig != nil}
  /// Clears the value of `shotChangeDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearShotChangeDetectionConfig() {self._shotChangeDetectionConfig = nil}

  /// Config for EXPLICIT_CONTENT_DETECTION.
  public var explicitContentDetectionConfig: Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentDetectionConfig {
    get {return _explicitContentDetectionConfig ?? Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentDetectionConfig()}
    set {_explicitContentDetectionConfig = newValue}
  }
  /// Returns true if `explicitContentDetectionConfig` has been explicitly set.
  public var hasExplicitContentDetectionConfig: Bool {return self._explicitContentDetectionConfig != nil}
  /// Clears the value of `explicitContentDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearExplicitContentDetectionConfig() {self._explicitContentDetectionConfig = nil}

  /// Config for SPEECH_TRANSCRIPTION.
  public var speechTranscriptionConfig: Google_Cloud_Videointelligence_V1p1beta1_SpeechTranscriptionConfig {
    get {return _speechTranscriptionConfig ?? Google_Cloud_Videointelligence_V1p1beta1_SpeechTranscriptionConfig()}
    set {_speechTranscriptionConfig = newValue}
  }
  /// Returns true if `speechTranscriptionConfig` has been explicitly set.
  public var hasSpeechTranscriptionConfig: Bool {return self._speechTranscriptionConfig != nil}
  /// Clears the value of `speechTranscriptionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearSpeechTranscriptionConfig() {self._speechTranscriptionConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _labelDetectionConfig: Google_Cloud_Videointelligence_V1p1beta1_LabelDetectionConfig? = nil
  fileprivate var _shotChangeDetectionConfig: Google_Cloud_Videointelligence_V1p1beta1_ShotChangeDetectionConfig? = nil
  fileprivate var _explicitContentDetectionConfig: Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentDetectionConfig? = nil
  fileprivate var _speechTranscriptionConfig: Google_Cloud_Videointelligence_V1p1beta1_SpeechTranscriptionConfig? = nil
}

/// Config for LABEL_DETECTION.
public struct Google_Cloud_Videointelligence_V1p1beta1_LabelDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// What labels should be detected with LABEL_DETECTION, in addition to
  /// video-level labels or segment-level labels.
  /// If unspecified, defaults to `SHOT_MODE`.
  public var labelDetectionMode: Google_Cloud_Videointelligence_V1p1beta1_LabelDetectionMode = .unspecified

  /// Whether the video has been shot from a stationary (i.e. non-moving) camera.
  /// When set to true, might improve detection accuracy for moving objects.
  /// Should be used with `SHOT_AND_FRAME_MODE` enabled.
  public var stationaryCamera: Bool = false

  /// Model to use for label detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for SHOT_CHANGE_DETECTION.
public struct Google_Cloud_Videointelligence_V1p1beta1_ShotChangeDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Model to use for shot change detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for EXPLICIT_CONTENT_DETECTION.
public struct Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Model to use for explicit content detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Video segment.
public struct Google_Cloud_Videointelligence_V1p1beta1_VideoSegment {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Time-offset, relative to the beginning of the video,
  /// corresponding to the start of the segment (inclusive).
  public var startTimeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _startTimeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_startTimeOffset = newValue}
  }
  /// Returns true if `startTimeOffset` has been explicitly set.
  public var hasStartTimeOffset: Bool {return self._startTimeOffset != nil}
  /// Clears the value of `startTimeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearStartTimeOffset() {self._startTimeOffset = nil}

  /// Time-offset, relative to the beginning of the video,
  /// corresponding to the end of the segment (inclusive).
  public var endTimeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _endTimeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_endTimeOffset = newValue}
  }
  /// Returns true if `endTimeOffset` has been explicitly set.
  public var hasEndTimeOffset: Bool {return self._endTimeOffset != nil}
  /// Clears the value of `endTimeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearEndTimeOffset() {self._endTimeOffset = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _startTimeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
  fileprivate var _endTimeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Video segment level annotation results for label detection.
public struct Google_Cloud_Videointelligence_V1p1beta1_LabelSegment {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video segment where a label was detected.
  public var segment: Google_Cloud_Videointelligence_V1p1beta1_VideoSegment {
    get {return _segment ?? Google_Cloud_Videointelligence_V1p1beta1_VideoSegment()}
    set {_segment = newValue}
  }
  /// Returns true if `segment` has been explicitly set.
  public var hasSegment: Bool {return self._segment != nil}
  /// Clears the value of `segment`. Subsequent reads from it will return its default value.
  public mutating func clearSegment() {self._segment = nil}

  /// Confidence that the label is accurate. Range: [0, 1].
  public var confidence: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _segment: Google_Cloud_Videointelligence_V1p1beta1_VideoSegment? = nil
}

/// Video frame level annotation results for label detection.
public struct Google_Cloud_Videointelligence_V1p1beta1_LabelFrame {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Time-offset, relative to the beginning of the video, corresponding to the
  /// video frame for this location.
  public var timeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _timeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_timeOffset = newValue}
  }
  /// Returns true if `timeOffset` has been explicitly set.
  public var hasTimeOffset: Bool {return self._timeOffset != nil}
  /// Clears the value of `timeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearTimeOffset() {self._timeOffset = nil}

  /// Confidence that the label is accurate. Range: [0, 1].
  public var confidence: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _timeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Detected entity from video analysis.
public struct Google_Cloud_Videointelligence_V1p1beta1_Entity {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Opaque entity ID. Some IDs may be available in
  /// [Google Knowledge Graph Search
  /// API](https://developers.google.com/knowledge-graph/).
  public var entityID: String = String()

  /// Textual description, e.g. `Fixed-gear bicycle`.
  public var description_p: String = String()

  /// Language code for `description` in BCP-47 format.
  public var languageCode: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Label annotation.
public struct Google_Cloud_Videointelligence_V1p1beta1_LabelAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Detected entity.
  public var entity: Google_Cloud_Videointelligence_V1p1beta1_Entity {
    get {return _entity ?? Google_Cloud_Videointelligence_V1p1beta1_Entity()}
    set {_entity = newValue}
  }
  /// Returns true if `entity` has been explicitly set.
  public var hasEntity: Bool {return self._entity != nil}
  /// Clears the value of `entity`. Subsequent reads from it will return its default value.
  public mutating func clearEntity() {self._entity = nil}

  /// Common categories for the detected entity.
  /// E.g. when the label is `Terrier` the category is likely `dog`. And in some
  /// cases there might be more than one categories e.g. `Terrier` could also be
  /// a `pet`.
  public var categoryEntities: [Google_Cloud_Videointelligence_V1p1beta1_Entity] = []

  /// All video segments where a label was detected.
  public var segments: [Google_Cloud_Videointelligence_V1p1beta1_LabelSegment] = []

  /// All video frames where a label was detected.
  public var frames: [Google_Cloud_Videointelligence_V1p1beta1_LabelFrame] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _entity: Google_Cloud_Videointelligence_V1p1beta1_Entity? = nil
}

/// Video frame level annotation results for explicit content.
public struct Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentFrame {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Time-offset, relative to the beginning of the video, corresponding to the
  /// video frame for this location.
  public var timeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _timeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_timeOffset = newValue}
  }
  /// Returns true if `timeOffset` has been explicitly set.
  public var hasTimeOffset: Bool {return self._timeOffset != nil}
  /// Clears the value of `timeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearTimeOffset() {self._timeOffset = nil}

  /// Likelihood of the pornography content..
  public var pornographyLikelihood: Google_Cloud_Videointelligence_V1p1beta1_Likelihood = .unspecified

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _timeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Explicit content annotation (based on per-frame visual signals only).
/// If no explicit content has been detected in a frame, no annotations are
/// present for that frame.
public struct Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// All video frames where explicit content was detected.
  public var frames: [Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentFrame] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Annotation results for a single video.
public struct Google_Cloud_Videointelligence_V1p1beta1_VideoAnnotationResults {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. Video file location in
  /// [Google Cloud Storage](https://cloud.google.com/storage/).
  public var inputUri: String = String()

  /// Label annotations on video level or user specified segment level.
  /// There is exactly one element for each unique label.
  public var segmentLabelAnnotations: [Google_Cloud_Videointelligence_V1p1beta1_LabelAnnotation] = []

  /// Label annotations on shot level.
  /// There is exactly one element for each unique label.
  public var shotLabelAnnotations: [Google_Cloud_Videointelligence_V1p1beta1_LabelAnnotation] = []

  /// Label annotations on frame level.
  /// There is exactly one element for each unique label.
  public var frameLabelAnnotations: [Google_Cloud_Videointelligence_V1p1beta1_LabelAnnotation] = []

  /// Shot annotations. Each shot is represented as a video segment.
  public var shotAnnotations: [Google_Cloud_Videointelligence_V1p1beta1_VideoSegment] = []

  /// Explicit content annotation.
  public var explicitAnnotation: Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentAnnotation {
    get {return _explicitAnnotation ?? Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentAnnotation()}
    set {_explicitAnnotation = newValue}
  }
  /// Returns true if `explicitAnnotation` has been explicitly set.
  public var hasExplicitAnnotation: Bool {return self._explicitAnnotation != nil}
  /// Clears the value of `explicitAnnotation`. Subsequent reads from it will return its default value.
  public mutating func clearExplicitAnnotation() {self._explicitAnnotation = nil}

  /// Speech transcription.
  public var speechTranscriptions: [Google_Cloud_Videointelligence_V1p1beta1_SpeechTranscription] = []

  /// Output only. If set, indicates an error. Note that for a single
  /// `AnnotateVideoRequest` some videos may succeed and some may fail.
  public var error: Google_Rpc_Status {
    get {return _error ?? Google_Rpc_Status()}
    set {_error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return self._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {self._error = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _explicitAnnotation: Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentAnnotation? = nil
  fileprivate var _error: Google_Rpc_Status? = nil
}

/// Video annotation response. Included in the `response`
/// field of the `Operation` returned by the `GetOperation`
/// call of the `google::longrunning::Operations` service.
public struct Google_Cloud_Videointelligence_V1p1beta1_AnnotateVideoResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Annotation results for all videos specified in `AnnotateVideoRequest`.
  public var annotationResults: [Google_Cloud_Videointelligence_V1p1beta1_VideoAnnotationResults] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Annotation progress for a single video.
public struct Google_Cloud_Videointelligence_V1p1beta1_VideoAnnotationProgress {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. Video file location in
  /// [Google Cloud Storage](https://cloud.google.com/storage/).
  public var inputUri: String = String()

  /// Output only. Approximate percentage processed thus far. Guaranteed to be
  /// 100 when fully processed.
  public var progressPercent: Int32 = 0

  /// Output only. Time when the request was received.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return self._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {self._startTime = nil}

  /// Output only. Time of the most recent update.
  public var updateTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _updateTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_updateTime = newValue}
  }
  /// Returns true if `updateTime` has been explicitly set.
  public var hasUpdateTime: Bool {return self._updateTime != nil}
  /// Clears the value of `updateTime`. Subsequent reads from it will return its default value.
  public mutating func clearUpdateTime() {self._updateTime = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
  fileprivate var _updateTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
}

/// Video annotation progress. Included in the `metadata`
/// field of the `Operation` returned by the `GetOperation`
/// call of the `google::longrunning::Operations` service.
public struct Google_Cloud_Videointelligence_V1p1beta1_AnnotateVideoProgress {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Progress metadata for all videos specified in `AnnotateVideoRequest`.
  public var annotationProgress: [Google_Cloud_Videointelligence_V1p1beta1_VideoAnnotationProgress] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for SPEECH_TRANSCRIPTION.
public struct Google_Cloud_Videointelligence_V1p1beta1_SpeechTranscriptionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. *Required* The language of the supplied audio as a
  /// [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
  /// Example: "en-US".
  /// See [Language Support](https://cloud.google.com/speech/docs/languages)
  /// for a list of the currently supported language codes.
  public var languageCode: String = String()

  /// Optional. Maximum number of recognition hypotheses to be returned.
  /// Specifically, the maximum number of `SpeechRecognitionAlternative` messages
  /// within each `SpeechTranscription`. The server may return fewer than
  /// `max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will
  /// return a maximum of one. If omitted, will return a maximum of one.
  public var maxAlternatives: Int32 = 0

  /// Optional. If set to `true`, the server will attempt to filter out
  /// profanities, replacing all but the initial character in each filtered word
  /// with asterisks, e.g. "f***". If set to `false` or omitted, profanities
  /// won't be filtered out.
  public var filterProfanity: Bool = false

  /// Optional. A means to provide context to assist the speech recognition.
  public var speechContexts: [Google_Cloud_Videointelligence_V1p1beta1_SpeechContext] = []

  /// Optional. If 'true', adds punctuation to recognition result hypotheses.
  /// This feature is only available in select languages. Setting this for
  /// requests in other languages has no effect at all. The default 'false' value
  /// does not add punctuation to result hypotheses. NOTE: "This is currently
  /// offered as an experimental service, complimentary to all users. In the
  /// future this may be exclusively available as a premium feature."
  public var enableAutomaticPunctuation: Bool = false

  /// Optional. For file formats, such as MXF or MKV, supporting multiple audio
  /// tracks, specify up to two tracks. Default: track 0.
  public var audioTracks: [Int32] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Provides "hints" to the speech recognizer to favor specific words and phrases
/// in the results.
public struct Google_Cloud_Videointelligence_V1p1beta1_SpeechContext {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. A list of strings containing words and phrases "hints" so that
  /// the speech recognition is more likely to recognize them. This can be used
  /// to improve the accuracy for specific words and phrases, for example, if
  /// specific commands are typically spoken by the user. This can also be used
  /// to add additional words to the vocabulary of the recognizer. See
  /// [usage limits](https://cloud.google.com/speech/limits#content).
  public var phrases: [String] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A speech recognition result corresponding to a portion of the audio.
public struct Google_Cloud_Videointelligence_V1p1beta1_SpeechTranscription {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// May contain one or more recognition hypotheses (up to the maximum specified
  /// in `max_alternatives`).  These alternatives are ordered in terms of
  /// accuracy, with the top (first) alternative being the most probable, as
  /// ranked by the recognizer.
  public var alternatives: [Google_Cloud_Videointelligence_V1p1beta1_SpeechRecognitionAlternative] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Alternative hypotheses (a.k.a. n-best list).
public struct Google_Cloud_Videointelligence_V1p1beta1_SpeechRecognitionAlternative {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. Transcript text representing the words that the user spoke.
  public var transcript: String = String()

  /// Output only. The confidence estimate between 0.0 and 1.0. A higher number
  /// indicates an estimated greater likelihood that the recognized words are
  /// correct. This field is set only for the top alternative.
  /// This field is not guaranteed to be accurate and users should not rely on it
  /// to be always provided.
  /// The default of 0.0 is a sentinel value indicating `confidence` was not set.
  public var confidence: Float = 0

  /// Output only. A list of word-specific information for each recognized word.
  public var words: [Google_Cloud_Videointelligence_V1p1beta1_WordInfo] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Word-specific information for recognized words. Word information is only
/// included in the response when certain request parameters are set, such
/// as `enable_word_time_offsets`.
public struct Google_Cloud_Videointelligence_V1p1beta1_WordInfo {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. Time offset relative to the beginning of the audio, and
  /// corresponding to the start of the spoken word. This field is only set if
  /// `enable_word_time_offsets=true` and only in the top hypothesis. This is an
  /// experimental feature and the accuracy of the time offset can vary.
  public var startTime: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _startTime ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return self._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {self._startTime = nil}

  /// Output only. Time offset relative to the beginning of the audio, and
  /// corresponding to the end of the spoken word. This field is only set if
  /// `enable_word_time_offsets=true` and only in the top hypothesis. This is an
  /// experimental feature and the accuracy of the time offset can vary.
  public var endTime: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _endTime ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_endTime = newValue}
  }
  /// Returns true if `endTime` has been explicitly set.
  public var hasEndTime: Bool {return self._endTime != nil}
  /// Clears the value of `endTime`. Subsequent reads from it will return its default value.
  public mutating func clearEndTime() {self._endTime = nil}

  /// Output only. The word corresponding to this set of information.
  public var word: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _startTime: SwiftProtobuf.Google_Protobuf_Duration? = nil
  fileprivate var _endTime: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.videointelligence.v1p1beta1"

extension Google_Cloud_Videointelligence_V1p1beta1_Feature: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "FEATURE_UNSPECIFIED"),
    1: .same(proto: "LABEL_DETECTION"),
    2: .same(proto: "SHOT_CHANGE_DETECTION"),
    3: .same(proto: "EXPLICIT_CONTENT_DETECTION"),
    6: .same(proto: "SPEECH_TRANSCRIPTION"),
  ]
}

extension Google_Cloud_Videointelligence_V1p1beta1_LabelDetectionMode: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "LABEL_DETECTION_MODE_UNSPECIFIED"),
    1: .same(proto: "SHOT_MODE"),
    2: .same(proto: "FRAME_MODE"),
    3: .same(proto: "SHOT_AND_FRAME_MODE"),
  ]
}

extension Google_Cloud_Videointelligence_V1p1beta1_Likelihood: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "LIKELIHOOD_UNSPECIFIED"),
    1: .same(proto: "VERY_UNLIKELY"),
    2: .same(proto: "UNLIKELY"),
    3: .same(proto: "POSSIBLE"),
    4: .same(proto: "LIKELY"),
    5: .same(proto: "VERY_LIKELY"),
  ]
}

extension Google_Cloud_Videointelligence_V1p1beta1_AnnotateVideoRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AnnotateVideoRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "input_uri"),
    6: .standard(proto: "input_content"),
    2: .same(proto: "features"),
    3: .standard(proto: "video_context"),
    4: .standard(proto: "output_uri"),
    5: .standard(proto: "location_id"),
  ]

  fileprivate class _StorageClass {
    var _inputUri: String = String()
    var _inputContent: Data = Data()
    var _features: [Google_Cloud_Videointelligence_V1p1beta1_Feature] = []
    var _videoContext: Google_Cloud_Videointelligence_V1p1beta1_VideoContext? = nil
    var _outputUri: String = String()
    var _locationID: String = String()

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _inputUri = source._inputUri
      _inputContent = source._inputContent
      _features = source._features
      _videoContext = source._videoContext
      _outputUri = source._outputUri
      _locationID = source._locationID
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularStringField(value: &_storage._inputUri) }()
        case 2: try { try decoder.decodeRepeatedEnumField(value: &_storage._features) }()
        case 3: try { try decoder.decodeSingularMessageField(value: &_storage._videoContext) }()
        case 4: try { try decoder.decodeSingularStringField(value: &_storage._outputUri) }()
        case 5: try { try decoder.decodeSingularStringField(value: &_storage._locationID) }()
        case 6: try { try decoder.decodeSingularBytesField(value: &_storage._inputContent) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if !_storage._inputUri.isEmpty {
        try visitor.visitSingularStringField(value: _storage._inputUri, fieldNumber: 1)
      }
      if !_storage._features.isEmpty {
        try visitor.visitPackedEnumField(value: _storage._features, fieldNumber: 2)
      }
      if let v = _storage._videoContext {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      }
      if !_storage._outputUri.isEmpty {
        try visitor.visitSingularStringField(value: _storage._outputUri, fieldNumber: 4)
      }
      if !_storage._locationID.isEmpty {
        try visitor.visitSingularStringField(value: _storage._locationID, fieldNumber: 5)
      }
      if !_storage._inputContent.isEmpty {
        try visitor.visitSingularBytesField(value: _storage._inputContent, fieldNumber: 6)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_AnnotateVideoRequest, rhs: Google_Cloud_Videointelligence_V1p1beta1_AnnotateVideoRequest) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._inputUri != rhs_storage._inputUri {return false}
        if _storage._inputContent != rhs_storage._inputContent {return false}
        if _storage._features != rhs_storage._features {return false}
        if _storage._videoContext != rhs_storage._videoContext {return false}
        if _storage._outputUri != rhs_storage._outputUri {return false}
        if _storage._locationID != rhs_storage._locationID {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_VideoContext: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoContext"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "segments"),
    2: .standard(proto: "label_detection_config"),
    3: .standard(proto: "shot_change_detection_config"),
    4: .standard(proto: "explicit_content_detection_config"),
    6: .standard(proto: "speech_transcription_config"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.segments) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._labelDetectionConfig) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._shotChangeDetectionConfig) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._explicitContentDetectionConfig) }()
      case 6: try { try decoder.decodeSingularMessageField(value: &self._speechTranscriptionConfig) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.segments.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.segments, fieldNumber: 1)
    }
    if let v = self._labelDetectionConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if let v = self._shotChangeDetectionConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if let v = self._explicitContentDetectionConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    if let v = self._speechTranscriptionConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_VideoContext, rhs: Google_Cloud_Videointelligence_V1p1beta1_VideoContext) -> Bool {
    if lhs.segments != rhs.segments {return false}
    if lhs._labelDetectionConfig != rhs._labelDetectionConfig {return false}
    if lhs._shotChangeDetectionConfig != rhs._shotChangeDetectionConfig {return false}
    if lhs._explicitContentDetectionConfig != rhs._explicitContentDetectionConfig {return false}
    if lhs._speechTranscriptionConfig != rhs._speechTranscriptionConfig {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_LabelDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "label_detection_mode"),
    2: .standard(proto: "stationary_camera"),
    3: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.labelDetectionMode) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.stationaryCamera) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.model) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.labelDetectionMode != .unspecified {
      try visitor.visitSingularEnumField(value: self.labelDetectionMode, fieldNumber: 1)
    }
    if self.stationaryCamera != false {
      try visitor.visitSingularBoolField(value: self.stationaryCamera, fieldNumber: 2)
    }
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_LabelDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p1beta1_LabelDetectionConfig) -> Bool {
    if lhs.labelDetectionMode != rhs.labelDetectionMode {return false}
    if lhs.stationaryCamera != rhs.stationaryCamera {return false}
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_ShotChangeDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ShotChangeDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.model) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_ShotChangeDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p1beta1_ShotChangeDetectionConfig) -> Bool {
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ExplicitContentDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.model) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentDetectionConfig) -> Bool {
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_VideoSegment: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoSegment"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "start_time_offset"),
    2: .standard(proto: "end_time_offset"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._startTimeOffset) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._endTimeOffset) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._startTimeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._endTimeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_VideoSegment, rhs: Google_Cloud_Videointelligence_V1p1beta1_VideoSegment) -> Bool {
    if lhs._startTimeOffset != rhs._startTimeOffset {return false}
    if lhs._endTimeOffset != rhs._endTimeOffset {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_LabelSegment: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelSegment"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "segment"),
    2: .same(proto: "confidence"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._segment) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._segment {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_LabelSegment, rhs: Google_Cloud_Videointelligence_V1p1beta1_LabelSegment) -> Bool {
    if lhs._segment != rhs._segment {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_LabelFrame: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelFrame"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "time_offset"),
    2: .same(proto: "confidence"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._timeOffset) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._timeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_LabelFrame, rhs: Google_Cloud_Videointelligence_V1p1beta1_LabelFrame) -> Bool {
    if lhs._timeOffset != rhs._timeOffset {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_Entity: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".Entity"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "entity_id"),
    2: .same(proto: "description"),
    3: .standard(proto: "language_code"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.entityID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.description_p) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.languageCode) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.entityID.isEmpty {
      try visitor.visitSingularStringField(value: self.entityID, fieldNumber: 1)
    }
    if !self.description_p.isEmpty {
      try visitor.visitSingularStringField(value: self.description_p, fieldNumber: 2)
    }
    if !self.languageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.languageCode, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_Entity, rhs: Google_Cloud_Videointelligence_V1p1beta1_Entity) -> Bool {
    if lhs.entityID != rhs.entityID {return false}
    if lhs.description_p != rhs.description_p {return false}
    if lhs.languageCode != rhs.languageCode {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_LabelAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "entity"),
    2: .standard(proto: "category_entities"),
    3: .same(proto: "segments"),
    4: .same(proto: "frames"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._entity) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.categoryEntities) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.segments) }()
      case 4: try { try decoder.decodeRepeatedMessageField(value: &self.frames) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._entity {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if !self.categoryEntities.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.categoryEntities, fieldNumber: 2)
    }
    if !self.segments.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.segments, fieldNumber: 3)
    }
    if !self.frames.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frames, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_LabelAnnotation, rhs: Google_Cloud_Videointelligence_V1p1beta1_LabelAnnotation) -> Bool {
    if lhs._entity != rhs._entity {return false}
    if lhs.categoryEntities != rhs.categoryEntities {return false}
    if lhs.segments != rhs.segments {return false}
    if lhs.frames != rhs.frames {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentFrame: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ExplicitContentFrame"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "time_offset"),
    2: .standard(proto: "pornography_likelihood"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._timeOffset) }()
      case 2: try { try decoder.decodeSingularEnumField(value: &self.pornographyLikelihood) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._timeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.pornographyLikelihood != .unspecified {
      try visitor.visitSingularEnumField(value: self.pornographyLikelihood, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentFrame, rhs: Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentFrame) -> Bool {
    if lhs._timeOffset != rhs._timeOffset {return false}
    if lhs.pornographyLikelihood != rhs.pornographyLikelihood {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ExplicitContentAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "frames"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.frames) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.frames.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frames, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentAnnotation, rhs: Google_Cloud_Videointelligence_V1p1beta1_ExplicitContentAnnotation) -> Bool {
    if lhs.frames != rhs.frames {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_VideoAnnotationResults: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoAnnotationResults"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "input_uri"),
    2: .standard(proto: "segment_label_annotations"),
    3: .standard(proto: "shot_label_annotations"),
    4: .standard(proto: "frame_label_annotations"),
    6: .standard(proto: "shot_annotations"),
    7: .standard(proto: "explicit_annotation"),
    11: .standard(proto: "speech_transcriptions"),
    9: .same(proto: "error"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.inputUri) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.segmentLabelAnnotations) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.shotLabelAnnotations) }()
      case 4: try { try decoder.decodeRepeatedMessageField(value: &self.frameLabelAnnotations) }()
      case 6: try { try decoder.decodeRepeatedMessageField(value: &self.shotAnnotations) }()
      case 7: try { try decoder.decodeSingularMessageField(value: &self._explicitAnnotation) }()
      case 9: try { try decoder.decodeSingularMessageField(value: &self._error) }()
      case 11: try { try decoder.decodeRepeatedMessageField(value: &self.speechTranscriptions) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.inputUri.isEmpty {
      try visitor.visitSingularStringField(value: self.inputUri, fieldNumber: 1)
    }
    if !self.segmentLabelAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.segmentLabelAnnotations, fieldNumber: 2)
    }
    if !self.shotLabelAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.shotLabelAnnotations, fieldNumber: 3)
    }
    if !self.frameLabelAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frameLabelAnnotations, fieldNumber: 4)
    }
    if !self.shotAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.shotAnnotations, fieldNumber: 6)
    }
    if let v = self._explicitAnnotation {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
    }
    if let v = self._error {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
    }
    if !self.speechTranscriptions.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.speechTranscriptions, fieldNumber: 11)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_VideoAnnotationResults, rhs: Google_Cloud_Videointelligence_V1p1beta1_VideoAnnotationResults) -> Bool {
    if lhs.inputUri != rhs.inputUri {return false}
    if lhs.segmentLabelAnnotations != rhs.segmentLabelAnnotations {return false}
    if lhs.shotLabelAnnotations != rhs.shotLabelAnnotations {return false}
    if lhs.frameLabelAnnotations != rhs.frameLabelAnnotations {return false}
    if lhs.shotAnnotations != rhs.shotAnnotations {return false}
    if lhs._explicitAnnotation != rhs._explicitAnnotation {return false}
    if lhs.speechTranscriptions != rhs.speechTranscriptions {return false}
    if lhs._error != rhs._error {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_AnnotateVideoResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AnnotateVideoResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "annotation_results"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.annotationResults) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.annotationResults.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.annotationResults, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_AnnotateVideoResponse, rhs: Google_Cloud_Videointelligence_V1p1beta1_AnnotateVideoResponse) -> Bool {
    if lhs.annotationResults != rhs.annotationResults {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_VideoAnnotationProgress: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoAnnotationProgress"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "input_uri"),
    2: .standard(proto: "progress_percent"),
    3: .standard(proto: "start_time"),
    4: .standard(proto: "update_time"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.inputUri) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.progressPercent) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._startTime) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._updateTime) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.inputUri.isEmpty {
      try visitor.visitSingularStringField(value: self.inputUri, fieldNumber: 1)
    }
    if self.progressPercent != 0 {
      try visitor.visitSingularInt32Field(value: self.progressPercent, fieldNumber: 2)
    }
    if let v = self._startTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if let v = self._updateTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_VideoAnnotationProgress, rhs: Google_Cloud_Videointelligence_V1p1beta1_VideoAnnotationProgress) -> Bool {
    if lhs.inputUri != rhs.inputUri {return false}
    if lhs.progressPercent != rhs.progressPercent {return false}
    if lhs._startTime != rhs._startTime {return false}
    if lhs._updateTime != rhs._updateTime {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_AnnotateVideoProgress: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AnnotateVideoProgress"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "annotation_progress"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.annotationProgress) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.annotationProgress.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.annotationProgress, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_AnnotateVideoProgress, rhs: Google_Cloud_Videointelligence_V1p1beta1_AnnotateVideoProgress) -> Bool {
    if lhs.annotationProgress != rhs.annotationProgress {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_SpeechTranscriptionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechTranscriptionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "language_code"),
    2: .standard(proto: "max_alternatives"),
    3: .standard(proto: "filter_profanity"),
    4: .standard(proto: "speech_contexts"),
    5: .standard(proto: "enable_automatic_punctuation"),
    6: .standard(proto: "audio_tracks"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.languageCode) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.maxAlternatives) }()
      case 3: try { try decoder.decodeSingularBoolField(value: &self.filterProfanity) }()
      case 4: try { try decoder.decodeRepeatedMessageField(value: &self.speechContexts) }()
      case 5: try { try decoder.decodeSingularBoolField(value: &self.enableAutomaticPunctuation) }()
      case 6: try { try decoder.decodeRepeatedInt32Field(value: &self.audioTracks) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.languageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.languageCode, fieldNumber: 1)
    }
    if self.maxAlternatives != 0 {
      try visitor.visitSingularInt32Field(value: self.maxAlternatives, fieldNumber: 2)
    }
    if self.filterProfanity != false {
      try visitor.visitSingularBoolField(value: self.filterProfanity, fieldNumber: 3)
    }
    if !self.speechContexts.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.speechContexts, fieldNumber: 4)
    }
    if self.enableAutomaticPunctuation != false {
      try visitor.visitSingularBoolField(value: self.enableAutomaticPunctuation, fieldNumber: 5)
    }
    if !self.audioTracks.isEmpty {
      try visitor.visitPackedInt32Field(value: self.audioTracks, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_SpeechTranscriptionConfig, rhs: Google_Cloud_Videointelligence_V1p1beta1_SpeechTranscriptionConfig) -> Bool {
    if lhs.languageCode != rhs.languageCode {return false}
    if lhs.maxAlternatives != rhs.maxAlternatives {return false}
    if lhs.filterProfanity != rhs.filterProfanity {return false}
    if lhs.speechContexts != rhs.speechContexts {return false}
    if lhs.enableAutomaticPunctuation != rhs.enableAutomaticPunctuation {return false}
    if lhs.audioTracks != rhs.audioTracks {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_SpeechContext: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechContext"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "phrases"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedStringField(value: &self.phrases) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.phrases.isEmpty {
      try visitor.visitRepeatedStringField(value: self.phrases, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_SpeechContext, rhs: Google_Cloud_Videointelligence_V1p1beta1_SpeechContext) -> Bool {
    if lhs.phrases != rhs.phrases {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_SpeechTranscription: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechTranscription"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "alternatives"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.alternatives) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.alternatives.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.alternatives, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_SpeechTranscription, rhs: Google_Cloud_Videointelligence_V1p1beta1_SpeechTranscription) -> Bool {
    if lhs.alternatives != rhs.alternatives {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_SpeechRecognitionAlternative: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechRecognitionAlternative"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "transcript"),
    2: .same(proto: "confidence"),
    3: .same(proto: "words"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.transcript) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.words) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.transcript.isEmpty {
      try visitor.visitSingularStringField(value: self.transcript, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    if !self.words.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.words, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_SpeechRecognitionAlternative, rhs: Google_Cloud_Videointelligence_V1p1beta1_SpeechRecognitionAlternative) -> Bool {
    if lhs.transcript != rhs.transcript {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.words != rhs.words {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p1beta1_WordInfo: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".WordInfo"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "start_time"),
    2: .standard(proto: "end_time"),
    3: .same(proto: "word"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._startTime) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._endTime) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.word) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._startTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._endTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if !self.word.isEmpty {
      try visitor.visitSingularStringField(value: self.word, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p1beta1_WordInfo, rhs: Google_Cloud_Videointelligence_V1p1beta1_WordInfo) -> Bool {
    if lhs._startTime != rhs._startTime {return false}
    if lhs._endTime != rhs._endTime {return false}
    if lhs.word != rhs.word {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
