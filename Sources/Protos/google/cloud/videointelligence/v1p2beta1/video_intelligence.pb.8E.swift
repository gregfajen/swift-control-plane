// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/videointelligence/v1p2beta1/video_intelligence.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2019 Google LLC.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Video annotation feature.
public enum Google_Cloud_Videointelligence_V1p2beta1_Feature: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Unspecified.
  case unspecified // = 0

  /// Label detection. Detect objects, such as dog or flower.
  case labelDetection // = 1

  /// Shot change detection.
  case shotChangeDetection // = 2

  /// Explicit content detection.
  case explicitContentDetection // = 3

  /// OCR text detection and tracking.
  case textDetection // = 7

  /// Object detection and tracking.
  case objectTracking // = 9
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .labelDetection
    case 2: self = .shotChangeDetection
    case 3: self = .explicitContentDetection
    case 7: self = .textDetection
    case 9: self = .objectTracking
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .labelDetection: return 1
    case .shotChangeDetection: return 2
    case .explicitContentDetection: return 3
    case .textDetection: return 7
    case .objectTracking: return 9
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Videointelligence_V1p2beta1_Feature: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Videointelligence_V1p2beta1_Feature] = [
    .unspecified,
    .labelDetection,
    .shotChangeDetection,
    .explicitContentDetection,
    .textDetection,
    .objectTracking,
  ]
}

#endif  // swift(>=4.2)

/// Label detection mode.
public enum Google_Cloud_Videointelligence_V1p2beta1_LabelDetectionMode: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Unspecified.
  case unspecified // = 0

  /// Detect shot-level labels.
  case shotMode // = 1

  /// Detect frame-level labels.
  case frameMode // = 2

  /// Detect both shot-level and frame-level labels.
  case shotAndFrameMode // = 3
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .shotMode
    case 2: self = .frameMode
    case 3: self = .shotAndFrameMode
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .shotMode: return 1
    case .frameMode: return 2
    case .shotAndFrameMode: return 3
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Videointelligence_V1p2beta1_LabelDetectionMode: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Videointelligence_V1p2beta1_LabelDetectionMode] = [
    .unspecified,
    .shotMode,
    .frameMode,
    .shotAndFrameMode,
  ]
}

#endif  // swift(>=4.2)

/// Bucketized representation of likelihood.
public enum Google_Cloud_Videointelligence_V1p2beta1_Likelihood: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Unspecified likelihood.
  case unspecified // = 0

  /// Very unlikely.
  case veryUnlikely // = 1

  /// Unlikely.
  case unlikely // = 2

  /// Possible.
  case possible // = 3

  /// Likely.
  case likely // = 4

  /// Very likely.
  case veryLikely // = 5
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .veryUnlikely
    case 2: self = .unlikely
    case 3: self = .possible
    case 4: self = .likely
    case 5: self = .veryLikely
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .veryUnlikely: return 1
    case .unlikely: return 2
    case .possible: return 3
    case .likely: return 4
    case .veryLikely: return 5
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Videointelligence_V1p2beta1_Likelihood: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Videointelligence_V1p2beta1_Likelihood] = [
    .unspecified,
    .veryUnlikely,
    .unlikely,
    .possible,
    .likely,
    .veryLikely,
  ]
}

#endif  // swift(>=4.2)

/// Video annotation request.
public struct Google_Cloud_Videointelligence_V1p2beta1_AnnotateVideoRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Input video location. Currently, only
  /// [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
  /// supported, which must be specified in the following format:
  /// `gs://bucket-id/object-id` (other URI formats return
  /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
  /// [Request URIs](https://cloud.google.com/storage/docs/request-endpoints).
  /// A video URI may include wildcards in `object-id`, and thus identify
  /// multiple videos. Supported wildcards: '*' to match 0 or more characters;
  /// '?' to match 1 character. If unset, the input video should be embedded
  /// in the request as `input_content`. If set, `input_content` should be unset.
  public var inputUri: String = String()

  /// The video data bytes.
  /// If unset, the input video(s) should be specified via `input_uri`.
  /// If set, `input_uri` should be unset.
  public var inputContent: Data = Data()

  /// Required. Requested video annotation features.
  public var features: [Google_Cloud_Videointelligence_V1p2beta1_Feature] = []

  /// Additional video context and/or feature-specific parameters.
  public var videoContext: Google_Cloud_Videointelligence_V1p2beta1_VideoContext {
    get {return _videoContext ?? Google_Cloud_Videointelligence_V1p2beta1_VideoContext()}
    set {_videoContext = newValue}
  }
  /// Returns true if `videoContext` has been explicitly set.
  public var hasVideoContext: Bool {return self._videoContext != nil}
  /// Clears the value of `videoContext`. Subsequent reads from it will return its default value.
  public mutating func clearVideoContext() {self._videoContext = nil}

  /// Optional. Location where the output (in JSON format) should be stored.
  /// Currently, only [Google Cloud Storage](https://cloud.google.com/storage/)
  /// URIs are supported, which must be specified in the following format:
  /// `gs://bucket-id/object-id` (other URI formats return
  /// [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For more information, see
  /// [Request URIs](https://cloud.google.com/storage/docs/request-endpoints).
  public var outputUri: String = String()

  /// Optional. Cloud region where annotation should take place. Supported cloud
  /// regions: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no region
  /// is specified, a region will be determined based on video file location.
  public var locationID: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _videoContext: Google_Cloud_Videointelligence_V1p2beta1_VideoContext? = nil
}

/// Video context and/or feature-specific parameters.
public struct Google_Cloud_Videointelligence_V1p2beta1_VideoContext {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video segments to annotate. The segments may overlap and are not required
  /// to be contiguous or span the whole video. If unspecified, each video is
  /// treated as a single segment.
  public var segments: [Google_Cloud_Videointelligence_V1p2beta1_VideoSegment] = []

  /// Config for LABEL_DETECTION.
  public var labelDetectionConfig: Google_Cloud_Videointelligence_V1p2beta1_LabelDetectionConfig {
    get {return _labelDetectionConfig ?? Google_Cloud_Videointelligence_V1p2beta1_LabelDetectionConfig()}
    set {_labelDetectionConfig = newValue}
  }
  /// Returns true if `labelDetectionConfig` has been explicitly set.
  public var hasLabelDetectionConfig: Bool {return self._labelDetectionConfig != nil}
  /// Clears the value of `labelDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearLabelDetectionConfig() {self._labelDetectionConfig = nil}

  /// Config for SHOT_CHANGE_DETECTION.
  public var shotChangeDetectionConfig: Google_Cloud_Videointelligence_V1p2beta1_ShotChangeDetectionConfig {
    get {return _shotChangeDetectionConfig ?? Google_Cloud_Videointelligence_V1p2beta1_ShotChangeDetectionConfig()}
    set {_shotChangeDetectionConfig = newValue}
  }
  /// Returns true if `shotChangeDetectionConfig` has been explicitly set.
  public var hasShotChangeDetectionConfig: Bool {return self._shotChangeDetectionConfig != nil}
  /// Clears the value of `shotChangeDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearShotChangeDetectionConfig() {self._shotChangeDetectionConfig = nil}

  /// Config for EXPLICIT_CONTENT_DETECTION.
  public var explicitContentDetectionConfig: Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentDetectionConfig {
    get {return _explicitContentDetectionConfig ?? Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentDetectionConfig()}
    set {_explicitContentDetectionConfig = newValue}
  }
  /// Returns true if `explicitContentDetectionConfig` has been explicitly set.
  public var hasExplicitContentDetectionConfig: Bool {return self._explicitContentDetectionConfig != nil}
  /// Clears the value of `explicitContentDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearExplicitContentDetectionConfig() {self._explicitContentDetectionConfig = nil}

  /// Config for TEXT_DETECTION.
  public var textDetectionConfig: Google_Cloud_Videointelligence_V1p2beta1_TextDetectionConfig {
    get {return _textDetectionConfig ?? Google_Cloud_Videointelligence_V1p2beta1_TextDetectionConfig()}
    set {_textDetectionConfig = newValue}
  }
  /// Returns true if `textDetectionConfig` has been explicitly set.
  public var hasTextDetectionConfig: Bool {return self._textDetectionConfig != nil}
  /// Clears the value of `textDetectionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearTextDetectionConfig() {self._textDetectionConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _labelDetectionConfig: Google_Cloud_Videointelligence_V1p2beta1_LabelDetectionConfig? = nil
  fileprivate var _shotChangeDetectionConfig: Google_Cloud_Videointelligence_V1p2beta1_ShotChangeDetectionConfig? = nil
  fileprivate var _explicitContentDetectionConfig: Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentDetectionConfig? = nil
  fileprivate var _textDetectionConfig: Google_Cloud_Videointelligence_V1p2beta1_TextDetectionConfig? = nil
}

/// Config for LABEL_DETECTION.
public struct Google_Cloud_Videointelligence_V1p2beta1_LabelDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// What labels should be detected with LABEL_DETECTION, in addition to
  /// video-level labels or segment-level labels.
  /// If unspecified, defaults to `SHOT_MODE`.
  public var labelDetectionMode: Google_Cloud_Videointelligence_V1p2beta1_LabelDetectionMode = .unspecified

  /// Whether the video has been shot from a stationary (i.e. non-moving) camera.
  /// When set to true, might improve detection accuracy for moving objects.
  /// Should be used with `SHOT_AND_FRAME_MODE` enabled.
  public var stationaryCamera: Bool = false

  /// Model to use for label detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for SHOT_CHANGE_DETECTION.
public struct Google_Cloud_Videointelligence_V1p2beta1_ShotChangeDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Model to use for shot change detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for EXPLICIT_CONTENT_DETECTION.
public struct Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Model to use for explicit content detection.
  /// Supported values: "builtin/stable" (the default if unset) and
  /// "builtin/latest".
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config for TEXT_DETECTION.
public struct Google_Cloud_Videointelligence_V1p2beta1_TextDetectionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Language hint can be specified if the language to be detected is known a
  /// priori. It can increase the accuracy of the detection. Language hint must
  /// be language code in BCP-47 format.
  ///
  /// Automatic language detection is performed if no hint is provided.
  public var languageHints: [String] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Video segment.
public struct Google_Cloud_Videointelligence_V1p2beta1_VideoSegment {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Time-offset, relative to the beginning of the video,
  /// corresponding to the start of the segment (inclusive).
  public var startTimeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _startTimeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_startTimeOffset = newValue}
  }
  /// Returns true if `startTimeOffset` has been explicitly set.
  public var hasStartTimeOffset: Bool {return self._startTimeOffset != nil}
  /// Clears the value of `startTimeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearStartTimeOffset() {self._startTimeOffset = nil}

  /// Time-offset, relative to the beginning of the video,
  /// corresponding to the end of the segment (inclusive).
  public var endTimeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _endTimeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_endTimeOffset = newValue}
  }
  /// Returns true if `endTimeOffset` has been explicitly set.
  public var hasEndTimeOffset: Bool {return self._endTimeOffset != nil}
  /// Clears the value of `endTimeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearEndTimeOffset() {self._endTimeOffset = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _startTimeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
  fileprivate var _endTimeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Video segment level annotation results for label detection.
public struct Google_Cloud_Videointelligence_V1p2beta1_LabelSegment {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video segment where a label was detected.
  public var segment: Google_Cloud_Videointelligence_V1p2beta1_VideoSegment {
    get {return _segment ?? Google_Cloud_Videointelligence_V1p2beta1_VideoSegment()}
    set {_segment = newValue}
  }
  /// Returns true if `segment` has been explicitly set.
  public var hasSegment: Bool {return self._segment != nil}
  /// Clears the value of `segment`. Subsequent reads from it will return its default value.
  public mutating func clearSegment() {self._segment = nil}

  /// Confidence that the label is accurate. Range: [0, 1].
  public var confidence: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _segment: Google_Cloud_Videointelligence_V1p2beta1_VideoSegment? = nil
}

/// Video frame level annotation results for label detection.
public struct Google_Cloud_Videointelligence_V1p2beta1_LabelFrame {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Time-offset, relative to the beginning of the video, corresponding to the
  /// video frame for this location.
  public var timeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _timeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_timeOffset = newValue}
  }
  /// Returns true if `timeOffset` has been explicitly set.
  public var hasTimeOffset: Bool {return self._timeOffset != nil}
  /// Clears the value of `timeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearTimeOffset() {self._timeOffset = nil}

  /// Confidence that the label is accurate. Range: [0, 1].
  public var confidence: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _timeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Detected entity from video analysis.
public struct Google_Cloud_Videointelligence_V1p2beta1_Entity {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Opaque entity ID. Some IDs may be available in
  /// [Google Knowledge Graph Search
  /// API](https://developers.google.com/knowledge-graph/).
  public var entityID: String = String()

  /// Textual description, e.g. `Fixed-gear bicycle`.
  public var description_p: String = String()

  /// Language code for `description` in BCP-47 format.
  public var languageCode: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Label annotation.
public struct Google_Cloud_Videointelligence_V1p2beta1_LabelAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Detected entity.
  public var entity: Google_Cloud_Videointelligence_V1p2beta1_Entity {
    get {return _entity ?? Google_Cloud_Videointelligence_V1p2beta1_Entity()}
    set {_entity = newValue}
  }
  /// Returns true if `entity` has been explicitly set.
  public var hasEntity: Bool {return self._entity != nil}
  /// Clears the value of `entity`. Subsequent reads from it will return its default value.
  public mutating func clearEntity() {self._entity = nil}

  /// Common categories for the detected entity.
  /// E.g. when the label is `Terrier` the category is likely `dog`. And in some
  /// cases there might be more than one categories e.g. `Terrier` could also be
  /// a `pet`.
  public var categoryEntities: [Google_Cloud_Videointelligence_V1p2beta1_Entity] = []

  /// All video segments where a label was detected.
  public var segments: [Google_Cloud_Videointelligence_V1p2beta1_LabelSegment] = []

  /// All video frames where a label was detected.
  public var frames: [Google_Cloud_Videointelligence_V1p2beta1_LabelFrame] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _entity: Google_Cloud_Videointelligence_V1p2beta1_Entity? = nil
}

/// Video frame level annotation results for explicit content.
public struct Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentFrame {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Time-offset, relative to the beginning of the video, corresponding to the
  /// video frame for this location.
  public var timeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _timeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_timeOffset = newValue}
  }
  /// Returns true if `timeOffset` has been explicitly set.
  public var hasTimeOffset: Bool {return self._timeOffset != nil}
  /// Clears the value of `timeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearTimeOffset() {self._timeOffset = nil}

  /// Likelihood of the pornography content..
  public var pornographyLikelihood: Google_Cloud_Videointelligence_V1p2beta1_Likelihood = .unspecified

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _timeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Explicit content annotation (based on per-frame visual signals only).
/// If no explicit content has been detected in a frame, no annotations are
/// present for that frame.
public struct Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// All video frames where explicit content was detected.
  public var frames: [Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentFrame] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Normalized bounding box.
/// The normalized vertex coordinates are relative to the original image.
/// Range: [0, 1].
public struct Google_Cloud_Videointelligence_V1p2beta1_NormalizedBoundingBox {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Left X coordinate.
  public var left: Float = 0

  /// Top Y coordinate.
  public var top: Float = 0

  /// Right X coordinate.
  public var right: Float = 0

  /// Bottom Y coordinate.
  public var bottom: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Annotation results for a single video.
public struct Google_Cloud_Videointelligence_V1p2beta1_VideoAnnotationResults {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video file location in
  /// [Google Cloud Storage](https://cloud.google.com/storage/).
  public var inputUri: String = String()

  /// Label annotations on video level or user specified segment level.
  /// There is exactly one element for each unique label.
  public var segmentLabelAnnotations: [Google_Cloud_Videointelligence_V1p2beta1_LabelAnnotation] = []

  /// Label annotations on shot level.
  /// There is exactly one element for each unique label.
  public var shotLabelAnnotations: [Google_Cloud_Videointelligence_V1p2beta1_LabelAnnotation] = []

  /// Label annotations on frame level.
  /// There is exactly one element for each unique label.
  public var frameLabelAnnotations: [Google_Cloud_Videointelligence_V1p2beta1_LabelAnnotation] = []

  /// Shot annotations. Each shot is represented as a video segment.
  public var shotAnnotations: [Google_Cloud_Videointelligence_V1p2beta1_VideoSegment] = []

  /// Explicit content annotation.
  public var explicitAnnotation: Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentAnnotation {
    get {return _explicitAnnotation ?? Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentAnnotation()}
    set {_explicitAnnotation = newValue}
  }
  /// Returns true if `explicitAnnotation` has been explicitly set.
  public var hasExplicitAnnotation: Bool {return self._explicitAnnotation != nil}
  /// Clears the value of `explicitAnnotation`. Subsequent reads from it will return its default value.
  public mutating func clearExplicitAnnotation() {self._explicitAnnotation = nil}

  /// OCR text detection and tracking.
  /// Annotations for list of detected text snippets. Each will have list of
  /// frame information associated with it.
  public var textAnnotations: [Google_Cloud_Videointelligence_V1p2beta1_TextAnnotation] = []

  /// Annotations for list of objects detected and tracked in video.
  public var objectAnnotations: [Google_Cloud_Videointelligence_V1p2beta1_ObjectTrackingAnnotation] = []

  /// If set, indicates an error. Note that for a single `AnnotateVideoRequest`
  /// some videos may succeed and some may fail.
  public var error: Google_Rpc_Status {
    get {return _error ?? Google_Rpc_Status()}
    set {_error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return self._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {self._error = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _explicitAnnotation: Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentAnnotation? = nil
  fileprivate var _error: Google_Rpc_Status? = nil
}

/// Video annotation response. Included in the `response`
/// field of the `Operation` returned by the `GetOperation`
/// call of the `google::longrunning::Operations` service.
public struct Google_Cloud_Videointelligence_V1p2beta1_AnnotateVideoResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Annotation results for all videos specified in `AnnotateVideoRequest`.
  public var annotationResults: [Google_Cloud_Videointelligence_V1p2beta1_VideoAnnotationResults] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Annotation progress for a single video.
public struct Google_Cloud_Videointelligence_V1p2beta1_VideoAnnotationProgress {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video file location in
  /// [Google Cloud Storage](https://cloud.google.com/storage/).
  public var inputUri: String = String()

  /// Approximate percentage processed thus far. Guaranteed to be
  /// 100 when fully processed.
  public var progressPercent: Int32 = 0

  /// Time when the request was received.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return self._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {self._startTime = nil}

  /// Time of the most recent update.
  public var updateTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _updateTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_updateTime = newValue}
  }
  /// Returns true if `updateTime` has been explicitly set.
  public var hasUpdateTime: Bool {return self._updateTime != nil}
  /// Clears the value of `updateTime`. Subsequent reads from it will return its default value.
  public mutating func clearUpdateTime() {self._updateTime = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
  fileprivate var _updateTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
}

/// Video annotation progress. Included in the `metadata`
/// field of the `Operation` returned by the `GetOperation`
/// call of the `google::longrunning::Operations` service.
public struct Google_Cloud_Videointelligence_V1p2beta1_AnnotateVideoProgress {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Progress metadata for all videos specified in `AnnotateVideoRequest`.
  public var annotationProgress: [Google_Cloud_Videointelligence_V1p2beta1_VideoAnnotationProgress] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A vertex represents a 2D point in the image.
/// NOTE: the normalized vertex coordinates are relative to the original image
/// and range from 0 to 1.
public struct Google_Cloud_Videointelligence_V1p2beta1_NormalizedVertex {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// X coordinate.
  public var x: Float = 0

  /// Y coordinate.
  public var y: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Normalized bounding polygon for text (that might not be aligned with axis).
/// Contains list of the corner points in clockwise order starting from
/// top-left corner. For example, for a rectangular bounding box:
/// When the text is horizontal it might look like:
///         0----1
///         |    |
///         3----2
///
/// When it's clockwise rotated 180 degrees around the top-left corner it
/// becomes:
///         2----3
///         |    |
///         1----0
///
/// and the vertex order will still be (0, 1, 2, 3). Note that values can be less
/// than 0, or greater than 1 due to trignometric calculations for location of
/// the box.
public struct Google_Cloud_Videointelligence_V1p2beta1_NormalizedBoundingPoly {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Normalized vertices of the bounding polygon.
  public var vertices: [Google_Cloud_Videointelligence_V1p2beta1_NormalizedVertex] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Video segment level annotation results for text detection.
public struct Google_Cloud_Videointelligence_V1p2beta1_TextSegment {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Video segment where a text snippet was detected.
  public var segment: Google_Cloud_Videointelligence_V1p2beta1_VideoSegment {
    get {return _segment ?? Google_Cloud_Videointelligence_V1p2beta1_VideoSegment()}
    set {_segment = newValue}
  }
  /// Returns true if `segment` has been explicitly set.
  public var hasSegment: Bool {return self._segment != nil}
  /// Clears the value of `segment`. Subsequent reads from it will return its default value.
  public mutating func clearSegment() {self._segment = nil}

  /// Confidence for the track of detected text. It is calculated as the highest
  /// over all frames where OCR detected text appears.
  public var confidence: Float = 0

  /// Information related to the frames where OCR detected text appears.
  public var frames: [Google_Cloud_Videointelligence_V1p2beta1_TextFrame] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _segment: Google_Cloud_Videointelligence_V1p2beta1_VideoSegment? = nil
}

/// Video frame level annotation results for text annotation (OCR).
/// Contains information regarding timestamp and bounding box locations for the
/// frames containing detected OCR text snippets.
public struct Google_Cloud_Videointelligence_V1p2beta1_TextFrame {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Bounding polygon of the detected text for this frame.
  public var rotatedBoundingBox: Google_Cloud_Videointelligence_V1p2beta1_NormalizedBoundingPoly {
    get {return _rotatedBoundingBox ?? Google_Cloud_Videointelligence_V1p2beta1_NormalizedBoundingPoly()}
    set {_rotatedBoundingBox = newValue}
  }
  /// Returns true if `rotatedBoundingBox` has been explicitly set.
  public var hasRotatedBoundingBox: Bool {return self._rotatedBoundingBox != nil}
  /// Clears the value of `rotatedBoundingBox`. Subsequent reads from it will return its default value.
  public mutating func clearRotatedBoundingBox() {self._rotatedBoundingBox = nil}

  /// Timestamp of this frame.
  public var timeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _timeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_timeOffset = newValue}
  }
  /// Returns true if `timeOffset` has been explicitly set.
  public var hasTimeOffset: Bool {return self._timeOffset != nil}
  /// Clears the value of `timeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearTimeOffset() {self._timeOffset = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _rotatedBoundingBox: Google_Cloud_Videointelligence_V1p2beta1_NormalizedBoundingPoly? = nil
  fileprivate var _timeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Annotations related to one detected OCR text snippet. This will contain the
/// corresponding text, confidence value, and frame level information for each
/// detection.
public struct Google_Cloud_Videointelligence_V1p2beta1_TextAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The detected text.
  public var text: String = String()

  /// All video segments where OCR detected text appears.
  public var segments: [Google_Cloud_Videointelligence_V1p2beta1_TextSegment] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Video frame level annotations for object detection and tracking. This field
/// stores per frame location, time offset, and confidence.
public struct Google_Cloud_Videointelligence_V1p2beta1_ObjectTrackingFrame {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The normalized bounding box location of this object track for the frame.
  public var normalizedBoundingBox: Google_Cloud_Videointelligence_V1p2beta1_NormalizedBoundingBox {
    get {return _normalizedBoundingBox ?? Google_Cloud_Videointelligence_V1p2beta1_NormalizedBoundingBox()}
    set {_normalizedBoundingBox = newValue}
  }
  /// Returns true if `normalizedBoundingBox` has been explicitly set.
  public var hasNormalizedBoundingBox: Bool {return self._normalizedBoundingBox != nil}
  /// Clears the value of `normalizedBoundingBox`. Subsequent reads from it will return its default value.
  public mutating func clearNormalizedBoundingBox() {self._normalizedBoundingBox = nil}

  /// The timestamp of the frame in microseconds.
  public var timeOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _timeOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_timeOffset = newValue}
  }
  /// Returns true if `timeOffset` has been explicitly set.
  public var hasTimeOffset: Bool {return self._timeOffset != nil}
  /// Clears the value of `timeOffset`. Subsequent reads from it will return its default value.
  public mutating func clearTimeOffset() {self._timeOffset = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _normalizedBoundingBox: Google_Cloud_Videointelligence_V1p2beta1_NormalizedBoundingBox? = nil
  fileprivate var _timeOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Annotations corresponding to one tracked object.
public struct Google_Cloud_Videointelligence_V1p2beta1_ObjectTrackingAnnotation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Entity to specify the object category that this track is labeled as.
  public var entity: Google_Cloud_Videointelligence_V1p2beta1_Entity {
    get {return _entity ?? Google_Cloud_Videointelligence_V1p2beta1_Entity()}
    set {_entity = newValue}
  }
  /// Returns true if `entity` has been explicitly set.
  public var hasEntity: Bool {return self._entity != nil}
  /// Clears the value of `entity`. Subsequent reads from it will return its default value.
  public mutating func clearEntity() {self._entity = nil}

  /// Object category's labeling confidence of this track.
  public var confidence: Float = 0

  /// Information corresponding to all frames where this object track appears.
  public var frames: [Google_Cloud_Videointelligence_V1p2beta1_ObjectTrackingFrame] = []

  /// Each object track corresponds to one video segment where it appears.
  public var segment: Google_Cloud_Videointelligence_V1p2beta1_VideoSegment {
    get {return _segment ?? Google_Cloud_Videointelligence_V1p2beta1_VideoSegment()}
    set {_segment = newValue}
  }
  /// Returns true if `segment` has been explicitly set.
  public var hasSegment: Bool {return self._segment != nil}
  /// Clears the value of `segment`. Subsequent reads from it will return its default value.
  public mutating func clearSegment() {self._segment = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _entity: Google_Cloud_Videointelligence_V1p2beta1_Entity? = nil
  fileprivate var _segment: Google_Cloud_Videointelligence_V1p2beta1_VideoSegment? = nil
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.videointelligence.v1p2beta1"

extension Google_Cloud_Videointelligence_V1p2beta1_Feature: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "FEATURE_UNSPECIFIED"),
    1: .same(proto: "LABEL_DETECTION"),
    2: .same(proto: "SHOT_CHANGE_DETECTION"),
    3: .same(proto: "EXPLICIT_CONTENT_DETECTION"),
    7: .same(proto: "TEXT_DETECTION"),
    9: .same(proto: "OBJECT_TRACKING"),
  ]
}

extension Google_Cloud_Videointelligence_V1p2beta1_LabelDetectionMode: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "LABEL_DETECTION_MODE_UNSPECIFIED"),
    1: .same(proto: "SHOT_MODE"),
    2: .same(proto: "FRAME_MODE"),
    3: .same(proto: "SHOT_AND_FRAME_MODE"),
  ]
}

extension Google_Cloud_Videointelligence_V1p2beta1_Likelihood: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "LIKELIHOOD_UNSPECIFIED"),
    1: .same(proto: "VERY_UNLIKELY"),
    2: .same(proto: "UNLIKELY"),
    3: .same(proto: "POSSIBLE"),
    4: .same(proto: "LIKELY"),
    5: .same(proto: "VERY_LIKELY"),
  ]
}

extension Google_Cloud_Videointelligence_V1p2beta1_AnnotateVideoRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AnnotateVideoRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "input_uri"),
    6: .standard(proto: "input_content"),
    2: .same(proto: "features"),
    3: .standard(proto: "video_context"),
    4: .standard(proto: "output_uri"),
    5: .standard(proto: "location_id"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.inputUri) }()
      case 2: try { try decoder.decodeRepeatedEnumField(value: &self.features) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._videoContext) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.outputUri) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.locationID) }()
      case 6: try { try decoder.decodeSingularBytesField(value: &self.inputContent) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.inputUri.isEmpty {
      try visitor.visitSingularStringField(value: self.inputUri, fieldNumber: 1)
    }
    if !self.features.isEmpty {
      try visitor.visitPackedEnumField(value: self.features, fieldNumber: 2)
    }
    if let v = self._videoContext {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if !self.outputUri.isEmpty {
      try visitor.visitSingularStringField(value: self.outputUri, fieldNumber: 4)
    }
    if !self.locationID.isEmpty {
      try visitor.visitSingularStringField(value: self.locationID, fieldNumber: 5)
    }
    if !self.inputContent.isEmpty {
      try visitor.visitSingularBytesField(value: self.inputContent, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_AnnotateVideoRequest, rhs: Google_Cloud_Videointelligence_V1p2beta1_AnnotateVideoRequest) -> Bool {
    if lhs.inputUri != rhs.inputUri {return false}
    if lhs.inputContent != rhs.inputContent {return false}
    if lhs.features != rhs.features {return false}
    if lhs._videoContext != rhs._videoContext {return false}
    if lhs.outputUri != rhs.outputUri {return false}
    if lhs.locationID != rhs.locationID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_VideoContext: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoContext"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "segments"),
    2: .standard(proto: "label_detection_config"),
    3: .standard(proto: "shot_change_detection_config"),
    4: .standard(proto: "explicit_content_detection_config"),
    8: .standard(proto: "text_detection_config"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.segments) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._labelDetectionConfig) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._shotChangeDetectionConfig) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._explicitContentDetectionConfig) }()
      case 8: try { try decoder.decodeSingularMessageField(value: &self._textDetectionConfig) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.segments.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.segments, fieldNumber: 1)
    }
    if let v = self._labelDetectionConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if let v = self._shotChangeDetectionConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if let v = self._explicitContentDetectionConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    if let v = self._textDetectionConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_VideoContext, rhs: Google_Cloud_Videointelligence_V1p2beta1_VideoContext) -> Bool {
    if lhs.segments != rhs.segments {return false}
    if lhs._labelDetectionConfig != rhs._labelDetectionConfig {return false}
    if lhs._shotChangeDetectionConfig != rhs._shotChangeDetectionConfig {return false}
    if lhs._explicitContentDetectionConfig != rhs._explicitContentDetectionConfig {return false}
    if lhs._textDetectionConfig != rhs._textDetectionConfig {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_LabelDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "label_detection_mode"),
    2: .standard(proto: "stationary_camera"),
    3: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.labelDetectionMode) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.stationaryCamera) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.model) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.labelDetectionMode != .unspecified {
      try visitor.visitSingularEnumField(value: self.labelDetectionMode, fieldNumber: 1)
    }
    if self.stationaryCamera != false {
      try visitor.visitSingularBoolField(value: self.stationaryCamera, fieldNumber: 2)
    }
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_LabelDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p2beta1_LabelDetectionConfig) -> Bool {
    if lhs.labelDetectionMode != rhs.labelDetectionMode {return false}
    if lhs.stationaryCamera != rhs.stationaryCamera {return false}
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_ShotChangeDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ShotChangeDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.model) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_ShotChangeDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p2beta1_ShotChangeDetectionConfig) -> Bool {
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ExplicitContentDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.model) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentDetectionConfig) -> Bool {
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_TextDetectionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TextDetectionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "language_hints"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedStringField(value: &self.languageHints) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.languageHints.isEmpty {
      try visitor.visitRepeatedStringField(value: self.languageHints, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_TextDetectionConfig, rhs: Google_Cloud_Videointelligence_V1p2beta1_TextDetectionConfig) -> Bool {
    if lhs.languageHints != rhs.languageHints {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_VideoSegment: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoSegment"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "start_time_offset"),
    2: .standard(proto: "end_time_offset"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._startTimeOffset) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._endTimeOffset) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._startTimeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._endTimeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_VideoSegment, rhs: Google_Cloud_Videointelligence_V1p2beta1_VideoSegment) -> Bool {
    if lhs._startTimeOffset != rhs._startTimeOffset {return false}
    if lhs._endTimeOffset != rhs._endTimeOffset {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_LabelSegment: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelSegment"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "segment"),
    2: .same(proto: "confidence"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._segment) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._segment {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_LabelSegment, rhs: Google_Cloud_Videointelligence_V1p2beta1_LabelSegment) -> Bool {
    if lhs._segment != rhs._segment {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_LabelFrame: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelFrame"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "time_offset"),
    2: .same(proto: "confidence"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._timeOffset) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._timeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_LabelFrame, rhs: Google_Cloud_Videointelligence_V1p2beta1_LabelFrame) -> Bool {
    if lhs._timeOffset != rhs._timeOffset {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_Entity: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".Entity"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "entity_id"),
    2: .same(proto: "description"),
    3: .standard(proto: "language_code"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.entityID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.description_p) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.languageCode) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.entityID.isEmpty {
      try visitor.visitSingularStringField(value: self.entityID, fieldNumber: 1)
    }
    if !self.description_p.isEmpty {
      try visitor.visitSingularStringField(value: self.description_p, fieldNumber: 2)
    }
    if !self.languageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.languageCode, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_Entity, rhs: Google_Cloud_Videointelligence_V1p2beta1_Entity) -> Bool {
    if lhs.entityID != rhs.entityID {return false}
    if lhs.description_p != rhs.description_p {return false}
    if lhs.languageCode != rhs.languageCode {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_LabelAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LabelAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "entity"),
    2: .standard(proto: "category_entities"),
    3: .same(proto: "segments"),
    4: .same(proto: "frames"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._entity) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.categoryEntities) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.segments) }()
      case 4: try { try decoder.decodeRepeatedMessageField(value: &self.frames) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._entity {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if !self.categoryEntities.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.categoryEntities, fieldNumber: 2)
    }
    if !self.segments.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.segments, fieldNumber: 3)
    }
    if !self.frames.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frames, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_LabelAnnotation, rhs: Google_Cloud_Videointelligence_V1p2beta1_LabelAnnotation) -> Bool {
    if lhs._entity != rhs._entity {return false}
    if lhs.categoryEntities != rhs.categoryEntities {return false}
    if lhs.segments != rhs.segments {return false}
    if lhs.frames != rhs.frames {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentFrame: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ExplicitContentFrame"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "time_offset"),
    2: .standard(proto: "pornography_likelihood"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._timeOffset) }()
      case 2: try { try decoder.decodeSingularEnumField(value: &self.pornographyLikelihood) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._timeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.pornographyLikelihood != .unspecified {
      try visitor.visitSingularEnumField(value: self.pornographyLikelihood, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentFrame, rhs: Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentFrame) -> Bool {
    if lhs._timeOffset != rhs._timeOffset {return false}
    if lhs.pornographyLikelihood != rhs.pornographyLikelihood {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ExplicitContentAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "frames"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.frames) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.frames.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frames, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentAnnotation, rhs: Google_Cloud_Videointelligence_V1p2beta1_ExplicitContentAnnotation) -> Bool {
    if lhs.frames != rhs.frames {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_NormalizedBoundingBox: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".NormalizedBoundingBox"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "left"),
    2: .same(proto: "top"),
    3: .same(proto: "right"),
    4: .same(proto: "bottom"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularFloatField(value: &self.left) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.top) }()
      case 3: try { try decoder.decodeSingularFloatField(value: &self.right) }()
      case 4: try { try decoder.decodeSingularFloatField(value: &self.bottom) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.left != 0 {
      try visitor.visitSingularFloatField(value: self.left, fieldNumber: 1)
    }
    if self.top != 0 {
      try visitor.visitSingularFloatField(value: self.top, fieldNumber: 2)
    }
    if self.right != 0 {
      try visitor.visitSingularFloatField(value: self.right, fieldNumber: 3)
    }
    if self.bottom != 0 {
      try visitor.visitSingularFloatField(value: self.bottom, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_NormalizedBoundingBox, rhs: Google_Cloud_Videointelligence_V1p2beta1_NormalizedBoundingBox) -> Bool {
    if lhs.left != rhs.left {return false}
    if lhs.top != rhs.top {return false}
    if lhs.right != rhs.right {return false}
    if lhs.bottom != rhs.bottom {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_VideoAnnotationResults: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoAnnotationResults"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "input_uri"),
    2: .standard(proto: "segment_label_annotations"),
    3: .standard(proto: "shot_label_annotations"),
    4: .standard(proto: "frame_label_annotations"),
    6: .standard(proto: "shot_annotations"),
    7: .standard(proto: "explicit_annotation"),
    12: .standard(proto: "text_annotations"),
    14: .standard(proto: "object_annotations"),
    9: .same(proto: "error"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.inputUri) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.segmentLabelAnnotations) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.shotLabelAnnotations) }()
      case 4: try { try decoder.decodeRepeatedMessageField(value: &self.frameLabelAnnotations) }()
      case 6: try { try decoder.decodeRepeatedMessageField(value: &self.shotAnnotations) }()
      case 7: try { try decoder.decodeSingularMessageField(value: &self._explicitAnnotation) }()
      case 9: try { try decoder.decodeSingularMessageField(value: &self._error) }()
      case 12: try { try decoder.decodeRepeatedMessageField(value: &self.textAnnotations) }()
      case 14: try { try decoder.decodeRepeatedMessageField(value: &self.objectAnnotations) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.inputUri.isEmpty {
      try visitor.visitSingularStringField(value: self.inputUri, fieldNumber: 1)
    }
    if !self.segmentLabelAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.segmentLabelAnnotations, fieldNumber: 2)
    }
    if !self.shotLabelAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.shotLabelAnnotations, fieldNumber: 3)
    }
    if !self.frameLabelAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frameLabelAnnotations, fieldNumber: 4)
    }
    if !self.shotAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.shotAnnotations, fieldNumber: 6)
    }
    if let v = self._explicitAnnotation {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
    }
    if let v = self._error {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
    }
    if !self.textAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.textAnnotations, fieldNumber: 12)
    }
    if !self.objectAnnotations.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.objectAnnotations, fieldNumber: 14)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_VideoAnnotationResults, rhs: Google_Cloud_Videointelligence_V1p2beta1_VideoAnnotationResults) -> Bool {
    if lhs.inputUri != rhs.inputUri {return false}
    if lhs.segmentLabelAnnotations != rhs.segmentLabelAnnotations {return false}
    if lhs.shotLabelAnnotations != rhs.shotLabelAnnotations {return false}
    if lhs.frameLabelAnnotations != rhs.frameLabelAnnotations {return false}
    if lhs.shotAnnotations != rhs.shotAnnotations {return false}
    if lhs._explicitAnnotation != rhs._explicitAnnotation {return false}
    if lhs.textAnnotations != rhs.textAnnotations {return false}
    if lhs.objectAnnotations != rhs.objectAnnotations {return false}
    if lhs._error != rhs._error {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_AnnotateVideoResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AnnotateVideoResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "annotation_results"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.annotationResults) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.annotationResults.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.annotationResults, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_AnnotateVideoResponse, rhs: Google_Cloud_Videointelligence_V1p2beta1_AnnotateVideoResponse) -> Bool {
    if lhs.annotationResults != rhs.annotationResults {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_VideoAnnotationProgress: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VideoAnnotationProgress"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "input_uri"),
    2: .standard(proto: "progress_percent"),
    3: .standard(proto: "start_time"),
    4: .standard(proto: "update_time"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.inputUri) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.progressPercent) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._startTime) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._updateTime) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.inputUri.isEmpty {
      try visitor.visitSingularStringField(value: self.inputUri, fieldNumber: 1)
    }
    if self.progressPercent != 0 {
      try visitor.visitSingularInt32Field(value: self.progressPercent, fieldNumber: 2)
    }
    if let v = self._startTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if let v = self._updateTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_VideoAnnotationProgress, rhs: Google_Cloud_Videointelligence_V1p2beta1_VideoAnnotationProgress) -> Bool {
    if lhs.inputUri != rhs.inputUri {return false}
    if lhs.progressPercent != rhs.progressPercent {return false}
    if lhs._startTime != rhs._startTime {return false}
    if lhs._updateTime != rhs._updateTime {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_AnnotateVideoProgress: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AnnotateVideoProgress"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "annotation_progress"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.annotationProgress) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.annotationProgress.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.annotationProgress, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_AnnotateVideoProgress, rhs: Google_Cloud_Videointelligence_V1p2beta1_AnnotateVideoProgress) -> Bool {
    if lhs.annotationProgress != rhs.annotationProgress {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_NormalizedVertex: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".NormalizedVertex"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "x"),
    2: .same(proto: "y"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularFloatField(value: &self.x) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.y) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.x != 0 {
      try visitor.visitSingularFloatField(value: self.x, fieldNumber: 1)
    }
    if self.y != 0 {
      try visitor.visitSingularFloatField(value: self.y, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_NormalizedVertex, rhs: Google_Cloud_Videointelligence_V1p2beta1_NormalizedVertex) -> Bool {
    if lhs.x != rhs.x {return false}
    if lhs.y != rhs.y {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_NormalizedBoundingPoly: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".NormalizedBoundingPoly"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "vertices"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.vertices) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.vertices.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.vertices, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_NormalizedBoundingPoly, rhs: Google_Cloud_Videointelligence_V1p2beta1_NormalizedBoundingPoly) -> Bool {
    if lhs.vertices != rhs.vertices {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_TextSegment: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TextSegment"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "segment"),
    2: .same(proto: "confidence"),
    3: .same(proto: "frames"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._segment) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.frames) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._segment {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 2)
    }
    if !self.frames.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frames, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_TextSegment, rhs: Google_Cloud_Videointelligence_V1p2beta1_TextSegment) -> Bool {
    if lhs._segment != rhs._segment {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.frames != rhs.frames {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_TextFrame: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TextFrame"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "rotated_bounding_box"),
    2: .standard(proto: "time_offset"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._rotatedBoundingBox) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._timeOffset) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._rotatedBoundingBox {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._timeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_TextFrame, rhs: Google_Cloud_Videointelligence_V1p2beta1_TextFrame) -> Bool {
    if lhs._rotatedBoundingBox != rhs._rotatedBoundingBox {return false}
    if lhs._timeOffset != rhs._timeOffset {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_TextAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TextAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "text"),
    2: .same(proto: "segments"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.text) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.segments) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.text.isEmpty {
      try visitor.visitSingularStringField(value: self.text, fieldNumber: 1)
    }
    if !self.segments.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.segments, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_TextAnnotation, rhs: Google_Cloud_Videointelligence_V1p2beta1_TextAnnotation) -> Bool {
    if lhs.text != rhs.text {return false}
    if lhs.segments != rhs.segments {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_ObjectTrackingFrame: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ObjectTrackingFrame"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "normalized_bounding_box"),
    2: .standard(proto: "time_offset"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._normalizedBoundingBox) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._timeOffset) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._normalizedBoundingBox {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._timeOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_ObjectTrackingFrame, rhs: Google_Cloud_Videointelligence_V1p2beta1_ObjectTrackingFrame) -> Bool {
    if lhs._normalizedBoundingBox != rhs._normalizedBoundingBox {return false}
    if lhs._timeOffset != rhs._timeOffset {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Videointelligence_V1p2beta1_ObjectTrackingAnnotation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ObjectTrackingAnnotation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "entity"),
    4: .same(proto: "confidence"),
    2: .same(proto: "frames"),
    3: .same(proto: "segment"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._entity) }()
      case 2: try { try decoder.decodeRepeatedMessageField(value: &self.frames) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._segment) }()
      case 4: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._entity {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if !self.frames.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.frames, fieldNumber: 2)
    }
    if let v = self._segment {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Videointelligence_V1p2beta1_ObjectTrackingAnnotation, rhs: Google_Cloud_Videointelligence_V1p2beta1_ObjectTrackingAnnotation) -> Bool {
    if lhs._entity != rhs._entity {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.frames != rhs.frames {return false}
    if lhs._segment != rhs._segment {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
