// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/mediatranslation/v1alpha1/media_translation.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2020 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Provides information to the speech translation that specifies how to process
/// the request.
public struct Google_Cloud_Mediatranslation_V1alpha1_TranslateSpeechConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Encoding of audio data.
  /// Supported formats:
  ///
  /// - `linear16`
  ///
  ///   Uncompressed 16-bit signed little-endian samples (Linear PCM).
  ///
  /// - `flac`
  ///
  ///   `flac` (Free Lossless Audio Codec) is the recommended encoding
  ///   because it is lossless--therefore recognition is not compromised--and
  ///   requires only about half the bandwidth of `linear16`.
  ///
  /// - `mulaw`
  ///
  ///   8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
  ///
  /// - `amr`
  ///
  ///   Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
  ///
  /// - `amr-wb`
  ///
  ///   Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
  ///
  /// - `ogg-opus`
  ///
  ///   Opus encoded audio frames in Ogg container
  ///   ([OggOpus](https://wiki.xiph.org/OggOpus)).
  ///   `sample_rate_hertz` must be one of 8000, 12000, 16000, 24000, or 48000.
  ///
  /// - `mp3`
  ///
  ///   MP3 audio. Support all standard MP3 bitrates (which range from 32-320
  ///   kbps). When using this encoding, `sample_rate_hertz` has to match the
  ///   sample rate of the file being used.
  public var audioEncoding: String = String()

  /// Required. Source language code (BCP-47) of the input audio.
  public var sourceLanguageCode: String = String()

  /// Required. Target language code (BCP-47) of the output.
  public var targetLanguageCode: String = String()

  /// Optional. A list of up to 3 additional language codes (BCP-47), listing possible
  /// alternative languages of the supplied audio. If alternative source
  /// languages are listed, speech translation result will translate in the most
  /// likely language detected including the main source_language_code. The
  /// translated result will include the language code of the language detected
  /// in the audio.
  /// Note:
  /// 1. If the provided alternative_source_language_code is not supported
  /// by current API version, we will skip that language code.
  /// 2. If user only provided one eligible alternative_source_language_codes,
  /// the translation will happen between source_language_code and
  /// alternative_source_language_codes. The target_language_code will be
  /// ignored. It will be useful in conversation mode.
  public var alternativeSourceLanguageCodes: [String] = []

  /// Optional. Sample rate in Hertz of the audio data. Valid values are:
  /// 8000-48000. 16000 is optimal. For best results, set the sampling rate of
  /// the audio source to 16000 Hz. If that's not possible, use the native sample
  /// rate of the audio source (instead of re-sampling).
  public var sampleRateHertz: Int32 = 0

  /// Optional.
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config used for streaming translation.
public struct Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The common config for all the following audio contents.
  public var audioConfig: Google_Cloud_Mediatranslation_V1alpha1_TranslateSpeechConfig {
    get {return _audioConfig ?? Google_Cloud_Mediatranslation_V1alpha1_TranslateSpeechConfig()}
    set {_audioConfig = newValue}
  }
  /// Returns true if `audioConfig` has been explicitly set.
  public var hasAudioConfig: Bool {return self._audioConfig != nil}
  /// Clears the value of `audioConfig`. Subsequent reads from it will return its default value.
  public mutating func clearAudioConfig() {self._audioConfig = nil}

  /// Optional. If `false` or omitted, the system performs
  /// continuous translation (continuing to wait for and process audio even if
  /// the user pauses speaking) until the client closes the input stream (gRPC
  /// API) or until the maximum time limit has been reached. May return multiple
  /// `StreamingTranslateSpeechResult`s with the `is_final` flag set to `true`.
  ///
  /// If `true`, the speech translator will detect a single spoken utterance.
  /// When it detects that the user has paused or stopped speaking, it will
  /// return an `END_OF_SINGLE_UTTERANCE` event and cease translation.
  /// When the client receives `END_OF_SINGLE_UTTERANCE` event, the client should
  /// stop sending the requests. However, clients should keep receiving remaining
  /// responses until the stream is terminated. To construct the complete
  /// sentence in a streaming way, one should override (if `is_final` of previous
  /// response is false), or append (if 'is_final' of previous response is true).
  public var singleUtterance: Bool = false

  /// Optional. Stability control for the media translation text. The value should be
  /// "LOW", "MEDIUM", "HIGH". It applies to text/text_and_audio translation
  /// only.
  /// For audio translation mode, we only support HIGH stability mode,
  /// low/medium stability mode will throw argument error.
  /// Default empty string will be treated as "HIGH" in audio translation mode;
  /// will be treated as "LOW" in other translation mode.
  /// Note that stability and speed would be trade off.
  /// 1. "LOW": In low mode, translation service will start to do translation
  /// right after getting recognition response. The speed will be faster.
  /// 2. "MEDIUM": In medium mode, translation service will
  /// check if the recognition response is stable enough or not, and only
  /// translate recognition response which is not likely to be changed later.
  /// 3. "HIGH": In high mode, translation service will wait for more stable
  /// recognition responses, and then start to do translation. Also, the
  /// following recognition responses cannot modify previous recognition
  /// responses. Thus it may impact quality in some situation. "HIGH" stability
  /// will generate "final" responses more frequently.
  public var stability: String = String()

  /// Optional. Translation mode, the value should be "text", "audio", "text_and_audio".
  /// Default empty string will be treated as "text".
  /// 1. "text": The response will be text translation. Text translation has a
  /// field "is_final". Detailed definition can be found in
  /// `TextTranslationResult`.
  /// 2. "audio": The response will be audio translation. Audio translation does
  /// not have "is_final" field, which means each audio translation response is
  /// stable and will not be changed by later response.
  /// Translation mode "audio" can only be used with "high" stability mode,
  /// 3. "text_and_audio": The response will have a text translation, when
  /// "is_final" is true, we will also output its corresponding audio
  /// translation. When "is_final" is false, audio_translation field will be
  /// empty.
  public var translationMode: String = String()

  /// Optional. If disable_interim_results is true, we will only return "final" responses.
  /// Otherwise, we will return all the responses. Default value will be false.
  /// User can only set disable_interim_results to be true with "high" stability
  /// mode.
  public var disableInterimResults: Bool = false

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _audioConfig: Google_Cloud_Mediatranslation_V1alpha1_TranslateSpeechConfig? = nil
}

/// The top-level message sent by the client for the `StreamingTranslateSpeech`
/// method. Multiple `StreamingTranslateSpeechRequest` messages are sent. The
/// first message must contain a `streaming_config` message and must not contain
/// `audio_content` data. All subsequent messages must contain `audio_content`
/// data and must not contain a `streaming_config` message.
public struct Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The streaming request, which is either a streaming config or content.
  public var streamingRequest: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechRequest.OneOf_StreamingRequest? = nil

  /// Provides information to the recognizer that specifies how to process the
  /// request. The first `StreamingTranslateSpeechRequest` message must contain
  /// a `streaming_config` message.
  public var streamingConfig: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechConfig {
    get {
      if case .streamingConfig(let v)? = streamingRequest {return v}
      return Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechConfig()
    }
    set {streamingRequest = .streamingConfig(newValue)}
  }

  /// The audio data to be translated. Sequential chunks of audio data are sent
  /// in sequential `StreamingTranslateSpeechRequest` messages. The first
  /// `StreamingTranslateSpeechRequest` message must not contain
  /// `audio_content` data and all subsequent `StreamingTranslateSpeechRequest`
  /// messages must contain `audio_content` data. The audio bytes must be
  /// encoded as specified in `StreamingTranslateSpeechConfig`. Note: as with
  /// all bytes fields, protobuffers use a pure binary representation (not
  /// base64).
  public var audioContent: Data {
    get {
      if case .audioContent(let v)? = streamingRequest {return v}
      return Data()
    }
    set {streamingRequest = .audioContent(newValue)}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The streaming request, which is either a streaming config or content.
  public enum OneOf_StreamingRequest: Equatable {
    /// Provides information to the recognizer that specifies how to process the
    /// request. The first `StreamingTranslateSpeechRequest` message must contain
    /// a `streaming_config` message.
    case streamingConfig(Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechConfig)
    /// The audio data to be translated. Sequential chunks of audio data are sent
    /// in sequential `StreamingTranslateSpeechRequest` messages. The first
    /// `StreamingTranslateSpeechRequest` message must not contain
    /// `audio_content` data and all subsequent `StreamingTranslateSpeechRequest`
    /// messages must contain `audio_content` data. The audio bytes must be
    /// encoded as specified in `StreamingTranslateSpeechConfig`. Note: as with
    /// all bytes fields, protobuffers use a pure binary representation (not
    /// base64).
    case audioContent(Data)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechRequest.OneOf_StreamingRequest, rhs: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechRequest.OneOf_StreamingRequest) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.streamingConfig, .streamingConfig): return {
        guard case .streamingConfig(let l) = lhs, case .streamingConfig(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.audioContent, .audioContent): return {
        guard case .audioContent(let l) = lhs, case .audioContent(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}
}

/// A streaming speech translation result corresponding to a portion of the audio
/// that is currently being processed.
public struct Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Text translation result.
  public var textTranslationResult: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult.TextTranslationResult {
    get {return _textTranslationResult ?? Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult.TextTranslationResult()}
    set {_textTranslationResult = newValue}
  }
  /// Returns true if `textTranslationResult` has been explicitly set.
  public var hasTextTranslationResult: Bool {return self._textTranslationResult != nil}
  /// Clears the value of `textTranslationResult`. Subsequent reads from it will return its default value.
  public mutating func clearTextTranslationResult() {self._textTranslationResult = nil}

  /// Audio translation result.
  public var audioTranslationResult: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult.AudioTranslationResult {
    get {return _audioTranslationResult ?? Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult.AudioTranslationResult()}
    set {_audioTranslationResult = newValue}
  }
  /// Returns true if `audioTranslationResult` has been explicitly set.
  public var hasAudioTranslationResult: Bool {return self._audioTranslationResult != nil}
  /// Clears the value of `audioTranslationResult`. Subsequent reads from it will return its default value.
  public mutating func clearAudioTranslationResult() {self._audioTranslationResult = nil}

  /// Output only. The debug only recognition result in original language. This field is debug
  /// only and will be set to empty string if not available.
  /// This is implementation detail and will not be backward compatible.
  public var recognitionResult: String = String()

  /// Output only.
  public var detectedSourceLanguageCode: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Text translation result.
  public struct TextTranslationResult {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Output only. The translated sentence.
    public var translation: String = String()

    /// Output only. If `false`, this `StreamingTranslateSpeechResult` represents
    /// an interim result that may change. If `true`, this is the final time the
    /// translation service will return this particular
    /// `StreamingTranslateSpeechResult`, the streaming translator will not
    /// return any further hypotheses for this portion of the transcript and
    /// corresponding audio.
    public var isFinal: Bool = false

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    public init() {}
  }

  /// Audio translation result.
  public struct AudioTranslationResult {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Output only. The translated audio.
    public var audioTranslation: Data = Data()

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    public init() {}
  }

  public init() {}

  fileprivate var _textTranslationResult: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult.TextTranslationResult? = nil
  fileprivate var _audioTranslationResult: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult.AudioTranslationResult? = nil
}

/// A streaming speech translation response corresponding to a portion of
/// the audio currently processed.
public struct Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. If set, returns a [google.rpc.Status][google.rpc.Status] message that
  /// specifies the error for the operation.
  public var error: Google_Rpc_Status {
    get {return _error ?? Google_Rpc_Status()}
    set {_error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return self._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {self._error = nil}

  /// Output only. The translation result that is currently being processed (For text
  /// translation, `is_final` could be `true` or `false`.
  /// For audio translation, we do not have is_final field, which means each
  /// audio response is stable and will not get changed later. For
  /// text_and_audio, we still have `is_final` field in text translation, but we
  /// only output corresponsding audio when `is_final` is true.).
  public var result: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult {
    get {return _result ?? Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult()}
    set {_result = newValue}
  }
  /// Returns true if `result` has been explicitly set.
  public var hasResult: Bool {return self._result != nil}
  /// Clears the value of `result`. Subsequent reads from it will return its default value.
  public mutating func clearResult() {self._result = nil}

  /// Output only. Indicates the type of speech event.
  public var speechEventType: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResponse.SpeechEventType = .unspecified

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Indicates the type of speech event.
  public enum SpeechEventType: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// No speech event specified.
    case unspecified // = 0

    /// This event indicates that the server has detected the end of the user's
    /// speech utterance and expects no additional speech. Therefore, the server
    /// will not process additional audio (although it may subsequently return
    /// additional results). When the client receives `END_OF_SINGLE_UTTERANCE`
    /// event, the client should stop sending the requests. However, clients
    /// should keep receiving remaining responses until the stream is terminated.
    /// To construct the complete sentence in a streaming way, one should
    /// override (if `is_final` of previous response is `false`), or append (if
    /// `is_final` of previous response is `true`). This event is only sent if
    /// `single_utterance` was set to `true`, and is not used otherwise.
    case endOfSingleUtterance // = 1
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .endOfSingleUtterance
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .endOfSingleUtterance: return 1
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}

  fileprivate var _error: Google_Rpc_Status? = nil
  fileprivate var _result: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult? = nil
}

#if swift(>=4.2)

extension Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResponse.SpeechEventType: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResponse.SpeechEventType] = [
    .unspecified,
    .endOfSingleUtterance,
  ]
}

#endif  // swift(>=4.2)

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.mediatranslation.v1alpha1"

extension Google_Cloud_Mediatranslation_V1alpha1_TranslateSpeechConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TranslateSpeechConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "audio_encoding"),
    2: .standard(proto: "source_language_code"),
    3: .standard(proto: "target_language_code"),
    6: .standard(proto: "alternative_source_language_codes"),
    4: .standard(proto: "sample_rate_hertz"),
    5: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.audioEncoding) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.sourceLanguageCode) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.targetLanguageCode) }()
      case 4: try { try decoder.decodeSingularInt32Field(value: &self.sampleRateHertz) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.model) }()
      case 6: try { try decoder.decodeRepeatedStringField(value: &self.alternativeSourceLanguageCodes) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.audioEncoding.isEmpty {
      try visitor.visitSingularStringField(value: self.audioEncoding, fieldNumber: 1)
    }
    if !self.sourceLanguageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.sourceLanguageCode, fieldNumber: 2)
    }
    if !self.targetLanguageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.targetLanguageCode, fieldNumber: 3)
    }
    if self.sampleRateHertz != 0 {
      try visitor.visitSingularInt32Field(value: self.sampleRateHertz, fieldNumber: 4)
    }
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 5)
    }
    if !self.alternativeSourceLanguageCodes.isEmpty {
      try visitor.visitRepeatedStringField(value: self.alternativeSourceLanguageCodes, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Mediatranslation_V1alpha1_TranslateSpeechConfig, rhs: Google_Cloud_Mediatranslation_V1alpha1_TranslateSpeechConfig) -> Bool {
    if lhs.audioEncoding != rhs.audioEncoding {return false}
    if lhs.sourceLanguageCode != rhs.sourceLanguageCode {return false}
    if lhs.targetLanguageCode != rhs.targetLanguageCode {return false}
    if lhs.alternativeSourceLanguageCodes != rhs.alternativeSourceLanguageCodes {return false}
    if lhs.sampleRateHertz != rhs.sampleRateHertz {return false}
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingTranslateSpeechConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "audio_config"),
    2: .standard(proto: "single_utterance"),
    3: .same(proto: "stability"),
    4: .standard(proto: "translation_mode"),
    5: .standard(proto: "disable_interim_results"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._audioConfig) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.singleUtterance) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.stability) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.translationMode) }()
      case 5: try { try decoder.decodeSingularBoolField(value: &self.disableInterimResults) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._audioConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.singleUtterance != false {
      try visitor.visitSingularBoolField(value: self.singleUtterance, fieldNumber: 2)
    }
    if !self.stability.isEmpty {
      try visitor.visitSingularStringField(value: self.stability, fieldNumber: 3)
    }
    if !self.translationMode.isEmpty {
      try visitor.visitSingularStringField(value: self.translationMode, fieldNumber: 4)
    }
    if self.disableInterimResults != false {
      try visitor.visitSingularBoolField(value: self.disableInterimResults, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechConfig, rhs: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechConfig) -> Bool {
    if lhs._audioConfig != rhs._audioConfig {return false}
    if lhs.singleUtterance != rhs.singleUtterance {return false}
    if lhs.stability != rhs.stability {return false}
    if lhs.translationMode != rhs.translationMode {return false}
    if lhs.disableInterimResults != rhs.disableInterimResults {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingTranslateSpeechRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "streaming_config"),
    2: .standard(proto: "audio_content"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        var v: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechConfig?
        if let current = self.streamingRequest {
          try decoder.handleConflictingOneOf()
          if case .streamingConfig(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.streamingRequest = .streamingConfig(v)}
      }()
      case 2: try {
        if self.streamingRequest != nil {try decoder.handleConflictingOneOf()}
        var v: Data?
        try decoder.decodeSingularBytesField(value: &v)
        if let v = v {self.streamingRequest = .audioContent(v)}
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.streamingRequest {
    case .streamingConfig?: try {
      guard case .streamingConfig(let v)? = self.streamingRequest else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }()
    case .audioContent?: try {
      guard case .audioContent(let v)? = self.streamingRequest else { preconditionFailure() }
      try visitor.visitSingularBytesField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechRequest, rhs: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechRequest) -> Bool {
    if lhs.streamingRequest != rhs.streamingRequest {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingTranslateSpeechResult"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "text_translation_result"),
    2: .standard(proto: "audio_translation_result"),
    3: .standard(proto: "recognition_result"),
    4: .standard(proto: "detected_source_language_code"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._textTranslationResult) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._audioTranslationResult) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.recognitionResult) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.detectedSourceLanguageCode) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._textTranslationResult {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._audioTranslationResult {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if !self.recognitionResult.isEmpty {
      try visitor.visitSingularStringField(value: self.recognitionResult, fieldNumber: 3)
    }
    if !self.detectedSourceLanguageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.detectedSourceLanguageCode, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult, rhs: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult) -> Bool {
    if lhs._textTranslationResult != rhs._textTranslationResult {return false}
    if lhs._audioTranslationResult != rhs._audioTranslationResult {return false}
    if lhs.recognitionResult != rhs.recognitionResult {return false}
    if lhs.detectedSourceLanguageCode != rhs.detectedSourceLanguageCode {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult.TextTranslationResult: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult.protoMessageName + ".TextTranslationResult"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "translation"),
    2: .standard(proto: "is_final"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.translation) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.isFinal) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.translation.isEmpty {
      try visitor.visitSingularStringField(value: self.translation, fieldNumber: 1)
    }
    if self.isFinal != false {
      try visitor.visitSingularBoolField(value: self.isFinal, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult.TextTranslationResult, rhs: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult.TextTranslationResult) -> Bool {
    if lhs.translation != rhs.translation {return false}
    if lhs.isFinal != rhs.isFinal {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult.AudioTranslationResult: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult.protoMessageName + ".AudioTranslationResult"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "audio_translation"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularBytesField(value: &self.audioTranslation) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.audioTranslation.isEmpty {
      try visitor.visitSingularBytesField(value: self.audioTranslation, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult.AudioTranslationResult, rhs: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResult.AudioTranslationResult) -> Bool {
    if lhs.audioTranslation != rhs.audioTranslation {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingTranslateSpeechResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "error"),
    2: .same(proto: "result"),
    3: .standard(proto: "speech_event_type"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._error) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._result) }()
      case 3: try { try decoder.decodeSingularEnumField(value: &self.speechEventType) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._error {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._result {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if self.speechEventType != .unspecified {
      try visitor.visitSingularEnumField(value: self.speechEventType, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResponse, rhs: Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResponse) -> Bool {
    if lhs._error != rhs._error {return false}
    if lhs._result != rhs._result {return false}
    if lhs.speechEventType != rhs.speechEventType {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Mediatranslation_V1alpha1_StreamingTranslateSpeechResponse.SpeechEventType: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "SPEECH_EVENT_TYPE_UNSPECIFIED"),
    1: .same(proto: "END_OF_SINGLE_UTTERANCE"),
  ]
}
