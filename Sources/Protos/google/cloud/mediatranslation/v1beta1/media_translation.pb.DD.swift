// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/mediatranslation/v1beta1/media_translation.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2020 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Provides information to the speech translation that specifies how to process
/// the request.
public struct Google_Cloud_Mediatranslation_V1beta1_TranslateSpeechConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Encoding of audio data.
  /// Supported formats:
  ///
  /// - `linear16`
  ///
  ///   Uncompressed 16-bit signed little-endian samples (Linear PCM).
  public var audioEncoding: String = String()

  /// Required. Source language code (BCP-47) of the input audio.
  public var sourceLanguageCode: String = String()

  /// Optional. A list of up to 3 additional language codes (BCP-47), listing possible
  /// alternative languages of the supplied audio. If alternative source
  /// languages are listed, speech translation result will translate in the most
  /// likely language detected including the main source_language_code. The
  /// translated result will include the language code of the language detected
  /// in the audio.
  public var alternativeSourceLanguageCodes: [String] = []

  /// Required. Target language code (BCP-47) of the output.
  public var targetLanguageCode: String = String()

  /// Optional. Sample rate in Hertz of the audio data. Valid values are:
  /// 8000-48000. 16000 is optimal. For best results, set the sampling rate of
  /// the audio source to 16000 Hz. If that's not possible, use the native sample
  /// rate of the audio source (instead of re-sampling). This field can only be
  /// omitted for `FLAC` and `WAV` audio files.
  public var sampleRateHertz: Int32 = 0

  /// Optional.
  public var model: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Config used for streaming translation.
public struct Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The common config for all the following audio contents.
  public var audioConfig: Google_Cloud_Mediatranslation_V1beta1_TranslateSpeechConfig {
    get {return _audioConfig ?? Google_Cloud_Mediatranslation_V1beta1_TranslateSpeechConfig()}
    set {_audioConfig = newValue}
  }
  /// Returns true if `audioConfig` has been explicitly set.
  public var hasAudioConfig: Bool {return self._audioConfig != nil}
  /// Clears the value of `audioConfig`. Subsequent reads from it will return its default value.
  public mutating func clearAudioConfig() {self._audioConfig = nil}

  /// Optional. If `false` or omitted, the system performs
  /// continuous translation (continuing to wait for and process audio even if
  /// the user pauses speaking) until the client closes the input stream (gRPC
  /// API) or until the maximum time limit has been reached. May return multiple
  /// `StreamingTranslateSpeechResult`s with the `is_final` flag set to `true`.
  ///
  /// If `true`, the speech translator will detect a single spoken utterance.
  /// When it detects that the user has paused or stopped speaking, it will
  /// return an `END_OF_SINGLE_UTTERANCE` event and cease translation.
  /// When the client receives 'END_OF_SINGLE_UTTERANCE' event, the client should
  /// stop sending the requests. However, clients should keep receiving remaining
  /// responses until the stream is terminated. To construct the complete
  /// sentence in a streaming way, one should override (if 'is_final' of previous
  /// response is false), or append (if 'is_final' of previous response is true).
  public var singleUtterance: Bool = false

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _audioConfig: Google_Cloud_Mediatranslation_V1beta1_TranslateSpeechConfig? = nil
}

/// The top-level message sent by the client for the `StreamingTranslateSpeech`
/// method. Multiple `StreamingTranslateSpeechRequest` messages are sent. The
/// first message must contain a `streaming_config` message and must not contain
/// `audio_content` data. All subsequent messages must contain `audio_content`
/// data and must not contain a `streaming_config` message.
public struct Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The streaming request, which is either a streaming config or content.
  public var streamingRequest: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechRequest.OneOf_StreamingRequest? = nil

  /// Provides information to the recognizer that specifies how to process the
  /// request. The first `StreamingTranslateSpeechRequest` message must contain
  /// a `streaming_config` message.
  public var streamingConfig: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechConfig {
    get {
      if case .streamingConfig(let v)? = streamingRequest {return v}
      return Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechConfig()
    }
    set {streamingRequest = .streamingConfig(newValue)}
  }

  /// The audio data to be translated. Sequential chunks of audio data are sent
  /// in sequential `StreamingTranslateSpeechRequest` messages. The first
  /// `StreamingTranslateSpeechRequest` message must not contain
  /// `audio_content` data and all subsequent `StreamingTranslateSpeechRequest`
  /// messages must contain `audio_content` data. The audio bytes must be
  /// encoded as specified in `StreamingTranslateSpeechConfig`. Note: as with
  /// all bytes fields, protobuffers use a pure binary representation (not
  /// base64).
  public var audioContent: Data {
    get {
      if case .audioContent(let v)? = streamingRequest {return v}
      return Data()
    }
    set {streamingRequest = .audioContent(newValue)}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The streaming request, which is either a streaming config or content.
  public enum OneOf_StreamingRequest: Equatable {
    /// Provides information to the recognizer that specifies how to process the
    /// request. The first `StreamingTranslateSpeechRequest` message must contain
    /// a `streaming_config` message.
    case streamingConfig(Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechConfig)
    /// The audio data to be translated. Sequential chunks of audio data are sent
    /// in sequential `StreamingTranslateSpeechRequest` messages. The first
    /// `StreamingTranslateSpeechRequest` message must not contain
    /// `audio_content` data and all subsequent `StreamingTranslateSpeechRequest`
    /// messages must contain `audio_content` data. The audio bytes must be
    /// encoded as specified in `StreamingTranslateSpeechConfig`. Note: as with
    /// all bytes fields, protobuffers use a pure binary representation (not
    /// base64).
    case audioContent(Data)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechRequest.OneOf_StreamingRequest, rhs: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechRequest.OneOf_StreamingRequest) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.streamingConfig, .streamingConfig): return {
        guard case .streamingConfig(let l) = lhs, case .streamingConfig(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.audioContent, .audioContent): return {
        guard case .audioContent(let l) = lhs, case .audioContent(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}
}

/// A streaming speech translation result corresponding to a portion of the audio
/// that is currently being processed.
public struct Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Translation result.
  ///
  /// Use oneof field to reserve for future tts result.
  public var result: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult.OneOf_Result? = nil

  /// Text translation result.
  public var textTranslationResult: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult.TextTranslationResult {
    get {
      if case .textTranslationResult(let v)? = result {return v}
      return Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult.TextTranslationResult()
    }
    set {result = .textTranslationResult(newValue)}
  }

  /// Output only. The debug only recognition result in original language. This field is debug
  /// only and will be set to empty string if not available.
  /// This is implementation detail and will not be backward compatible.
  ///
  /// Still need to decide whether to expose this field by default.
  public var recognitionResult: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Translation result.
  ///
  /// Use oneof field to reserve for future tts result.
  public enum OneOf_Result: Equatable {
    /// Text translation result.
    case textTranslationResult(Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult.TextTranslationResult)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult.OneOf_Result, rhs: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult.OneOf_Result) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.textTranslationResult, .textTranslationResult): return {
        guard case .textTranslationResult(let l) = lhs, case .textTranslationResult(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      }
    }
  #endif
  }

  /// Text translation result.
  public struct TextTranslationResult {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Output only. The translated sentence.
    public var translation: String = String()

    /// Output only. If `false`, this `StreamingTranslateSpeechResult` represents
    /// an interim result that may change. If `true`, this is the final time the
    /// translation service will return this particular
    /// `StreamingTranslateSpeechResult`, the streaming translator will not
    /// return any further hypotheses for this portion of the transcript and
    /// corresponding audio.
    public var isFinal: Bool = false

    /// Output only. The source language code (BCP-47) detected in the audio. Speech
    /// translation result will translate in the most likely language detected
    /// including the alternative source languages and main source_language_code.
    public var detectedSourceLanguageCode: String = String()

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    public init() {}
  }

  public init() {}
}

/// A streaming speech translation response corresponding to a portion of
/// the audio currently processed.
public struct Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. If set, returns a [google.rpc.Status][google.rpc.Status] message that
  /// specifies the error for the operation.
  public var error: Google_Rpc_Status {
    get {return _error ?? Google_Rpc_Status()}
    set {_error = newValue}
  }
  /// Returns true if `error` has been explicitly set.
  public var hasError: Bool {return self._error != nil}
  /// Clears the value of `error`. Subsequent reads from it will return its default value.
  public mutating func clearError() {self._error = nil}

  /// Output only. The translation result that is currently being processed (is_final could be
  /// true or false).
  public var result: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult {
    get {return _result ?? Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult()}
    set {_result = newValue}
  }
  /// Returns true if `result` has been explicitly set.
  public var hasResult: Bool {return self._result != nil}
  /// Clears the value of `result`. Subsequent reads from it will return its default value.
  public mutating func clearResult() {self._result = nil}

  /// Output only. Indicates the type of speech event.
  public var speechEventType: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResponse.SpeechEventType = .unspecified

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Indicates the type of speech event.
  public enum SpeechEventType: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// No speech event specified.
    case unspecified // = 0

    /// This event indicates that the server has detected the end of the user's
    /// speech utterance and expects no additional speech. Therefore, the server
    /// will not process additional audio (although it may subsequently return
    /// additional results). When the client receives 'END_OF_SINGLE_UTTERANCE'
    /// event, the client should stop sending the requests. However, clients
    /// should keep receiving remaining responses until the stream is terminated.
    /// To construct the complete sentence in a streaming way, one should
    /// override (if 'is_final' of previous response is false), or append (if
    /// 'is_final' of previous response is true). This event is only sent if
    /// `single_utterance` was set to `true`, and is not used otherwise.
    case endOfSingleUtterance // = 1
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .endOfSingleUtterance
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .endOfSingleUtterance: return 1
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}

  fileprivate var _error: Google_Rpc_Status? = nil
  fileprivate var _result: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult? = nil
}

#if swift(>=4.2)

extension Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResponse.SpeechEventType: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResponse.SpeechEventType] = [
    .unspecified,
    .endOfSingleUtterance,
  ]
}

#endif  // swift(>=4.2)

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.mediatranslation.v1beta1"

extension Google_Cloud_Mediatranslation_V1beta1_TranslateSpeechConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TranslateSpeechConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "audio_encoding"),
    2: .standard(proto: "source_language_code"),
    6: .standard(proto: "alternative_source_language_codes"),
    3: .standard(proto: "target_language_code"),
    4: .standard(proto: "sample_rate_hertz"),
    5: .same(proto: "model"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.audioEncoding) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.sourceLanguageCode) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.targetLanguageCode) }()
      case 4: try { try decoder.decodeSingularInt32Field(value: &self.sampleRateHertz) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.model) }()
      case 6: try { try decoder.decodeRepeatedStringField(value: &self.alternativeSourceLanguageCodes) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.audioEncoding.isEmpty {
      try visitor.visitSingularStringField(value: self.audioEncoding, fieldNumber: 1)
    }
    if !self.sourceLanguageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.sourceLanguageCode, fieldNumber: 2)
    }
    if !self.targetLanguageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.targetLanguageCode, fieldNumber: 3)
    }
    if self.sampleRateHertz != 0 {
      try visitor.visitSingularInt32Field(value: self.sampleRateHertz, fieldNumber: 4)
    }
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 5)
    }
    if !self.alternativeSourceLanguageCodes.isEmpty {
      try visitor.visitRepeatedStringField(value: self.alternativeSourceLanguageCodes, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Mediatranslation_V1beta1_TranslateSpeechConfig, rhs: Google_Cloud_Mediatranslation_V1beta1_TranslateSpeechConfig) -> Bool {
    if lhs.audioEncoding != rhs.audioEncoding {return false}
    if lhs.sourceLanguageCode != rhs.sourceLanguageCode {return false}
    if lhs.alternativeSourceLanguageCodes != rhs.alternativeSourceLanguageCodes {return false}
    if lhs.targetLanguageCode != rhs.targetLanguageCode {return false}
    if lhs.sampleRateHertz != rhs.sampleRateHertz {return false}
    if lhs.model != rhs.model {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingTranslateSpeechConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "audio_config"),
    2: .standard(proto: "single_utterance"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._audioConfig) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.singleUtterance) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._audioConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if self.singleUtterance != false {
      try visitor.visitSingularBoolField(value: self.singleUtterance, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechConfig, rhs: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechConfig) -> Bool {
    if lhs._audioConfig != rhs._audioConfig {return false}
    if lhs.singleUtterance != rhs.singleUtterance {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingTranslateSpeechRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "streaming_config"),
    2: .standard(proto: "audio_content"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        var v: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechConfig?
        if let current = self.streamingRequest {
          try decoder.handleConflictingOneOf()
          if case .streamingConfig(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.streamingRequest = .streamingConfig(v)}
      }()
      case 2: try {
        if self.streamingRequest != nil {try decoder.handleConflictingOneOf()}
        var v: Data?
        try decoder.decodeSingularBytesField(value: &v)
        if let v = v {self.streamingRequest = .audioContent(v)}
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.streamingRequest {
    case .streamingConfig?: try {
      guard case .streamingConfig(let v)? = self.streamingRequest else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }()
    case .audioContent?: try {
      guard case .audioContent(let v)? = self.streamingRequest else { preconditionFailure() }
      try visitor.visitSingularBytesField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechRequest, rhs: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechRequest) -> Bool {
    if lhs.streamingRequest != rhs.streamingRequest {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingTranslateSpeechResult"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "text_translation_result"),
    3: .standard(proto: "recognition_result"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        var v: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult.TextTranslationResult?
        if let current = self.result {
          try decoder.handleConflictingOneOf()
          if case .textTranslationResult(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.result = .textTranslationResult(v)}
      }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.recognitionResult) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if case .textTranslationResult(let v)? = self.result {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if !self.recognitionResult.isEmpty {
      try visitor.visitSingularStringField(value: self.recognitionResult, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult, rhs: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult) -> Bool {
    if lhs.result != rhs.result {return false}
    if lhs.recognitionResult != rhs.recognitionResult {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult.TextTranslationResult: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult.protoMessageName + ".TextTranslationResult"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "translation"),
    2: .standard(proto: "is_final"),
    3: .standard(proto: "detected_source_language_code"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.translation) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.isFinal) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.detectedSourceLanguageCode) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.translation.isEmpty {
      try visitor.visitSingularStringField(value: self.translation, fieldNumber: 1)
    }
    if self.isFinal != false {
      try visitor.visitSingularBoolField(value: self.isFinal, fieldNumber: 2)
    }
    if !self.detectedSourceLanguageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.detectedSourceLanguageCode, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult.TextTranslationResult, rhs: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResult.TextTranslationResult) -> Bool {
    if lhs.translation != rhs.translation {return false}
    if lhs.isFinal != rhs.isFinal {return false}
    if lhs.detectedSourceLanguageCode != rhs.detectedSourceLanguageCode {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".StreamingTranslateSpeechResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "error"),
    2: .same(proto: "result"),
    3: .standard(proto: "speech_event_type"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._error) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._result) }()
      case 3: try { try decoder.decodeSingularEnumField(value: &self.speechEventType) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._error {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._result {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if self.speechEventType != .unspecified {
      try visitor.visitSingularEnumField(value: self.speechEventType, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResponse, rhs: Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResponse) -> Bool {
    if lhs._error != rhs._error {return false}
    if lhs._result != rhs._result {return false}
    if lhs.speechEventType != rhs.speechEventType {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Mediatranslation_V1beta1_StreamingTranslateSpeechResponse.SpeechEventType: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "SPEECH_EVENT_TYPE_UNSPECIFIED"),
    1: .same(proto: "END_OF_SINGLE_UTTERANCE"),
  ]
}
