//
// DO NOT EDIT.
//
// Generated by the protocol buffer compiler.
// Source: google/cloud/automl/v1beta1/prediction_service.proto
//

//
// Copyright 2018, gRPC Authors All rights reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
import GRPC
import NIO
import SwiftProtobuf


/// Usage: instantiate Google_Cloud_Automl_V1beta1_PredictionServiceClient, then call methods of this protocol to make API calls.
public protocol Google_Cloud_Automl_V1beta1_PredictionServiceClientProtocol: GRPCClient {
  func predict(
    _ request: Google_Cloud_Automl_V1beta1_PredictRequest,
    callOptions: CallOptions?
  ) -> UnaryCall<Google_Cloud_Automl_V1beta1_PredictRequest, Google_Cloud_Automl_V1beta1_PredictResponse>

  func batchPredict(
    _ request: Google_Cloud_Automl_V1beta1_BatchPredictRequest,
    callOptions: CallOptions?
  ) -> UnaryCall<Google_Cloud_Automl_V1beta1_BatchPredictRequest, Google_Longrunning_Operation>

}

extension Google_Cloud_Automl_V1beta1_PredictionServiceClientProtocol {

  /// Perform an online prediction. The prediction result will be directly
  /// returned in the response.
  /// Available for following ML problems, and their expected request payloads:
  /// * Image Classification - Image in .JPEG, .GIF or .PNG format, image_bytes
  ///                          up to 30MB.
  /// * Image Object Detection - Image in .JPEG, .GIF or .PNG format, image_bytes
  ///                            up to 30MB.
  /// * Text Classification - TextSnippet, content up to 60,000 characters,
  ///                         UTF-8 encoded.
  /// * Text Extraction - TextSnippet, content up to 30,000 characters,
  ///                     UTF-8 NFC encoded.
  /// * Translation - TextSnippet, content up to 25,000 characters, UTF-8
  ///                 encoded.
  /// * Tables - Row, with column values matching the columns of the model,
  ///            up to 5MB. Not available for FORECASTING
  ///
  /// [prediction_type][google.cloud.automl.v1beta1.TablesModelMetadata.prediction_type].
  /// * Text Sentiment - TextSnippet, content up 500 characters, UTF-8
  ///                     encoded.
  ///
  /// - Parameters:
  ///   - request: Request to send to Predict.
  ///   - callOptions: Call options.
  /// - Returns: A `UnaryCall` with futures for the metadata, status and response.
  public func predict(
    _ request: Google_Cloud_Automl_V1beta1_PredictRequest,
    callOptions: CallOptions? = nil
  ) -> UnaryCall<Google_Cloud_Automl_V1beta1_PredictRequest, Google_Cloud_Automl_V1beta1_PredictResponse> {
    return self.makeUnaryCall(
      path: "/google.cloud.automl.v1beta1.PredictionService/Predict",
      request: request,
      callOptions: callOptions ?? self.defaultCallOptions
    )
  }

  /// Perform a batch prediction. Unlike the online [Predict][google.cloud.automl.v1beta1.PredictionService.Predict], batch
  /// prediction result won't be immediately available in the response. Instead,
  /// a long running operation object is returned. User can poll the operation
  /// result via [GetOperation][google.longrunning.Operations.GetOperation]
  /// method. Once the operation is done, [BatchPredictResult][google.cloud.automl.v1beta1.BatchPredictResult] is returned in
  /// the [response][google.longrunning.Operation.response] field.
  /// Available for following ML problems:
  /// * Image Classification
  /// * Image Object Detection
  /// * Video Classification
  /// * Video Object Tracking * Text Extraction
  /// * Tables
  ///
  /// - Parameters:
  ///   - request: Request to send to BatchPredict.
  ///   - callOptions: Call options.
  /// - Returns: A `UnaryCall` with futures for the metadata, status and response.
  public func batchPredict(
    _ request: Google_Cloud_Automl_V1beta1_BatchPredictRequest,
    callOptions: CallOptions? = nil
  ) -> UnaryCall<Google_Cloud_Automl_V1beta1_BatchPredictRequest, Google_Longrunning_Operation> {
    return self.makeUnaryCall(
      path: "/google.cloud.automl.v1beta1.PredictionService/BatchPredict",
      request: request,
      callOptions: callOptions ?? self.defaultCallOptions
    )
  }
}

public final class Google_Cloud_Automl_V1beta1_PredictionServiceClient: Google_Cloud_Automl_V1beta1_PredictionServiceClientProtocol {
  public let channel: GRPCChannel
  public var defaultCallOptions: CallOptions

  /// Creates a client for the google.cloud.automl.v1beta1.PredictionService service.
  ///
  /// - Parameters:
  ///   - channel: `GRPCChannel` to the service host.
  ///   - defaultCallOptions: Options to use for each service call if the user doesn't provide them.
  public init(channel: GRPCChannel, defaultCallOptions: CallOptions = CallOptions()) {
    self.channel = channel
    self.defaultCallOptions = defaultCallOptions
  }
}

/// To build a server, implement a class that conforms to this protocol.
public protocol Google_Cloud_Automl_V1beta1_PredictionServiceProvider: CallHandlerProvider {
  /// Perform an online prediction. The prediction result will be directly
  /// returned in the response.
  /// Available for following ML problems, and their expected request payloads:
  /// * Image Classification - Image in .JPEG, .GIF or .PNG format, image_bytes
  ///                          up to 30MB.
  /// * Image Object Detection - Image in .JPEG, .GIF or .PNG format, image_bytes
  ///                            up to 30MB.
  /// * Text Classification - TextSnippet, content up to 60,000 characters,
  ///                         UTF-8 encoded.
  /// * Text Extraction - TextSnippet, content up to 30,000 characters,
  ///                     UTF-8 NFC encoded.
  /// * Translation - TextSnippet, content up to 25,000 characters, UTF-8
  ///                 encoded.
  /// * Tables - Row, with column values matching the columns of the model,
  ///            up to 5MB. Not available for FORECASTING
  ///
  /// [prediction_type][google.cloud.automl.v1beta1.TablesModelMetadata.prediction_type].
  /// * Text Sentiment - TextSnippet, content up 500 characters, UTF-8
  ///                     encoded.
  func predict(request: Google_Cloud_Automl_V1beta1_PredictRequest, context: StatusOnlyCallContext) -> EventLoopFuture<Google_Cloud_Automl_V1beta1_PredictResponse>
  /// Perform a batch prediction. Unlike the online [Predict][google.cloud.automl.v1beta1.PredictionService.Predict], batch
  /// prediction result won't be immediately available in the response. Instead,
  /// a long running operation object is returned. User can poll the operation
  /// result via [GetOperation][google.longrunning.Operations.GetOperation]
  /// method. Once the operation is done, [BatchPredictResult][google.cloud.automl.v1beta1.BatchPredictResult] is returned in
  /// the [response][google.longrunning.Operation.response] field.
  /// Available for following ML problems:
  /// * Image Classification
  /// * Image Object Detection
  /// * Video Classification
  /// * Video Object Tracking * Text Extraction
  /// * Tables
  func batchPredict(request: Google_Cloud_Automl_V1beta1_BatchPredictRequest, context: StatusOnlyCallContext) -> EventLoopFuture<Google_Longrunning_Operation>
}

extension Google_Cloud_Automl_V1beta1_PredictionServiceProvider {
  public var serviceName: Substring { return "google.cloud.automl.v1beta1.PredictionService" }

  /// Determines, calls and returns the appropriate request handler, depending on the request's method.
  /// Returns nil for methods not handled by this service.
  public func handleMethod(_ methodName: Substring, callHandlerContext: CallHandlerContext) -> GRPCCallHandler? {
    switch methodName {
    case "Predict":
      return CallHandlerFactory.makeUnary(callHandlerContext: callHandlerContext) { context in
        return { request in
          self.predict(request: request, context: context)
        }
      }

    case "BatchPredict":
      return CallHandlerFactory.makeUnary(callHandlerContext: callHandlerContext) { context in
        return { request in
          self.batchPredict(request: request, context: context)
        }
      }

    default: return nil
    }
  }
}

