// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/dataproc/v1beta2/workflow_templates.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2020 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// A Dataproc workflow template resource.
public struct Google_Cloud_Dataproc_V1beta2_WorkflowTemplate {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The template id.
  ///
  /// The id must contain only letters (a-z, A-Z), numbers (0-9),
  /// underscores (_), and hyphens (-). Cannot begin or end with underscore
  /// or hyphen. Must consist of between 3 and 50 characters.
  ///
  /// .
  public var id: String = String()

  /// Output only. The resource name of the workflow template, as described
  /// in https://cloud.google.com/apis/design/resource_names.
  ///
  /// * For `projects.regions.workflowTemplates`, the resource name of the
  ///   template has the following format:
  ///   `projects/{project_id}/regions/{region}/workflowTemplates/{template_id}`
  ///
  /// * For `projects.locations.workflowTemplates`, the resource name of the
  ///   template has the following format:
  ///   `projects/{project_id}/locations/{location}/workflowTemplates/{template_id}`
  public var name: String = String()

  /// Optional. Used to perform a consistent read-modify-write.
  ///
  /// This field should be left blank for a `CreateWorkflowTemplate` request. It
  /// is required for an `UpdateWorkflowTemplate` request, and must match the
  /// current server version. A typical update template flow would fetch the
  /// current template with a `GetWorkflowTemplate` request, which will return
  /// the current template with the `version` field filled in with the
  /// current server version. The user updates other fields in the template,
  /// then returns it as part of the `UpdateWorkflowTemplate` request.
  public var version: Int32 = 0

  /// Output only. The time template was created.
  public var createTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _createTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_createTime = newValue}
  }
  /// Returns true if `createTime` has been explicitly set.
  public var hasCreateTime: Bool {return self._createTime != nil}
  /// Clears the value of `createTime`. Subsequent reads from it will return its default value.
  public mutating func clearCreateTime() {self._createTime = nil}

  /// Output only. The time template was last updated.
  public var updateTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _updateTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_updateTime = newValue}
  }
  /// Returns true if `updateTime` has been explicitly set.
  public var hasUpdateTime: Bool {return self._updateTime != nil}
  /// Clears the value of `updateTime`. Subsequent reads from it will return its default value.
  public mutating func clearUpdateTime() {self._updateTime = nil}

  /// Optional. The labels to associate with this template. These labels
  /// will be propagated to all jobs and clusters created by the workflow
  /// instance.
  ///
  /// Label **keys** must contain 1 to 63 characters, and must conform to
  /// [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
  ///
  /// Label **values** may be empty, but, if present, must contain 1 to 63
  /// characters, and must conform to
  /// [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
  ///
  /// No more than 32 labels can be associated with a template.
  public var labels: Dictionary<String,String> = [:]

  /// Required. WorkflowTemplate scheduling information.
  public var placement: Google_Cloud_Dataproc_V1beta2_WorkflowTemplatePlacement {
    get {return _placement ?? Google_Cloud_Dataproc_V1beta2_WorkflowTemplatePlacement()}
    set {_placement = newValue}
  }
  /// Returns true if `placement` has been explicitly set.
  public var hasPlacement: Bool {return self._placement != nil}
  /// Clears the value of `placement`. Subsequent reads from it will return its default value.
  public mutating func clearPlacement() {self._placement = nil}

  /// Required. The Directed Acyclic Graph of Jobs to submit.
  public var jobs: [Google_Cloud_Dataproc_V1beta2_OrderedJob] = []

  /// Optional. Template parameters whose values are substituted into the
  /// template. Values for parameters must be provided when the template is
  /// instantiated.
  public var parameters: [Google_Cloud_Dataproc_V1beta2_TemplateParameter] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _createTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
  fileprivate var _updateTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
  fileprivate var _placement: Google_Cloud_Dataproc_V1beta2_WorkflowTemplatePlacement? = nil
}

/// Specifies workflow execution target.
///
/// Either `managed_cluster` or `cluster_selector` is required.
public struct Google_Cloud_Dataproc_V1beta2_WorkflowTemplatePlacement {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Specifies where workflow executes; either on a managed
  /// cluster or an existing cluster chosen by labels.
  public var placement: Google_Cloud_Dataproc_V1beta2_WorkflowTemplatePlacement.OneOf_Placement? = nil

  /// Optional. A cluster that is managed by the workflow.
  public var managedCluster: Google_Cloud_Dataproc_V1beta2_ManagedCluster {
    get {
      if case .managedCluster(let v)? = placement {return v}
      return Google_Cloud_Dataproc_V1beta2_ManagedCluster()
    }
    set {placement = .managedCluster(newValue)}
  }

  /// Optional. A selector that chooses target cluster for jobs based
  /// on metadata.
  ///
  /// The selector is evaluated at the time each job is submitted.
  public var clusterSelector: Google_Cloud_Dataproc_V1beta2_ClusterSelector {
    get {
      if case .clusterSelector(let v)? = placement {return v}
      return Google_Cloud_Dataproc_V1beta2_ClusterSelector()
    }
    set {placement = .clusterSelector(newValue)}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Required. Specifies where workflow executes; either on a managed
  /// cluster or an existing cluster chosen by labels.
  public enum OneOf_Placement: Equatable {
    /// Optional. A cluster that is managed by the workflow.
    case managedCluster(Google_Cloud_Dataproc_V1beta2_ManagedCluster)
    /// Optional. A selector that chooses target cluster for jobs based
    /// on metadata.
    ///
    /// The selector is evaluated at the time each job is submitted.
    case clusterSelector(Google_Cloud_Dataproc_V1beta2_ClusterSelector)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_WorkflowTemplatePlacement.OneOf_Placement, rhs: Google_Cloud_Dataproc_V1beta2_WorkflowTemplatePlacement.OneOf_Placement) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.managedCluster, .managedCluster): return {
        guard case .managedCluster(let l) = lhs, case .managedCluster(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.clusterSelector, .clusterSelector): return {
        guard case .clusterSelector(let l) = lhs, case .clusterSelector(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}
}

/// Cluster that is managed by the workflow.
public struct Google_Cloud_Dataproc_V1beta2_ManagedCluster {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The cluster name prefix. A unique cluster name will be formed by
  /// appending a random suffix.
  ///
  /// The name must contain only lower-case letters (a-z), numbers (0-9),
  /// and hyphens (-). Must begin with a letter. Cannot begin or end with
  /// hyphen. Must consist of between 2 and 35 characters.
  public var clusterName: String = String()

  /// Required. The cluster configuration.
  public var config: Google_Cloud_Dataproc_V1beta2_ClusterConfig {
    get {return _config ?? Google_Cloud_Dataproc_V1beta2_ClusterConfig()}
    set {_config = newValue}
  }
  /// Returns true if `config` has been explicitly set.
  public var hasConfig: Bool {return self._config != nil}
  /// Clears the value of `config`. Subsequent reads from it will return its default value.
  public mutating func clearConfig() {self._config = nil}

  /// Optional. The labels to associate with this cluster.
  ///
  /// Label keys must be between 1 and 63 characters long, and must conform to
  /// the following PCRE regular expression:
  /// [\p{Ll}\p{Lo}][\p{Ll}\p{Lo}\p{N}_-]{0,62}
  ///
  /// Label values must be between 1 and 63 characters long, and must conform to
  /// the following PCRE regular expression: [\p{Ll}\p{Lo}\p{N}_-]{0,63}
  ///
  /// No more than 32 labels can be associated with a given cluster.
  public var labels: Dictionary<String,String> = [:]

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _config: Google_Cloud_Dataproc_V1beta2_ClusterConfig? = nil
}

/// A selector that chooses target cluster for jobs based on metadata.
public struct Google_Cloud_Dataproc_V1beta2_ClusterSelector {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. The zone where workflow process executes. This parameter does not
  /// affect the selection of the cluster.
  ///
  /// If unspecified, the zone of the first cluster matching the selector
  /// is used.
  public var zone: String = String()

  /// Required. The cluster labels. Cluster must have all labels
  /// to match.
  public var clusterLabels: Dictionary<String,String> = [:]

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A job executed by the workflow.
public struct Google_Cloud_Dataproc_V1beta2_OrderedJob {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The step id. The id must be unique among all jobs
  /// within the template.
  ///
  /// The step id is used as prefix for job id, as job
  /// `goog-dataproc-workflow-step-id` label, and in
  /// [prerequisiteStepIds][google.cloud.dataproc.v1beta2.OrderedJob.prerequisite_step_ids] field from other
  /// steps.
  ///
  /// The id must contain only letters (a-z, A-Z), numbers (0-9),
  /// underscores (_), and hyphens (-). Cannot begin or end with underscore
  /// or hyphen. Must consist of between 3 and 50 characters.
  public var stepID: String = String()

  /// Required. The job definition.
  public var jobType: Google_Cloud_Dataproc_V1beta2_OrderedJob.OneOf_JobType? = nil

  public var hadoopJob: Google_Cloud_Dataproc_V1beta2_HadoopJob {
    get {
      if case .hadoopJob(let v)? = jobType {return v}
      return Google_Cloud_Dataproc_V1beta2_HadoopJob()
    }
    set {jobType = .hadoopJob(newValue)}
  }

  public var sparkJob: Google_Cloud_Dataproc_V1beta2_SparkJob {
    get {
      if case .sparkJob(let v)? = jobType {return v}
      return Google_Cloud_Dataproc_V1beta2_SparkJob()
    }
    set {jobType = .sparkJob(newValue)}
  }

  public var pysparkJob: Google_Cloud_Dataproc_V1beta2_PySparkJob {
    get {
      if case .pysparkJob(let v)? = jobType {return v}
      return Google_Cloud_Dataproc_V1beta2_PySparkJob()
    }
    set {jobType = .pysparkJob(newValue)}
  }

  public var hiveJob: Google_Cloud_Dataproc_V1beta2_HiveJob {
    get {
      if case .hiveJob(let v)? = jobType {return v}
      return Google_Cloud_Dataproc_V1beta2_HiveJob()
    }
    set {jobType = .hiveJob(newValue)}
  }

  public var pigJob: Google_Cloud_Dataproc_V1beta2_PigJob {
    get {
      if case .pigJob(let v)? = jobType {return v}
      return Google_Cloud_Dataproc_V1beta2_PigJob()
    }
    set {jobType = .pigJob(newValue)}
  }

  /// Spark R job
  public var sparkRJob: Google_Cloud_Dataproc_V1beta2_SparkRJob {
    get {
      if case .sparkRJob(let v)? = jobType {return v}
      return Google_Cloud_Dataproc_V1beta2_SparkRJob()
    }
    set {jobType = .sparkRJob(newValue)}
  }

  public var sparkSqlJob: Google_Cloud_Dataproc_V1beta2_SparkSqlJob {
    get {
      if case .sparkSqlJob(let v)? = jobType {return v}
      return Google_Cloud_Dataproc_V1beta2_SparkSqlJob()
    }
    set {jobType = .sparkSqlJob(newValue)}
  }

  /// Presto job
  public var prestoJob: Google_Cloud_Dataproc_V1beta2_PrestoJob {
    get {
      if case .prestoJob(let v)? = jobType {return v}
      return Google_Cloud_Dataproc_V1beta2_PrestoJob()
    }
    set {jobType = .prestoJob(newValue)}
  }

  /// Optional. The labels to associate with this job.
  ///
  /// Label keys must be between 1 and 63 characters long, and must conform to
  /// the following regular expression:
  /// [\p{Ll}\p{Lo}][\p{Ll}\p{Lo}\p{N}_-]{0,62}
  ///
  /// Label values must be between 1 and 63 characters long, and must conform to
  /// the following regular expression: [\p{Ll}\p{Lo}\p{N}_-]{0,63}
  ///
  /// No more than 32 labels can be associated with a given job.
  public var labels: Dictionary<String,String> = [:]

  /// Optional. Job scheduling configuration.
  public var scheduling: Google_Cloud_Dataproc_V1beta2_JobScheduling {
    get {return _scheduling ?? Google_Cloud_Dataproc_V1beta2_JobScheduling()}
    set {_scheduling = newValue}
  }
  /// Returns true if `scheduling` has been explicitly set.
  public var hasScheduling: Bool {return self._scheduling != nil}
  /// Clears the value of `scheduling`. Subsequent reads from it will return its default value.
  public mutating func clearScheduling() {self._scheduling = nil}

  /// Optional. The optional list of prerequisite job step_ids.
  /// If not specified, the job will start at the beginning of workflow.
  public var prerequisiteStepIds: [String] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Required. The job definition.
  public enum OneOf_JobType: Equatable {
    case hadoopJob(Google_Cloud_Dataproc_V1beta2_HadoopJob)
    case sparkJob(Google_Cloud_Dataproc_V1beta2_SparkJob)
    case pysparkJob(Google_Cloud_Dataproc_V1beta2_PySparkJob)
    case hiveJob(Google_Cloud_Dataproc_V1beta2_HiveJob)
    case pigJob(Google_Cloud_Dataproc_V1beta2_PigJob)
    /// Spark R job
    case sparkRJob(Google_Cloud_Dataproc_V1beta2_SparkRJob)
    case sparkSqlJob(Google_Cloud_Dataproc_V1beta2_SparkSqlJob)
    /// Presto job
    case prestoJob(Google_Cloud_Dataproc_V1beta2_PrestoJob)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_OrderedJob.OneOf_JobType, rhs: Google_Cloud_Dataproc_V1beta2_OrderedJob.OneOf_JobType) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.hadoopJob, .hadoopJob): return {
        guard case .hadoopJob(let l) = lhs, case .hadoopJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.sparkJob, .sparkJob): return {
        guard case .sparkJob(let l) = lhs, case .sparkJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.pysparkJob, .pysparkJob): return {
        guard case .pysparkJob(let l) = lhs, case .pysparkJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.hiveJob, .hiveJob): return {
        guard case .hiveJob(let l) = lhs, case .hiveJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.pigJob, .pigJob): return {
        guard case .pigJob(let l) = lhs, case .pigJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.sparkRJob, .sparkRJob): return {
        guard case .sparkRJob(let l) = lhs, case .sparkRJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.sparkSqlJob, .sparkSqlJob): return {
        guard case .sparkSqlJob(let l) = lhs, case .sparkSqlJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.prestoJob, .prestoJob): return {
        guard case .prestoJob(let l) = lhs, case .prestoJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}

  fileprivate var _scheduling: Google_Cloud_Dataproc_V1beta2_JobScheduling? = nil
}

/// A configurable parameter that replaces one or more fields in the template.
/// Parameterizable fields:
/// - Labels
/// - File uris
/// - Job properties
/// - Job arguments
/// - Script variables
/// - Main class (in HadoopJob and SparkJob)
/// - Zone (in ClusterSelector)
public struct Google_Cloud_Dataproc_V1beta2_TemplateParameter {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Parameter name.
  /// The parameter name is used as the key, and paired with the
  /// parameter value, which are passed to the template when the template
  /// is instantiated.
  /// The name must contain only capital letters (A-Z), numbers (0-9), and
  /// underscores (_), and must not start with a number. The maximum length is
  /// 40 characters.
  public var name: String = String()

  /// Required. Paths to all fields that the parameter replaces.
  /// A field is allowed to appear in at most one parameter's list of field
  /// paths.
  ///
  /// A field path is similar in syntax to a [google.protobuf.FieldMask][google.protobuf.FieldMask].
  /// For example, a field path that references the zone field of a workflow
  /// template's cluster selector would be specified as
  /// `placement.clusterSelector.zone`.
  ///
  /// Also, field paths can reference fields using the following syntax:
  ///
  /// * Values in maps can be referenced by key:
  ///     * labels['key']
  ///     * placement.clusterSelector.clusterLabels['key']
  ///     * placement.managedCluster.labels['key']
  ///     * placement.clusterSelector.clusterLabels['key']
  ///     * jobs['step-id'].labels['key']
  ///
  /// * Jobs in the jobs list can be referenced by step-id:
  ///     * jobs['step-id'].hadoopJob.mainJarFileUri
  ///     * jobs['step-id'].hiveJob.queryFileUri
  ///     * jobs['step-id'].pySparkJob.mainPythonFileUri
  ///     * jobs['step-id'].hadoopJob.jarFileUris[0]
  ///     * jobs['step-id'].hadoopJob.archiveUris[0]
  ///     * jobs['step-id'].hadoopJob.fileUris[0]
  ///     * jobs['step-id'].pySparkJob.pythonFileUris[0]
  ///
  /// * Items in repeated fields can be referenced by a zero-based index:
  ///     * jobs['step-id'].sparkJob.args[0]
  ///
  /// * Other examples:
  ///     * jobs['step-id'].hadoopJob.properties['key']
  ///     * jobs['step-id'].hadoopJob.args[0]
  ///     * jobs['step-id'].hiveJob.scriptVariables['key']
  ///     * jobs['step-id'].hadoopJob.mainJarFileUri
  ///     * placement.clusterSelector.zone
  ///
  /// It may not be possible to parameterize maps and repeated fields in their
  /// entirety since only individual map values and individual items in repeated
  /// fields can be referenced. For example, the following field paths are
  /// invalid:
  ///
  /// - placement.clusterSelector.clusterLabels
  /// - jobs['step-id'].sparkJob.args
  public var fields: [String] = []

  /// Optional. Brief description of the parameter.
  /// Must not exceed 1024 characters.
  public var description_p: String = String()

  /// Optional. Validation rules to be applied to this parameter's value.
  public var validation: Google_Cloud_Dataproc_V1beta2_ParameterValidation {
    get {return _validation ?? Google_Cloud_Dataproc_V1beta2_ParameterValidation()}
    set {_validation = newValue}
  }
  /// Returns true if `validation` has been explicitly set.
  public var hasValidation: Bool {return self._validation != nil}
  /// Clears the value of `validation`. Subsequent reads from it will return its default value.
  public mutating func clearValidation() {self._validation = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _validation: Google_Cloud_Dataproc_V1beta2_ParameterValidation? = nil
}

/// Configuration for parameter validation.
public struct Google_Cloud_Dataproc_V1beta2_ParameterValidation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The type of validation to be performed.
  public var validationType: Google_Cloud_Dataproc_V1beta2_ParameterValidation.OneOf_ValidationType? = nil

  /// Validation based on regular expressions.
  public var regex: Google_Cloud_Dataproc_V1beta2_RegexValidation {
    get {
      if case .regex(let v)? = validationType {return v}
      return Google_Cloud_Dataproc_V1beta2_RegexValidation()
    }
    set {validationType = .regex(newValue)}
  }

  /// Validation based on a list of allowed values.
  public var values: Google_Cloud_Dataproc_V1beta2_ValueValidation {
    get {
      if case .values(let v)? = validationType {return v}
      return Google_Cloud_Dataproc_V1beta2_ValueValidation()
    }
    set {validationType = .values(newValue)}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Required. The type of validation to be performed.
  public enum OneOf_ValidationType: Equatable {
    /// Validation based on regular expressions.
    case regex(Google_Cloud_Dataproc_V1beta2_RegexValidation)
    /// Validation based on a list of allowed values.
    case values(Google_Cloud_Dataproc_V1beta2_ValueValidation)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ParameterValidation.OneOf_ValidationType, rhs: Google_Cloud_Dataproc_V1beta2_ParameterValidation.OneOf_ValidationType) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.regex, .regex): return {
        guard case .regex(let l) = lhs, case .regex(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.values, .values): return {
        guard case .values(let l) = lhs, case .values(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}
}

/// Validation based on regular expressions.
public struct Google_Cloud_Dataproc_V1beta2_RegexValidation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. RE2 regular expressions used to validate the parameter's value.
  /// The value must match the regex in its entirety (substring
  /// matches are not sufficient).
  public var regexes: [String] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Validation based on a list of allowed values.
public struct Google_Cloud_Dataproc_V1beta2_ValueValidation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. List of allowed values for the parameter.
  public var values: [String] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A Dataproc workflow template resource.
public struct Google_Cloud_Dataproc_V1beta2_WorkflowMetadata {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The resource name of the workflow template as described
  /// in https://cloud.google.com/apis/design/resource_names.
  ///
  /// * For `projects.regions.workflowTemplates`, the resource name of the
  ///   template has the following format:
  ///   `projects/{project_id}/regions/{region}/workflowTemplates/{template_id}`
  ///
  /// * For `projects.locations.workflowTemplates`, the resource name of the
  ///   template has the following format:
  ///   `projects/{project_id}/locations/{location}/workflowTemplates/{template_id}`
  public var template: String {
    get {return _storage._template}
    set {_uniqueStorage()._template = newValue}
  }

  /// Output only. The version of template at the time of
  /// workflow instantiation.
  public var version: Int32 {
    get {return _storage._version}
    set {_uniqueStorage()._version = newValue}
  }

  /// Output only. The create cluster operation metadata.
  public var createCluster: Google_Cloud_Dataproc_V1beta2_ClusterOperation {
    get {return _storage._createCluster ?? Google_Cloud_Dataproc_V1beta2_ClusterOperation()}
    set {_uniqueStorage()._createCluster = newValue}
  }
  /// Returns true if `createCluster` has been explicitly set.
  public var hasCreateCluster: Bool {return _storage._createCluster != nil}
  /// Clears the value of `createCluster`. Subsequent reads from it will return its default value.
  public mutating func clearCreateCluster() {_uniqueStorage()._createCluster = nil}

  /// Output only. The workflow graph.
  public var graph: Google_Cloud_Dataproc_V1beta2_WorkflowGraph {
    get {return _storage._graph ?? Google_Cloud_Dataproc_V1beta2_WorkflowGraph()}
    set {_uniqueStorage()._graph = newValue}
  }
  /// Returns true if `graph` has been explicitly set.
  public var hasGraph: Bool {return _storage._graph != nil}
  /// Clears the value of `graph`. Subsequent reads from it will return its default value.
  public mutating func clearGraph() {_uniqueStorage()._graph = nil}

  /// Output only. The delete cluster operation metadata.
  public var deleteCluster: Google_Cloud_Dataproc_V1beta2_ClusterOperation {
    get {return _storage._deleteCluster ?? Google_Cloud_Dataproc_V1beta2_ClusterOperation()}
    set {_uniqueStorage()._deleteCluster = newValue}
  }
  /// Returns true if `deleteCluster` has been explicitly set.
  public var hasDeleteCluster: Bool {return _storage._deleteCluster != nil}
  /// Clears the value of `deleteCluster`. Subsequent reads from it will return its default value.
  public mutating func clearDeleteCluster() {_uniqueStorage()._deleteCluster = nil}

  /// Output only. The workflow state.
  public var state: Google_Cloud_Dataproc_V1beta2_WorkflowMetadata.State {
    get {return _storage._state}
    set {_uniqueStorage()._state = newValue}
  }

  /// Output only. The name of the target cluster.
  public var clusterName: String {
    get {return _storage._clusterName}
    set {_uniqueStorage()._clusterName = newValue}
  }

  /// Map from parameter names to values that were used for those parameters.
  public var parameters: Dictionary<String,String> {
    get {return _storage._parameters}
    set {_uniqueStorage()._parameters = newValue}
  }

  /// Output only. Workflow start time.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return _storage._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {_uniqueStorage()._startTime = nil}

  /// Output only. Workflow end time.
  public var endTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _storage._endTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_uniqueStorage()._endTime = newValue}
  }
  /// Returns true if `endTime` has been explicitly set.
  public var hasEndTime: Bool {return _storage._endTime != nil}
  /// Clears the value of `endTime`. Subsequent reads from it will return its default value.
  public mutating func clearEndTime() {_uniqueStorage()._endTime = nil}

  /// Output only. The UUID of target cluster.
  public var clusterUuid: String {
    get {return _storage._clusterUuid}
    set {_uniqueStorage()._clusterUuid = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The operation state.
  public enum State: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// Unused.
    case unknown // = 0

    /// The operation has been created.
    case pending // = 1

    /// The operation is running.
    case running // = 2

    /// The operation is done; either cancelled or completed.
    case done // = 3
    case UNRECOGNIZED(Int)

    public init() {
      self = .unknown
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unknown
      case 1: self = .pending
      case 2: self = .running
      case 3: self = .done
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unknown: return 0
      case .pending: return 1
      case .running: return 2
      case .done: return 3
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

#if swift(>=4.2)

extension Google_Cloud_Dataproc_V1beta2_WorkflowMetadata.State: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dataproc_V1beta2_WorkflowMetadata.State] = [
    .unknown,
    .pending,
    .running,
    .done,
  ]
}

#endif  // swift(>=4.2)

/// The cluster operation triggered by a workflow.
public struct Google_Cloud_Dataproc_V1beta2_ClusterOperation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The id of the cluster operation.
  public var operationID: String = String()

  /// Output only. Error, if operation failed.
  public var error: String = String()

  /// Output only. Indicates the operation is done.
  public var done: Bool = false

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// The workflow graph.
public struct Google_Cloud_Dataproc_V1beta2_WorkflowGraph {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The workflow nodes.
  public var nodes: [Google_Cloud_Dataproc_V1beta2_WorkflowNode] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// The workflow node.
public struct Google_Cloud_Dataproc_V1beta2_WorkflowNode {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The name of the node.
  public var stepID: String = String()

  /// Output only. Node's prerequisite nodes.
  public var prerequisiteStepIds: [String] = []

  /// Output only. The job id; populated after the node enters RUNNING state.
  public var jobID: String = String()

  /// Output only. The node state.
  public var state: Google_Cloud_Dataproc_V1beta2_WorkflowNode.NodeState = .nodeStatusUnspecified

  /// Output only. The error detail.
  public var error: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The workflow node state.
  public enum NodeState: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// State is unspecified.
    case nodeStatusUnspecified // = 0

    /// The node is awaiting prerequisite node to finish.
    case blocked // = 1

    /// The node is runnable but not running.
    case runnable // = 2

    /// The node is running.
    case running // = 3

    /// The node completed successfully.
    case completed // = 4

    /// The node failed. A node can be marked FAILED because
    /// its ancestor or peer failed.
    case failed // = 5
    case UNRECOGNIZED(Int)

    public init() {
      self = .nodeStatusUnspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .nodeStatusUnspecified
      case 1: self = .blocked
      case 2: self = .runnable
      case 3: self = .running
      case 4: self = .completed
      case 5: self = .failed
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .nodeStatusUnspecified: return 0
      case .blocked: return 1
      case .runnable: return 2
      case .running: return 3
      case .completed: return 4
      case .failed: return 5
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}
}

#if swift(>=4.2)

extension Google_Cloud_Dataproc_V1beta2_WorkflowNode.NodeState: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dataproc_V1beta2_WorkflowNode.NodeState] = [
    .nodeStatusUnspecified,
    .blocked,
    .runnable,
    .running,
    .completed,
    .failed,
  ]
}

#endif  // swift(>=4.2)

/// A request to create a workflow template.
public struct Google_Cloud_Dataproc_V1beta2_CreateWorkflowTemplateRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The resource name of the region or location, as described
  /// in https://cloud.google.com/apis/design/resource_names.
  ///
  /// * For `projects.regions.workflowTemplates,create`, the resource name of the
  ///   region has the following format:
  ///   `projects/{project_id}/regions/{region}`
  ///
  /// * For `projects.locations.workflowTemplates.create`, the resource name of
  ///   the location has the following format:
  ///   `projects/{project_id}/locations/{location}`
  public var parent: String = String()

  /// Required. The Dataproc workflow template to create.
  public var template: Google_Cloud_Dataproc_V1beta2_WorkflowTemplate {
    get {return _template ?? Google_Cloud_Dataproc_V1beta2_WorkflowTemplate()}
    set {_template = newValue}
  }
  /// Returns true if `template` has been explicitly set.
  public var hasTemplate: Bool {return self._template != nil}
  /// Clears the value of `template`. Subsequent reads from it will return its default value.
  public mutating func clearTemplate() {self._template = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _template: Google_Cloud_Dataproc_V1beta2_WorkflowTemplate? = nil
}

/// A request to fetch a workflow template.
public struct Google_Cloud_Dataproc_V1beta2_GetWorkflowTemplateRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The resource name of the workflow template, as described
  /// in https://cloud.google.com/apis/design/resource_names.
  ///
  /// * For `projects.regions.workflowTemplates.get`, the resource name of the
  ///   template has the following format:
  ///   `projects/{project_id}/regions/{region}/workflowTemplates/{template_id}`
  ///
  /// * For `projects.locations.workflowTemplates.get`, the resource name of the
  ///   template has the following format:
  ///   `projects/{project_id}/locations/{location}/workflowTemplates/{template_id}`
  public var name: String = String()

  /// Optional. The version of workflow template to retrieve. Only previously
  /// instantiated versions can be retrieved.
  ///
  /// If unspecified, retrieves the current version.
  public var version: Int32 = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A request to instantiate a workflow template.
public struct Google_Cloud_Dataproc_V1beta2_InstantiateWorkflowTemplateRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The resource name of the workflow template, as described
  /// in https://cloud.google.com/apis/design/resource_names.
  ///
  /// * For `projects.regions.workflowTemplates.instantiate`, the resource name
  /// of the template has the following format:
  ///   `projects/{project_id}/regions/{region}/workflowTemplates/{template_id}`
  ///
  /// * For `projects.locations.workflowTemplates.instantiate`, the resource name
  ///   of the template has the following format:
  ///   `projects/{project_id}/locations/{location}/workflowTemplates/{template_id}`
  public var name: String = String()

  /// Optional. The version of workflow template to instantiate. If specified,
  /// the workflow will be instantiated only if the current version of
  /// the workflow template has the supplied version.
  ///
  /// This option cannot be used to instantiate a previous version of
  /// workflow template.
  public var version: Int32 = 0

  /// Deprecated. Please use `request_id` field instead.
  public var instanceID: String = String()

  /// Optional. A tag that prevents multiple concurrent workflow
  /// instances with the same tag from running. This mitigates risk of
  /// concurrent instances started due to retries.
  ///
  /// It is recommended to always set this value to a
  /// [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).
  ///
  /// The tag must contain only letters (a-z, A-Z), numbers (0-9),
  /// underscores (_), and hyphens (-). The maximum length is 40 characters.
  public var requestID: String = String()

  /// Optional. Map from parameter names to values that should be used for those
  /// parameters. Values may not exceed 100 characters.
  public var parameters: Dictionary<String,String> = [:]

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A request to instantiate an inline workflow template.
public struct Google_Cloud_Dataproc_V1beta2_InstantiateInlineWorkflowTemplateRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The resource name of the region or location, as described
  /// in https://cloud.google.com/apis/design/resource_names.
  ///
  /// * For `projects.regions.workflowTemplates,instantiateinline`, the resource
  ///   name of the region has the following format:
  ///   `projects/{project_id}/regions/{region}`
  ///
  /// * For `projects.locations.workflowTemplates.instantiateinline`, the
  ///   resource name of the location has the following format:
  ///   `projects/{project_id}/locations/{location}`
  public var parent: String = String()

  /// Required. The workflow template to instantiate.
  public var template: Google_Cloud_Dataproc_V1beta2_WorkflowTemplate {
    get {return _template ?? Google_Cloud_Dataproc_V1beta2_WorkflowTemplate()}
    set {_template = newValue}
  }
  /// Returns true if `template` has been explicitly set.
  public var hasTemplate: Bool {return self._template != nil}
  /// Clears the value of `template`. Subsequent reads from it will return its default value.
  public mutating func clearTemplate() {self._template = nil}

  /// Deprecated. Please use `request_id` field instead.
  public var instanceID: String = String()

  /// Optional. A tag that prevents multiple concurrent workflow
  /// instances with the same tag from running. This mitigates risk of
  /// concurrent instances started due to retries.
  ///
  /// It is recommended to always set this value to a
  /// [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).
  ///
  /// The tag must contain only letters (a-z, A-Z), numbers (0-9),
  /// underscores (_), and hyphens (-). The maximum length is 40 characters.
  public var requestID: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _template: Google_Cloud_Dataproc_V1beta2_WorkflowTemplate? = nil
}

/// A request to update a workflow template.
public struct Google_Cloud_Dataproc_V1beta2_UpdateWorkflowTemplateRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The updated workflow template.
  ///
  /// The `template.version` field must match the current version.
  public var template: Google_Cloud_Dataproc_V1beta2_WorkflowTemplate {
    get {return _template ?? Google_Cloud_Dataproc_V1beta2_WorkflowTemplate()}
    set {_template = newValue}
  }
  /// Returns true if `template` has been explicitly set.
  public var hasTemplate: Bool {return self._template != nil}
  /// Clears the value of `template`. Subsequent reads from it will return its default value.
  public mutating func clearTemplate() {self._template = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _template: Google_Cloud_Dataproc_V1beta2_WorkflowTemplate? = nil
}

/// A request to list workflow templates in a project.
public struct Google_Cloud_Dataproc_V1beta2_ListWorkflowTemplatesRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The resource name of the region or location, as described
  /// in https://cloud.google.com/apis/design/resource_names.
  ///
  /// * For `projects.regions.workflowTemplates,list`, the resource
  ///   name of the region has the following format:
  ///   `projects/{project_id}/regions/{region}`
  ///
  /// * For `projects.locations.workflowTemplates.list`, the
  ///   resource name of the location has the following format:
  ///   `projects/{project_id}/locations/{location}`
  public var parent: String = String()

  /// Optional. The maximum number of results to return in each response.
  public var pageSize: Int32 = 0

  /// Optional. The page token, returned by a previous call, to request the
  /// next page of results.
  public var pageToken: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A response to a request to list workflow templates in a project.
public struct Google_Cloud_Dataproc_V1beta2_ListWorkflowTemplatesResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. WorkflowTemplates list.
  public var templates: [Google_Cloud_Dataproc_V1beta2_WorkflowTemplate] = []

  /// Output only. This token is included in the response if there are more
  /// results to fetch. To fetch additional results, provide this value as the
  /// page_token in a subsequent <code>ListWorkflowTemplatesRequest</code>.
  public var nextPageToken: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A request to delete a workflow template.
///
/// Currently started workflows will remain running.
public struct Google_Cloud_Dataproc_V1beta2_DeleteWorkflowTemplateRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The resource name of the workflow template, as described
  /// in https://cloud.google.com/apis/design/resource_names.
  ///
  /// * For `projects.regions.workflowTemplates.delete`, the resource name
  /// of the template has the following format:
  ///   `projects/{project_id}/regions/{region}/workflowTemplates/{template_id}`
  ///
  /// * For `projects.locations.workflowTemplates.instantiate`, the resource name
  ///   of the template has the following format:
  ///   `projects/{project_id}/locations/{location}/workflowTemplates/{template_id}`
  public var name: String = String()

  /// Optional. The version of workflow template to delete. If specified,
  /// will only delete the template if the current server version matches
  /// specified version.
  public var version: Int32 = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.dataproc.v1beta2"

extension Google_Cloud_Dataproc_V1beta2_WorkflowTemplate: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".WorkflowTemplate"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .same(proto: "id"),
    1: .same(proto: "name"),
    3: .same(proto: "version"),
    4: .standard(proto: "create_time"),
    5: .standard(proto: "update_time"),
    6: .same(proto: "labels"),
    7: .same(proto: "placement"),
    8: .same(proto: "jobs"),
    9: .same(proto: "parameters"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.id) }()
      case 3: try { try decoder.decodeSingularInt32Field(value: &self.version) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._createTime) }()
      case 5: try { try decoder.decodeSingularMessageField(value: &self._updateTime) }()
      case 6: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.labels) }()
      case 7: try { try decoder.decodeSingularMessageField(value: &self._placement) }()
      case 8: try { try decoder.decodeRepeatedMessageField(value: &self.jobs) }()
      case 9: try { try decoder.decodeRepeatedMessageField(value: &self.parameters) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if !self.id.isEmpty {
      try visitor.visitSingularStringField(value: self.id, fieldNumber: 2)
    }
    if self.version != 0 {
      try visitor.visitSingularInt32Field(value: self.version, fieldNumber: 3)
    }
    if let v = self._createTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    if let v = self._updateTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    }
    if !self.labels.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.labels, fieldNumber: 6)
    }
    if let v = self._placement {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
    }
    if !self.jobs.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.jobs, fieldNumber: 8)
    }
    if !self.parameters.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.parameters, fieldNumber: 9)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_WorkflowTemplate, rhs: Google_Cloud_Dataproc_V1beta2_WorkflowTemplate) -> Bool {
    if lhs.id != rhs.id {return false}
    if lhs.name != rhs.name {return false}
    if lhs.version != rhs.version {return false}
    if lhs._createTime != rhs._createTime {return false}
    if lhs._updateTime != rhs._updateTime {return false}
    if lhs.labels != rhs.labels {return false}
    if lhs._placement != rhs._placement {return false}
    if lhs.jobs != rhs.jobs {return false}
    if lhs.parameters != rhs.parameters {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_WorkflowTemplatePlacement: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".WorkflowTemplatePlacement"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "managed_cluster"),
    2: .standard(proto: "cluster_selector"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        var v: Google_Cloud_Dataproc_V1beta2_ManagedCluster?
        if let current = self.placement {
          try decoder.handleConflictingOneOf()
          if case .managedCluster(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.placement = .managedCluster(v)}
      }()
      case 2: try {
        var v: Google_Cloud_Dataproc_V1beta2_ClusterSelector?
        if let current = self.placement {
          try decoder.handleConflictingOneOf()
          if case .clusterSelector(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.placement = .clusterSelector(v)}
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.placement {
    case .managedCluster?: try {
      guard case .managedCluster(let v)? = self.placement else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }()
    case .clusterSelector?: try {
      guard case .clusterSelector(let v)? = self.placement else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_WorkflowTemplatePlacement, rhs: Google_Cloud_Dataproc_V1beta2_WorkflowTemplatePlacement) -> Bool {
    if lhs.placement != rhs.placement {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ManagedCluster: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ManagedCluster"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .standard(proto: "cluster_name"),
    3: .same(proto: "config"),
    4: .same(proto: "labels"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 2: try { try decoder.decodeSingularStringField(value: &self.clusterName) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._config) }()
      case 4: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.labels) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.clusterName.isEmpty {
      try visitor.visitSingularStringField(value: self.clusterName, fieldNumber: 2)
    }
    if let v = self._config {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if !self.labels.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.labels, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ManagedCluster, rhs: Google_Cloud_Dataproc_V1beta2_ManagedCluster) -> Bool {
    if lhs.clusterName != rhs.clusterName {return false}
    if lhs._config != rhs._config {return false}
    if lhs.labels != rhs.labels {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ClusterSelector: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ClusterSelector"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "zone"),
    2: .standard(proto: "cluster_labels"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.zone) }()
      case 2: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.clusterLabels) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.zone.isEmpty {
      try visitor.visitSingularStringField(value: self.zone, fieldNumber: 1)
    }
    if !self.clusterLabels.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.clusterLabels, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ClusterSelector, rhs: Google_Cloud_Dataproc_V1beta2_ClusterSelector) -> Bool {
    if lhs.zone != rhs.zone {return false}
    if lhs.clusterLabels != rhs.clusterLabels {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_OrderedJob: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".OrderedJob"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "step_id"),
    2: .standard(proto: "hadoop_job"),
    3: .standard(proto: "spark_job"),
    4: .standard(proto: "pyspark_job"),
    5: .standard(proto: "hive_job"),
    6: .standard(proto: "pig_job"),
    11: .standard(proto: "spark_r_job"),
    7: .standard(proto: "spark_sql_job"),
    12: .standard(proto: "presto_job"),
    8: .same(proto: "labels"),
    9: .same(proto: "scheduling"),
    10: .standard(proto: "prerequisite_step_ids"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.stepID) }()
      case 2: try {
        var v: Google_Cloud_Dataproc_V1beta2_HadoopJob?
        if let current = self.jobType {
          try decoder.handleConflictingOneOf()
          if case .hadoopJob(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.jobType = .hadoopJob(v)}
      }()
      case 3: try {
        var v: Google_Cloud_Dataproc_V1beta2_SparkJob?
        if let current = self.jobType {
          try decoder.handleConflictingOneOf()
          if case .sparkJob(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.jobType = .sparkJob(v)}
      }()
      case 4: try {
        var v: Google_Cloud_Dataproc_V1beta2_PySparkJob?
        if let current = self.jobType {
          try decoder.handleConflictingOneOf()
          if case .pysparkJob(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.jobType = .pysparkJob(v)}
      }()
      case 5: try {
        var v: Google_Cloud_Dataproc_V1beta2_HiveJob?
        if let current = self.jobType {
          try decoder.handleConflictingOneOf()
          if case .hiveJob(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.jobType = .hiveJob(v)}
      }()
      case 6: try {
        var v: Google_Cloud_Dataproc_V1beta2_PigJob?
        if let current = self.jobType {
          try decoder.handleConflictingOneOf()
          if case .pigJob(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.jobType = .pigJob(v)}
      }()
      case 7: try {
        var v: Google_Cloud_Dataproc_V1beta2_SparkSqlJob?
        if let current = self.jobType {
          try decoder.handleConflictingOneOf()
          if case .sparkSqlJob(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.jobType = .sparkSqlJob(v)}
      }()
      case 8: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.labels) }()
      case 9: try { try decoder.decodeSingularMessageField(value: &self._scheduling) }()
      case 10: try { try decoder.decodeRepeatedStringField(value: &self.prerequisiteStepIds) }()
      case 11: try {
        var v: Google_Cloud_Dataproc_V1beta2_SparkRJob?
        if let current = self.jobType {
          try decoder.handleConflictingOneOf()
          if case .sparkRJob(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.jobType = .sparkRJob(v)}
      }()
      case 12: try {
        var v: Google_Cloud_Dataproc_V1beta2_PrestoJob?
        if let current = self.jobType {
          try decoder.handleConflictingOneOf()
          if case .prestoJob(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.jobType = .prestoJob(v)}
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.stepID.isEmpty {
      try visitor.visitSingularStringField(value: self.stepID, fieldNumber: 1)
    }
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.jobType {
    case .hadoopJob?: try {
      guard case .hadoopJob(let v)? = self.jobType else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case .sparkJob?: try {
      guard case .sparkJob(let v)? = self.jobType else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }()
    case .pysparkJob?: try {
      guard case .pysparkJob(let v)? = self.jobType else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }()
    case .hiveJob?: try {
      guard case .hiveJob(let v)? = self.jobType else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    }()
    case .pigJob?: try {
      guard case .pigJob(let v)? = self.jobType else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
    }()
    case .sparkSqlJob?: try {
      guard case .sparkSqlJob(let v)? = self.jobType else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
    }()
    default: break
    }
    if !self.labels.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.labels, fieldNumber: 8)
    }
    if let v = self._scheduling {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
    }
    if !self.prerequisiteStepIds.isEmpty {
      try visitor.visitRepeatedStringField(value: self.prerequisiteStepIds, fieldNumber: 10)
    }
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.jobType {
    case .sparkRJob?: try {
      guard case .sparkRJob(let v)? = self.jobType else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 11)
    }()
    case .prestoJob?: try {
      guard case .prestoJob(let v)? = self.jobType else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 12)
    }()
    default: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_OrderedJob, rhs: Google_Cloud_Dataproc_V1beta2_OrderedJob) -> Bool {
    if lhs.stepID != rhs.stepID {return false}
    if lhs.jobType != rhs.jobType {return false}
    if lhs.labels != rhs.labels {return false}
    if lhs._scheduling != rhs._scheduling {return false}
    if lhs.prerequisiteStepIds != rhs.prerequisiteStepIds {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_TemplateParameter: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".TemplateParameter"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .same(proto: "fields"),
    3: .same(proto: "description"),
    4: .same(proto: "validation"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeRepeatedStringField(value: &self.fields) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.description_p) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._validation) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if !self.fields.isEmpty {
      try visitor.visitRepeatedStringField(value: self.fields, fieldNumber: 2)
    }
    if !self.description_p.isEmpty {
      try visitor.visitSingularStringField(value: self.description_p, fieldNumber: 3)
    }
    if let v = self._validation {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_TemplateParameter, rhs: Google_Cloud_Dataproc_V1beta2_TemplateParameter) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.fields != rhs.fields {return false}
    if lhs.description_p != rhs.description_p {return false}
    if lhs._validation != rhs._validation {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ParameterValidation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ParameterValidation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "regex"),
    2: .same(proto: "values"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        var v: Google_Cloud_Dataproc_V1beta2_RegexValidation?
        if let current = self.validationType {
          try decoder.handleConflictingOneOf()
          if case .regex(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.validationType = .regex(v)}
      }()
      case 2: try {
        var v: Google_Cloud_Dataproc_V1beta2_ValueValidation?
        if let current = self.validationType {
          try decoder.handleConflictingOneOf()
          if case .values(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.validationType = .values(v)}
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.validationType {
    case .regex?: try {
      guard case .regex(let v)? = self.validationType else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }()
    case .values?: try {
      guard case .values(let v)? = self.validationType else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ParameterValidation, rhs: Google_Cloud_Dataproc_V1beta2_ParameterValidation) -> Bool {
    if lhs.validationType != rhs.validationType {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_RegexValidation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".RegexValidation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "regexes"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedStringField(value: &self.regexes) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.regexes.isEmpty {
      try visitor.visitRepeatedStringField(value: self.regexes, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_RegexValidation, rhs: Google_Cloud_Dataproc_V1beta2_RegexValidation) -> Bool {
    if lhs.regexes != rhs.regexes {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ValueValidation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ValueValidation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "values"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedStringField(value: &self.values) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.values.isEmpty {
      try visitor.visitRepeatedStringField(value: self.values, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ValueValidation, rhs: Google_Cloud_Dataproc_V1beta2_ValueValidation) -> Bool {
    if lhs.values != rhs.values {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_WorkflowMetadata: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".WorkflowMetadata"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "template"),
    2: .same(proto: "version"),
    3: .standard(proto: "create_cluster"),
    4: .same(proto: "graph"),
    5: .standard(proto: "delete_cluster"),
    6: .same(proto: "state"),
    7: .standard(proto: "cluster_name"),
    8: .same(proto: "parameters"),
    9: .standard(proto: "start_time"),
    10: .standard(proto: "end_time"),
    11: .standard(proto: "cluster_uuid"),
  ]

  fileprivate class _StorageClass {
    var _template: String = String()
    var _version: Int32 = 0
    var _createCluster: Google_Cloud_Dataproc_V1beta2_ClusterOperation? = nil
    var _graph: Google_Cloud_Dataproc_V1beta2_WorkflowGraph? = nil
    var _deleteCluster: Google_Cloud_Dataproc_V1beta2_ClusterOperation? = nil
    var _state: Google_Cloud_Dataproc_V1beta2_WorkflowMetadata.State = .unknown
    var _clusterName: String = String()
    var _parameters: Dictionary<String,String> = [:]
    var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _endTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
    var _clusterUuid: String = String()

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _template = source._template
      _version = source._version
      _createCluster = source._createCluster
      _graph = source._graph
      _deleteCluster = source._deleteCluster
      _state = source._state
      _clusterName = source._clusterName
      _parameters = source._parameters
      _startTime = source._startTime
      _endTime = source._endTime
      _clusterUuid = source._clusterUuid
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularStringField(value: &_storage._template) }()
        case 2: try { try decoder.decodeSingularInt32Field(value: &_storage._version) }()
        case 3: try { try decoder.decodeSingularMessageField(value: &_storage._createCluster) }()
        case 4: try { try decoder.decodeSingularMessageField(value: &_storage._graph) }()
        case 5: try { try decoder.decodeSingularMessageField(value: &_storage._deleteCluster) }()
        case 6: try { try decoder.decodeSingularEnumField(value: &_storage._state) }()
        case 7: try { try decoder.decodeSingularStringField(value: &_storage._clusterName) }()
        case 8: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &_storage._parameters) }()
        case 9: try { try decoder.decodeSingularMessageField(value: &_storage._startTime) }()
        case 10: try { try decoder.decodeSingularMessageField(value: &_storage._endTime) }()
        case 11: try { try decoder.decodeSingularStringField(value: &_storage._clusterUuid) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if !_storage._template.isEmpty {
        try visitor.visitSingularStringField(value: _storage._template, fieldNumber: 1)
      }
      if _storage._version != 0 {
        try visitor.visitSingularInt32Field(value: _storage._version, fieldNumber: 2)
      }
      if let v = _storage._createCluster {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      }
      if let v = _storage._graph {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
      }
      if let v = _storage._deleteCluster {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      }
      if _storage._state != .unknown {
        try visitor.visitSingularEnumField(value: _storage._state, fieldNumber: 6)
      }
      if !_storage._clusterName.isEmpty {
        try visitor.visitSingularStringField(value: _storage._clusterName, fieldNumber: 7)
      }
      if !_storage._parameters.isEmpty {
        try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: _storage._parameters, fieldNumber: 8)
      }
      if let v = _storage._startTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
      }
      if let v = _storage._endTime {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 10)
      }
      if !_storage._clusterUuid.isEmpty {
        try visitor.visitSingularStringField(value: _storage._clusterUuid, fieldNumber: 11)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_WorkflowMetadata, rhs: Google_Cloud_Dataproc_V1beta2_WorkflowMetadata) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._template != rhs_storage._template {return false}
        if _storage._version != rhs_storage._version {return false}
        if _storage._createCluster != rhs_storage._createCluster {return false}
        if _storage._graph != rhs_storage._graph {return false}
        if _storage._deleteCluster != rhs_storage._deleteCluster {return false}
        if _storage._state != rhs_storage._state {return false}
        if _storage._clusterName != rhs_storage._clusterName {return false}
        if _storage._parameters != rhs_storage._parameters {return false}
        if _storage._startTime != rhs_storage._startTime {return false}
        if _storage._endTime != rhs_storage._endTime {return false}
        if _storage._clusterUuid != rhs_storage._clusterUuid {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_WorkflowMetadata.State: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "UNKNOWN"),
    1: .same(proto: "PENDING"),
    2: .same(proto: "RUNNING"),
    3: .same(proto: "DONE"),
  ]
}

extension Google_Cloud_Dataproc_V1beta2_ClusterOperation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ClusterOperation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "operation_id"),
    2: .same(proto: "error"),
    3: .same(proto: "done"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.operationID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.error) }()
      case 3: try { try decoder.decodeSingularBoolField(value: &self.done) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.operationID.isEmpty {
      try visitor.visitSingularStringField(value: self.operationID, fieldNumber: 1)
    }
    if !self.error.isEmpty {
      try visitor.visitSingularStringField(value: self.error, fieldNumber: 2)
    }
    if self.done != false {
      try visitor.visitSingularBoolField(value: self.done, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ClusterOperation, rhs: Google_Cloud_Dataproc_V1beta2_ClusterOperation) -> Bool {
    if lhs.operationID != rhs.operationID {return false}
    if lhs.error != rhs.error {return false}
    if lhs.done != rhs.done {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_WorkflowGraph: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".WorkflowGraph"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "nodes"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.nodes) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.nodes.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.nodes, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_WorkflowGraph, rhs: Google_Cloud_Dataproc_V1beta2_WorkflowGraph) -> Bool {
    if lhs.nodes != rhs.nodes {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_WorkflowNode: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".WorkflowNode"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "step_id"),
    2: .standard(proto: "prerequisite_step_ids"),
    3: .standard(proto: "job_id"),
    5: .same(proto: "state"),
    6: .same(proto: "error"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.stepID) }()
      case 2: try { try decoder.decodeRepeatedStringField(value: &self.prerequisiteStepIds) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.jobID) }()
      case 5: try { try decoder.decodeSingularEnumField(value: &self.state) }()
      case 6: try { try decoder.decodeSingularStringField(value: &self.error) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.stepID.isEmpty {
      try visitor.visitSingularStringField(value: self.stepID, fieldNumber: 1)
    }
    if !self.prerequisiteStepIds.isEmpty {
      try visitor.visitRepeatedStringField(value: self.prerequisiteStepIds, fieldNumber: 2)
    }
    if !self.jobID.isEmpty {
      try visitor.visitSingularStringField(value: self.jobID, fieldNumber: 3)
    }
    if self.state != .nodeStatusUnspecified {
      try visitor.visitSingularEnumField(value: self.state, fieldNumber: 5)
    }
    if !self.error.isEmpty {
      try visitor.visitSingularStringField(value: self.error, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_WorkflowNode, rhs: Google_Cloud_Dataproc_V1beta2_WorkflowNode) -> Bool {
    if lhs.stepID != rhs.stepID {return false}
    if lhs.prerequisiteStepIds != rhs.prerequisiteStepIds {return false}
    if lhs.jobID != rhs.jobID {return false}
    if lhs.state != rhs.state {return false}
    if lhs.error != rhs.error {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_WorkflowNode.NodeState: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "NODE_STATUS_UNSPECIFIED"),
    1: .same(proto: "BLOCKED"),
    2: .same(proto: "RUNNABLE"),
    3: .same(proto: "RUNNING"),
    4: .same(proto: "COMPLETED"),
    5: .same(proto: "FAILED"),
  ]
}

extension Google_Cloud_Dataproc_V1beta2_CreateWorkflowTemplateRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".CreateWorkflowTemplateRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "parent"),
    2: .same(proto: "template"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.parent) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._template) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.parent.isEmpty {
      try visitor.visitSingularStringField(value: self.parent, fieldNumber: 1)
    }
    if let v = self._template {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_CreateWorkflowTemplateRequest, rhs: Google_Cloud_Dataproc_V1beta2_CreateWorkflowTemplateRequest) -> Bool {
    if lhs.parent != rhs.parent {return false}
    if lhs._template != rhs._template {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_GetWorkflowTemplateRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".GetWorkflowTemplateRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .same(proto: "version"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.version) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if self.version != 0 {
      try visitor.visitSingularInt32Field(value: self.version, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_GetWorkflowTemplateRequest, rhs: Google_Cloud_Dataproc_V1beta2_GetWorkflowTemplateRequest) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.version != rhs.version {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_InstantiateWorkflowTemplateRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".InstantiateWorkflowTemplateRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .same(proto: "version"),
    3: .standard(proto: "instance_id"),
    5: .standard(proto: "request_id"),
    4: .same(proto: "parameters"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.version) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.instanceID) }()
      case 4: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.parameters) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.requestID) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if self.version != 0 {
      try visitor.visitSingularInt32Field(value: self.version, fieldNumber: 2)
    }
    if !self.instanceID.isEmpty {
      try visitor.visitSingularStringField(value: self.instanceID, fieldNumber: 3)
    }
    if !self.parameters.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.parameters, fieldNumber: 4)
    }
    if !self.requestID.isEmpty {
      try visitor.visitSingularStringField(value: self.requestID, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_InstantiateWorkflowTemplateRequest, rhs: Google_Cloud_Dataproc_V1beta2_InstantiateWorkflowTemplateRequest) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.version != rhs.version {return false}
    if lhs.instanceID != rhs.instanceID {return false}
    if lhs.requestID != rhs.requestID {return false}
    if lhs.parameters != rhs.parameters {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_InstantiateInlineWorkflowTemplateRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".InstantiateInlineWorkflowTemplateRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "parent"),
    2: .same(proto: "template"),
    3: .standard(proto: "instance_id"),
    4: .standard(proto: "request_id"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.parent) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._template) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.instanceID) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.requestID) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.parent.isEmpty {
      try visitor.visitSingularStringField(value: self.parent, fieldNumber: 1)
    }
    if let v = self._template {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if !self.instanceID.isEmpty {
      try visitor.visitSingularStringField(value: self.instanceID, fieldNumber: 3)
    }
    if !self.requestID.isEmpty {
      try visitor.visitSingularStringField(value: self.requestID, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_InstantiateInlineWorkflowTemplateRequest, rhs: Google_Cloud_Dataproc_V1beta2_InstantiateInlineWorkflowTemplateRequest) -> Bool {
    if lhs.parent != rhs.parent {return false}
    if lhs._template != rhs._template {return false}
    if lhs.instanceID != rhs.instanceID {return false}
    if lhs.requestID != rhs.requestID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_UpdateWorkflowTemplateRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".UpdateWorkflowTemplateRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "template"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._template) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._template {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_UpdateWorkflowTemplateRequest, rhs: Google_Cloud_Dataproc_V1beta2_UpdateWorkflowTemplateRequest) -> Bool {
    if lhs._template != rhs._template {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ListWorkflowTemplatesRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ListWorkflowTemplatesRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "parent"),
    2: .standard(proto: "page_size"),
    3: .standard(proto: "page_token"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.parent) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.pageSize) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.pageToken) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.parent.isEmpty {
      try visitor.visitSingularStringField(value: self.parent, fieldNumber: 1)
    }
    if self.pageSize != 0 {
      try visitor.visitSingularInt32Field(value: self.pageSize, fieldNumber: 2)
    }
    if !self.pageToken.isEmpty {
      try visitor.visitSingularStringField(value: self.pageToken, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ListWorkflowTemplatesRequest, rhs: Google_Cloud_Dataproc_V1beta2_ListWorkflowTemplatesRequest) -> Bool {
    if lhs.parent != rhs.parent {return false}
    if lhs.pageSize != rhs.pageSize {return false}
    if lhs.pageToken != rhs.pageToken {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ListWorkflowTemplatesResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ListWorkflowTemplatesResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "templates"),
    2: .standard(proto: "next_page_token"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.templates) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.nextPageToken) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.templates.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.templates, fieldNumber: 1)
    }
    if !self.nextPageToken.isEmpty {
      try visitor.visitSingularStringField(value: self.nextPageToken, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ListWorkflowTemplatesResponse, rhs: Google_Cloud_Dataproc_V1beta2_ListWorkflowTemplatesResponse) -> Bool {
    if lhs.templates != rhs.templates {return false}
    if lhs.nextPageToken != rhs.nextPageToken {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_DeleteWorkflowTemplateRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".DeleteWorkflowTemplateRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .same(proto: "version"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.version) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if self.version != 0 {
      try visitor.visitSingularInt32Field(value: self.version, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_DeleteWorkflowTemplateRequest, rhs: Google_Cloud_Dataproc_V1beta2_DeleteWorkflowTemplateRequest) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.version != rhs.version {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
