// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/dataproc/v1beta2/jobs.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2020 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// The runtime logging config of the job.
public struct Google_Cloud_Dataproc_V1beta2_LoggingConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The per-package log levels for the driver. This may include
  /// "root" package name to configure rootLogger.
  /// Examples:
  ///   'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
  public var driverLogLevels: Dictionary<String,Google_Cloud_Dataproc_V1beta2_LoggingConfig.Level> = [:]

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The Log4j level for job execution. When running an
  /// [Apache Hive](http://hive.apache.org/) job, Cloud
  /// Dataproc configures the Hive client to an equivalent verbosity level.
  public enum Level: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// Level is unspecified. Use default level for log4j.
    case unspecified // = 0

    /// Use ALL level for log4j.
    case all // = 1

    /// Use TRACE level for log4j.
    case trace // = 2

    /// Use DEBUG level for log4j.
    case debug // = 3

    /// Use INFO level for log4j.
    case info // = 4

    /// Use WARN level for log4j.
    case warn // = 5

    /// Use ERROR level for log4j.
    case error // = 6

    /// Use FATAL level for log4j.
    case fatal // = 7

    /// Turn off log4j.
    case off // = 8
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .all
      case 2: self = .trace
      case 3: self = .debug
      case 4: self = .info
      case 5: self = .warn
      case 6: self = .error
      case 7: self = .fatal
      case 8: self = .off
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .all: return 1
      case .trace: return 2
      case .debug: return 3
      case .info: return 4
      case .warn: return 5
      case .error: return 6
      case .fatal: return 7
      case .off: return 8
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}
}

#if swift(>=4.2)

extension Google_Cloud_Dataproc_V1beta2_LoggingConfig.Level: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dataproc_V1beta2_LoggingConfig.Level] = [
    .unspecified,
    .all,
    .trace,
    .debug,
    .info,
    .warn,
    .error,
    .fatal,
    .off,
  ]
}

#endif  // swift(>=4.2)

/// A Dataproc job for running
/// [Apache Hadoop
/// MapReduce](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
/// jobs on [Apache Hadoop
/// YARN](https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
public struct Google_Cloud_Dataproc_V1beta2_HadoopJob {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Indicates the location of the driver's main class. Specify
  /// either the jar file that contains the main class or the main class name.
  /// To specify both, add the jar file to `jar_file_uris`, and then specify
  /// the main class name in this property.
  public var driver: Google_Cloud_Dataproc_V1beta2_HadoopJob.OneOf_Driver? = nil

  /// The HCFS URI of the jar file containing the main class.
  /// Examples:
  ///     'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar'
  ///     'hdfs:/tmp/test-samples/custom-wordcount.jar'
  ///     'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
  public var mainJarFileUri: String {
    get {
      if case .mainJarFileUri(let v)? = driver {return v}
      return String()
    }
    set {driver = .mainJarFileUri(newValue)}
  }

  /// The name of the driver's main class. The jar file containing the class
  /// must be in the default CLASSPATH or specified in `jar_file_uris`.
  public var mainClass: String {
    get {
      if case .mainClass(let v)? = driver {return v}
      return String()
    }
    set {driver = .mainClass(newValue)}
  }

  /// Optional. The arguments to pass to the driver. Do not
  /// include arguments, such as `-libjars` or `-Dfoo=bar`, that can be set as
  /// job properties, since a collision may occur that causes an incorrect job
  /// submission.
  public var args: [String] = []

  /// Optional. Jar file URIs to add to the CLASSPATHs of the
  /// Hadoop driver and tasks.
  public var jarFileUris: [String] = []

  /// Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied
  /// to the working directory of Hadoop drivers and distributed tasks. Useful
  /// for naively parallel tasks.
  public var fileUris: [String] = []

  /// Optional. HCFS URIs of archives to be extracted in the working directory of
  /// Hadoop drivers and tasks. Supported file types:
  /// .jar, .tar, .tar.gz, .tgz, or .zip.
  public var archiveUris: [String] = []

  /// Optional. A mapping of property names to values, used to configure Hadoop.
  /// Properties that conflict with values set by the Dataproc API may be
  /// overwritten. Can include properties set in /etc/hadoop/conf/*-site and
  /// classes in user code.
  public var properties: Dictionary<String,String> = [:]

  /// Optional. The runtime log config for job execution.
  public var loggingConfig: Google_Cloud_Dataproc_V1beta2_LoggingConfig {
    get {return _loggingConfig ?? Google_Cloud_Dataproc_V1beta2_LoggingConfig()}
    set {_loggingConfig = newValue}
  }
  /// Returns true if `loggingConfig` has been explicitly set.
  public var hasLoggingConfig: Bool {return self._loggingConfig != nil}
  /// Clears the value of `loggingConfig`. Subsequent reads from it will return its default value.
  public mutating func clearLoggingConfig() {self._loggingConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Required. Indicates the location of the driver's main class. Specify
  /// either the jar file that contains the main class or the main class name.
  /// To specify both, add the jar file to `jar_file_uris`, and then specify
  /// the main class name in this property.
  public enum OneOf_Driver: Equatable {
    /// The HCFS URI of the jar file containing the main class.
    /// Examples:
    ///     'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar'
    ///     'hdfs:/tmp/test-samples/custom-wordcount.jar'
    ///     'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
    case mainJarFileUri(String)
    /// The name of the driver's main class. The jar file containing the class
    /// must be in the default CLASSPATH or specified in `jar_file_uris`.
    case mainClass(String)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_HadoopJob.OneOf_Driver, rhs: Google_Cloud_Dataproc_V1beta2_HadoopJob.OneOf_Driver) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.mainJarFileUri, .mainJarFileUri): return {
        guard case .mainJarFileUri(let l) = lhs, case .mainJarFileUri(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.mainClass, .mainClass): return {
        guard case .mainClass(let l) = lhs, case .mainClass(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}

  fileprivate var _loggingConfig: Google_Cloud_Dataproc_V1beta2_LoggingConfig? = nil
}

/// A Dataproc job for running [Apache Spark](http://spark.apache.org/)
/// applications on YARN.
/// The specification of the main method to call to drive the job.
/// Specify either the jar file that contains the main class or the main class
/// name. To pass both a main jar and a main class in that jar, add the jar to
/// `CommonJob.jar_file_uris`, and then specify the main class name in
/// `main_class`.
public struct Google_Cloud_Dataproc_V1beta2_SparkJob {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  public var driver: Google_Cloud_Dataproc_V1beta2_SparkJob.OneOf_Driver? = nil

  /// The HCFS URI of the jar file that contains the main class.
  public var mainJarFileUri: String {
    get {
      if case .mainJarFileUri(let v)? = driver {return v}
      return String()
    }
    set {driver = .mainJarFileUri(newValue)}
  }

  /// The name of the driver's main class. The jar file that contains the class
  /// must be in the default CLASSPATH or specified in `jar_file_uris`.
  public var mainClass: String {
    get {
      if case .mainClass(let v)? = driver {return v}
      return String()
    }
    set {driver = .mainClass(newValue)}
  }

  /// Optional. The arguments to pass to the driver. Do not include arguments,
  /// such as `--conf`, that can be set as job properties, since a collision may
  /// occur that causes an incorrect job submission.
  public var args: [String] = []

  /// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the
  /// Spark driver and tasks.
  public var jarFileUris: [String] = []

  /// Optional. HCFS URIs of files to be copied to the working directory of
  /// Spark drivers and distributed tasks. Useful for naively parallel tasks.
  public var fileUris: [String] = []

  /// Optional. HCFS URIs of archives to be extracted in the working directory
  /// of Spark drivers and tasks. Supported file types:
  /// .jar, .tar, .tar.gz, .tgz, and .zip.
  public var archiveUris: [String] = []

  /// Optional. A mapping of property names to values, used to configure Spark.
  /// Properties that conflict with values set by the Dataproc API may be
  /// overwritten. Can include properties set in
  /// /etc/spark/conf/spark-defaults.conf and classes in user code.
  public var properties: Dictionary<String,String> = [:]

  /// Optional. The runtime log config for job execution.
  public var loggingConfig: Google_Cloud_Dataproc_V1beta2_LoggingConfig {
    get {return _loggingConfig ?? Google_Cloud_Dataproc_V1beta2_LoggingConfig()}
    set {_loggingConfig = newValue}
  }
  /// Returns true if `loggingConfig` has been explicitly set.
  public var hasLoggingConfig: Bool {return self._loggingConfig != nil}
  /// Clears the value of `loggingConfig`. Subsequent reads from it will return its default value.
  public mutating func clearLoggingConfig() {self._loggingConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public enum OneOf_Driver: Equatable {
    /// The HCFS URI of the jar file that contains the main class.
    case mainJarFileUri(String)
    /// The name of the driver's main class. The jar file that contains the class
    /// must be in the default CLASSPATH or specified in `jar_file_uris`.
    case mainClass(String)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_SparkJob.OneOf_Driver, rhs: Google_Cloud_Dataproc_V1beta2_SparkJob.OneOf_Driver) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.mainJarFileUri, .mainJarFileUri): return {
        guard case .mainJarFileUri(let l) = lhs, case .mainJarFileUri(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.mainClass, .mainClass): return {
        guard case .mainClass(let l) = lhs, case .mainClass(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}

  fileprivate var _loggingConfig: Google_Cloud_Dataproc_V1beta2_LoggingConfig? = nil
}

/// A Dataproc job for running
/// [Apache
/// PySpark](https://spark.apache.org/docs/0.9.0/python-programming-guide.html)
/// applications on YARN.
public struct Google_Cloud_Dataproc_V1beta2_PySparkJob {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The HCFS URI of the main Python file to use as the driver. Must
  /// be a .py file.
  public var mainPythonFileUri: String = String()

  /// Optional. The arguments to pass to the driver.  Do not include arguments,
  /// such as `--conf`, that can be set as job properties, since a collision may
  /// occur that causes an incorrect job submission.
  public var args: [String] = []

  /// Optional. HCFS file URIs of Python files to pass to the PySpark
  /// framework. Supported file types: .py, .egg, and .zip.
  public var pythonFileUris: [String] = []

  /// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the
  /// Python driver and tasks.
  public var jarFileUris: [String] = []

  /// Optional. HCFS URIs of files to be copied to the working directory of
  /// Python drivers and distributed tasks. Useful for naively parallel tasks.
  public var fileUris: [String] = []

  /// Optional. HCFS URIs of archives to be extracted in the working directory of
  /// .jar, .tar, .tar.gz, .tgz, and .zip.
  public var archiveUris: [String] = []

  /// Optional. A mapping of property names to values, used to configure PySpark.
  /// Properties that conflict with values set by the Dataproc API may be
  /// overwritten. Can include properties set in
  /// /etc/spark/conf/spark-defaults.conf and classes in user code.
  public var properties: Dictionary<String,String> = [:]

  /// Optional. The runtime log config for job execution.
  public var loggingConfig: Google_Cloud_Dataproc_V1beta2_LoggingConfig {
    get {return _loggingConfig ?? Google_Cloud_Dataproc_V1beta2_LoggingConfig()}
    set {_loggingConfig = newValue}
  }
  /// Returns true if `loggingConfig` has been explicitly set.
  public var hasLoggingConfig: Bool {return self._loggingConfig != nil}
  /// Clears the value of `loggingConfig`. Subsequent reads from it will return its default value.
  public mutating func clearLoggingConfig() {self._loggingConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _loggingConfig: Google_Cloud_Dataproc_V1beta2_LoggingConfig? = nil
}

/// A list of queries to run on a cluster.
public struct Google_Cloud_Dataproc_V1beta2_QueryList {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The queries to execute. You do not need to terminate a query
  /// with a semicolon. Multiple queries can be specified in one string
  /// by separating each with a semicolon. Here is an example of an Cloud
  /// Dataproc API snippet that uses a QueryList to specify a HiveJob:
  ///
  ///     "hiveJob": {
  ///       "queryList": {
  ///         "queries": [
  ///           "query1",
  ///           "query2",
  ///           "query3;query4",
  ///         ]
  ///       }
  ///     }
  public var queries: [String] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A Dataproc job for running [Apache Hive](https://hive.apache.org/)
/// queries on YARN.
public struct Google_Cloud_Dataproc_V1beta2_HiveJob {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The sequence of Hive queries to execute, specified as either
  /// an HCFS file URI or a list of queries.
  public var queries: Google_Cloud_Dataproc_V1beta2_HiveJob.OneOf_Queries? = nil

  /// The HCFS URI of the script that contains Hive queries.
  public var queryFileUri: String {
    get {
      if case .queryFileUri(let v)? = queries {return v}
      return String()
    }
    set {queries = .queryFileUri(newValue)}
  }

  /// A list of queries.
  public var queryList: Google_Cloud_Dataproc_V1beta2_QueryList {
    get {
      if case .queryList(let v)? = queries {return v}
      return Google_Cloud_Dataproc_V1beta2_QueryList()
    }
    set {queries = .queryList(newValue)}
  }

  /// Optional. Whether to continue executing queries if a query fails.
  /// The default value is `false`. Setting to `true` can be useful when
  /// executing independent parallel queries.
  public var continueOnFailure: Bool = false

  /// Optional. Mapping of query variable names to values (equivalent to the
  /// Hive command: `SET name="value";`).
  public var scriptVariables: Dictionary<String,String> = [:]

  /// Optional. A mapping of property names and values, used to configure Hive.
  /// Properties that conflict with values set by the Dataproc API may be
  /// overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml,
  /// /etc/hive/conf/hive-site.xml, and classes in user code.
  public var properties: Dictionary<String,String> = [:]

  /// Optional. HCFS URIs of jar files to add to the CLASSPATH of the
  /// Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes
  /// and UDFs.
  public var jarFileUris: [String] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Required. The sequence of Hive queries to execute, specified as either
  /// an HCFS file URI or a list of queries.
  public enum OneOf_Queries: Equatable {
    /// The HCFS URI of the script that contains Hive queries.
    case queryFileUri(String)
    /// A list of queries.
    case queryList(Google_Cloud_Dataproc_V1beta2_QueryList)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_HiveJob.OneOf_Queries, rhs: Google_Cloud_Dataproc_V1beta2_HiveJob.OneOf_Queries) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.queryFileUri, .queryFileUri): return {
        guard case .queryFileUri(let l) = lhs, case .queryFileUri(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.queryList, .queryList): return {
        guard case .queryList(let l) = lhs, case .queryList(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}
}

/// A Dataproc job for running [Apache Spark
/// SQL](http://spark.apache.org/sql/) queries.
public struct Google_Cloud_Dataproc_V1beta2_SparkSqlJob {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The sequence of Spark SQL queries to execute, specified as
  /// either an HCFS file URI or as a list of queries.
  public var queries: Google_Cloud_Dataproc_V1beta2_SparkSqlJob.OneOf_Queries? = nil

  /// The HCFS URI of the script that contains SQL queries.
  public var queryFileUri: String {
    get {
      if case .queryFileUri(let v)? = queries {return v}
      return String()
    }
    set {queries = .queryFileUri(newValue)}
  }

  /// A list of queries.
  public var queryList: Google_Cloud_Dataproc_V1beta2_QueryList {
    get {
      if case .queryList(let v)? = queries {return v}
      return Google_Cloud_Dataproc_V1beta2_QueryList()
    }
    set {queries = .queryList(newValue)}
  }

  /// Optional. Mapping of query variable names to values (equivalent to the
  /// Spark SQL command: SET `name="value";`).
  public var scriptVariables: Dictionary<String,String> = [:]

  /// Optional. A mapping of property names to values, used to configure
  /// Spark SQL's SparkConf. Properties that conflict with values set by the
  /// Dataproc API may be overwritten.
  public var properties: Dictionary<String,String> = [:]

  /// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
  public var jarFileUris: [String] = []

  /// Optional. The runtime log config for job execution.
  public var loggingConfig: Google_Cloud_Dataproc_V1beta2_LoggingConfig {
    get {return _loggingConfig ?? Google_Cloud_Dataproc_V1beta2_LoggingConfig()}
    set {_loggingConfig = newValue}
  }
  /// Returns true if `loggingConfig` has been explicitly set.
  public var hasLoggingConfig: Bool {return self._loggingConfig != nil}
  /// Clears the value of `loggingConfig`. Subsequent reads from it will return its default value.
  public mutating func clearLoggingConfig() {self._loggingConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Required. The sequence of Spark SQL queries to execute, specified as
  /// either an HCFS file URI or as a list of queries.
  public enum OneOf_Queries: Equatable {
    /// The HCFS URI of the script that contains SQL queries.
    case queryFileUri(String)
    /// A list of queries.
    case queryList(Google_Cloud_Dataproc_V1beta2_QueryList)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_SparkSqlJob.OneOf_Queries, rhs: Google_Cloud_Dataproc_V1beta2_SparkSqlJob.OneOf_Queries) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.queryFileUri, .queryFileUri): return {
        guard case .queryFileUri(let l) = lhs, case .queryFileUri(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.queryList, .queryList): return {
        guard case .queryList(let l) = lhs, case .queryList(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}

  fileprivate var _loggingConfig: Google_Cloud_Dataproc_V1beta2_LoggingConfig? = nil
}

/// A Dataproc job for running [Apache Pig](https://pig.apache.org/)
/// queries on YARN.
public struct Google_Cloud_Dataproc_V1beta2_PigJob {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The sequence of Pig queries to execute, specified as an HCFS
  /// file URI or a list of queries.
  public var queries: Google_Cloud_Dataproc_V1beta2_PigJob.OneOf_Queries? = nil

  /// The HCFS URI of the script that contains the Pig queries.
  public var queryFileUri: String {
    get {
      if case .queryFileUri(let v)? = queries {return v}
      return String()
    }
    set {queries = .queryFileUri(newValue)}
  }

  /// A list of queries.
  public var queryList: Google_Cloud_Dataproc_V1beta2_QueryList {
    get {
      if case .queryList(let v)? = queries {return v}
      return Google_Cloud_Dataproc_V1beta2_QueryList()
    }
    set {queries = .queryList(newValue)}
  }

  /// Optional. Whether to continue executing queries if a query fails.
  /// The default value is `false`. Setting to `true` can be useful when
  /// executing independent parallel queries.
  public var continueOnFailure: Bool = false

  /// Optional. Mapping of query variable names to values (equivalent to the Pig
  /// command: `name=[value]`).
  public var scriptVariables: Dictionary<String,String> = [:]

  /// Optional. A mapping of property names to values, used to configure Pig.
  /// Properties that conflict with values set by the Dataproc API may be
  /// overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml,
  /// /etc/pig/conf/pig.properties, and classes in user code.
  public var properties: Dictionary<String,String> = [:]

  /// Optional. HCFS URIs of jar files to add to the CLASSPATH of
  /// the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
  public var jarFileUris: [String] = []

  /// Optional. The runtime log config for job execution.
  public var loggingConfig: Google_Cloud_Dataproc_V1beta2_LoggingConfig {
    get {return _loggingConfig ?? Google_Cloud_Dataproc_V1beta2_LoggingConfig()}
    set {_loggingConfig = newValue}
  }
  /// Returns true if `loggingConfig` has been explicitly set.
  public var hasLoggingConfig: Bool {return self._loggingConfig != nil}
  /// Clears the value of `loggingConfig`. Subsequent reads from it will return its default value.
  public mutating func clearLoggingConfig() {self._loggingConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Required. The sequence of Pig queries to execute, specified as an HCFS
  /// file URI or a list of queries.
  public enum OneOf_Queries: Equatable {
    /// The HCFS URI of the script that contains the Pig queries.
    case queryFileUri(String)
    /// A list of queries.
    case queryList(Google_Cloud_Dataproc_V1beta2_QueryList)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_PigJob.OneOf_Queries, rhs: Google_Cloud_Dataproc_V1beta2_PigJob.OneOf_Queries) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.queryFileUri, .queryFileUri): return {
        guard case .queryFileUri(let l) = lhs, case .queryFileUri(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.queryList, .queryList): return {
        guard case .queryList(let l) = lhs, case .queryList(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}

  fileprivate var _loggingConfig: Google_Cloud_Dataproc_V1beta2_LoggingConfig? = nil
}

/// A Dataproc job for running
/// [Apache SparkR](https://spark.apache.org/docs/latest/sparkr.html)
/// applications on YARN.
public struct Google_Cloud_Dataproc_V1beta2_SparkRJob {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The HCFS URI of the main R file to use as the driver.
  /// Must be a .R file.
  public var mainRFileUri: String = String()

  /// Optional. The arguments to pass to the driver.  Do not include arguments,
  /// such as `--conf`, that can be set as job properties, since a collision may
  /// occur that causes an incorrect job submission.
  public var args: [String] = []

  /// Optional. HCFS URIs of files to be copied to the working directory of
  /// R drivers and distributed tasks. Useful for naively parallel tasks.
  public var fileUris: [String] = []

  /// Optional. HCFS URIs of archives to be extracted in the working directory of
  /// Spark drivers and tasks. Supported file types:
  /// .jar, .tar, .tar.gz, .tgz, and .zip.
  public var archiveUris: [String] = []

  /// Optional. A mapping of property names to values, used to configure SparkR.
  /// Properties that conflict with values set by the Dataproc API may be
  /// overwritten. Can include properties set in
  /// /etc/spark/conf/spark-defaults.conf and classes in user code.
  public var properties: Dictionary<String,String> = [:]

  /// Optional. The runtime log config for job execution.
  public var loggingConfig: Google_Cloud_Dataproc_V1beta2_LoggingConfig {
    get {return _loggingConfig ?? Google_Cloud_Dataproc_V1beta2_LoggingConfig()}
    set {_loggingConfig = newValue}
  }
  /// Returns true if `loggingConfig` has been explicitly set.
  public var hasLoggingConfig: Bool {return self._loggingConfig != nil}
  /// Clears the value of `loggingConfig`. Subsequent reads from it will return its default value.
  public mutating func clearLoggingConfig() {self._loggingConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _loggingConfig: Google_Cloud_Dataproc_V1beta2_LoggingConfig? = nil
}

/// A Dataproc job for running [Presto](https://prestosql.io/) queries.
/// **IMPORTANT**: The [Dataproc Presto Optional
/// Component](https://cloud.google.com/dataproc/docs/concepts/components/presto)
/// must be enabled when the cluster is created to submit a Presto job to the
/// cluster.
public struct Google_Cloud_Dataproc_V1beta2_PrestoJob {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The sequence of Presto queries to execute, specified as
  /// either an HCFS file URI or as a list of queries.
  public var queries: Google_Cloud_Dataproc_V1beta2_PrestoJob.OneOf_Queries? = nil

  /// The HCFS URI of the script that contains SQL queries.
  public var queryFileUri: String {
    get {
      if case .queryFileUri(let v)? = queries {return v}
      return String()
    }
    set {queries = .queryFileUri(newValue)}
  }

  /// A list of queries.
  public var queryList: Google_Cloud_Dataproc_V1beta2_QueryList {
    get {
      if case .queryList(let v)? = queries {return v}
      return Google_Cloud_Dataproc_V1beta2_QueryList()
    }
    set {queries = .queryList(newValue)}
  }

  /// Optional. Whether to continue executing queries if a query fails.
  /// The default value is `false`. Setting to `true` can be useful when
  /// executing independent parallel queries.
  public var continueOnFailure: Bool = false

  /// Optional. The format in which query output will be displayed. See the
  /// Presto documentation for supported output formats
  public var outputFormat: String = String()

  /// Optional. Presto client tags to attach to this query
  public var clientTags: [String] = []

  /// Optional. A mapping of property names to values. Used to set Presto
  /// [session properties](https://prestodb.io/docs/current/sql/set-session.html)
  /// Equivalent to using the --session flag in the Presto CLI
  public var properties: Dictionary<String,String> = [:]

  /// Optional. The runtime log config for job execution.
  public var loggingConfig: Google_Cloud_Dataproc_V1beta2_LoggingConfig {
    get {return _loggingConfig ?? Google_Cloud_Dataproc_V1beta2_LoggingConfig()}
    set {_loggingConfig = newValue}
  }
  /// Returns true if `loggingConfig` has been explicitly set.
  public var hasLoggingConfig: Bool {return self._loggingConfig != nil}
  /// Clears the value of `loggingConfig`. Subsequent reads from it will return its default value.
  public mutating func clearLoggingConfig() {self._loggingConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Required. The sequence of Presto queries to execute, specified as
  /// either an HCFS file URI or as a list of queries.
  public enum OneOf_Queries: Equatable {
    /// The HCFS URI of the script that contains SQL queries.
    case queryFileUri(String)
    /// A list of queries.
    case queryList(Google_Cloud_Dataproc_V1beta2_QueryList)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_PrestoJob.OneOf_Queries, rhs: Google_Cloud_Dataproc_V1beta2_PrestoJob.OneOf_Queries) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.queryFileUri, .queryFileUri): return {
        guard case .queryFileUri(let l) = lhs, case .queryFileUri(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.queryList, .queryList): return {
        guard case .queryList(let l) = lhs, case .queryList(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}

  fileprivate var _loggingConfig: Google_Cloud_Dataproc_V1beta2_LoggingConfig? = nil
}

/// Dataproc job config.
public struct Google_Cloud_Dataproc_V1beta2_JobPlacement {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The name of the cluster where the job will be submitted.
  public var clusterName: String = String()

  /// Output only. A cluster UUID generated by the Dataproc service when
  /// the job is submitted.
  public var clusterUuid: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Dataproc job status.
public struct Google_Cloud_Dataproc_V1beta2_JobStatus {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. A state message specifying the overall job state.
  public var state: Google_Cloud_Dataproc_V1beta2_JobStatus.State = .unspecified

  /// Output only. Optional Job state details, such as an error
  /// description if the state is <code>ERROR</code>.
  public var details: String = String()

  /// Output only. The time when this state was entered.
  public var stateStartTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _stateStartTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_stateStartTime = newValue}
  }
  /// Returns true if `stateStartTime` has been explicitly set.
  public var hasStateStartTime: Bool {return self._stateStartTime != nil}
  /// Clears the value of `stateStartTime`. Subsequent reads from it will return its default value.
  public mutating func clearStateStartTime() {self._stateStartTime = nil}

  /// Output only. Additional state information, which includes
  /// status reported by the agent.
  public var substate: Google_Cloud_Dataproc_V1beta2_JobStatus.Substate = .unspecified

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The job state.
  public enum State: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// The job state is unknown.
    case unspecified // = 0

    /// The job is pending; it has been submitted, but is not yet running.
    case pending // = 1

    /// Job has been received by the service and completed initial setup;
    /// it will soon be submitted to the cluster.
    case setupDone // = 8

    /// The job is running on the cluster.
    case running // = 2

    /// A CancelJob request has been received, but is pending.
    case cancelPending // = 3

    /// Transient in-flight resources have been canceled, and the request to
    /// cancel the running job has been issued to the cluster.
    case cancelStarted // = 7

    /// The job cancellation was successful.
    case cancelled // = 4

    /// The job has completed successfully.
    case done // = 5

    /// The job has completed, but encountered an error.
    case error // = 6

    /// Job attempt has failed. The detail field contains failure details for
    /// this attempt.
    ///
    /// Applies to restartable jobs only.
    case attemptFailure // = 9
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .pending
      case 2: self = .running
      case 3: self = .cancelPending
      case 4: self = .cancelled
      case 5: self = .done
      case 6: self = .error
      case 7: self = .cancelStarted
      case 8: self = .setupDone
      case 9: self = .attemptFailure
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .pending: return 1
      case .running: return 2
      case .cancelPending: return 3
      case .cancelled: return 4
      case .done: return 5
      case .error: return 6
      case .cancelStarted: return 7
      case .setupDone: return 8
      case .attemptFailure: return 9
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  /// The job substate.
  public enum Substate: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// The job substate is unknown.
    case unspecified // = 0

    /// The Job is submitted to the agent.
    ///
    /// Applies to RUNNING state.
    case submitted // = 1

    /// The Job has been received and is awaiting execution (it may be waiting
    /// for a condition to be met). See the "details" field for the reason for
    /// the delay.
    ///
    /// Applies to RUNNING state.
    case queued // = 2

    /// The agent-reported status is out of date, which may be caused by a
    /// loss of communication between the agent and Dataproc. If the
    /// agent does not send a timely update, the job will fail.
    ///
    /// Applies to RUNNING state.
    case staleStatus // = 3
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .submitted
      case 2: self = .queued
      case 3: self = .staleStatus
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .submitted: return 1
      case .queued: return 2
      case .staleStatus: return 3
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}

  fileprivate var _stateStartTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
}

#if swift(>=4.2)

extension Google_Cloud_Dataproc_V1beta2_JobStatus.State: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dataproc_V1beta2_JobStatus.State] = [
    .unspecified,
    .pending,
    .setupDone,
    .running,
    .cancelPending,
    .cancelStarted,
    .cancelled,
    .done,
    .error,
    .attemptFailure,
  ]
}

extension Google_Cloud_Dataproc_V1beta2_JobStatus.Substate: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dataproc_V1beta2_JobStatus.Substate] = [
    .unspecified,
    .submitted,
    .queued,
    .staleStatus,
  ]
}

#endif  // swift(>=4.2)

/// Encapsulates the full scoping used to reference a job.
public struct Google_Cloud_Dataproc_V1beta2_JobReference {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The ID of the Google Cloud Platform project that the job
  /// belongs to.
  public var projectID: String = String()

  /// Optional. The job ID, which must be unique within the project.
  /// The ID must contain only letters (a-z, A-Z), numbers (0-9),
  /// underscores (_), or hyphens (-). The maximum length is 100 characters.
  ///
  /// If not specified by the caller, the job ID will be provided by the server.
  public var jobID: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A YARN application created by a job. Application information is a subset of
/// <code>org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto</code>.
///
/// **Beta Feature**: This report is available for testing purposes only. It may
/// be changed before final release.
public struct Google_Cloud_Dataproc_V1beta2_YarnApplication {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The application name.
  public var name: String = String()

  /// Output only. The application state.
  public var state: Google_Cloud_Dataproc_V1beta2_YarnApplication.State = .unspecified

  /// Output only. The numerical progress of the application, from 1 to 100.
  public var progress: Float = 0

  /// Output only. The HTTP URL of the ApplicationMaster, HistoryServer, or
  /// TimelineServer that provides application-specific information. The URL uses
  /// the internal hostname, and requires a proxy server for resolution and,
  /// possibly, access.
  public var trackingURL: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The application state, corresponding to
  /// <code>YarnProtos.YarnApplicationStateProto</code>.
  public enum State: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// Status is unspecified.
    case unspecified // = 0

    /// Status is NEW.
    case new // = 1

    /// Status is NEW_SAVING.
    case newSaving // = 2

    /// Status is SUBMITTED.
    case submitted // = 3

    /// Status is ACCEPTED.
    case accepted // = 4

    /// Status is RUNNING.
    case running // = 5

    /// Status is FINISHED.
    case finished // = 6

    /// Status is FAILED.
    case failed // = 7

    /// Status is KILLED.
    case killed // = 8
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .new
      case 2: self = .newSaving
      case 3: self = .submitted
      case 4: self = .accepted
      case 5: self = .running
      case 6: self = .finished
      case 7: self = .failed
      case 8: self = .killed
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .new: return 1
      case .newSaving: return 2
      case .submitted: return 3
      case .accepted: return 4
      case .running: return 5
      case .finished: return 6
      case .failed: return 7
      case .killed: return 8
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}
}

#if swift(>=4.2)

extension Google_Cloud_Dataproc_V1beta2_YarnApplication.State: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dataproc_V1beta2_YarnApplication.State] = [
    .unspecified,
    .new,
    .newSaving,
    .submitted,
    .accepted,
    .running,
    .finished,
    .failed,
    .killed,
  ]
}

#endif  // swift(>=4.2)

/// A Dataproc job resource.
public struct Google_Cloud_Dataproc_V1beta2_Job {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. The fully qualified reference to the job, which can be used to
  /// obtain the equivalent REST path of the job resource. If this property
  /// is not specified when a job is created, the server generates a
  /// <code>job_id</code>.
  public var reference: Google_Cloud_Dataproc_V1beta2_JobReference {
    get {return _storage._reference ?? Google_Cloud_Dataproc_V1beta2_JobReference()}
    set {_uniqueStorage()._reference = newValue}
  }
  /// Returns true if `reference` has been explicitly set.
  public var hasReference: Bool {return _storage._reference != nil}
  /// Clears the value of `reference`. Subsequent reads from it will return its default value.
  public mutating func clearReference() {_uniqueStorage()._reference = nil}

  /// Required. Job information, including how, when, and where to
  /// run the job.
  public var placement: Google_Cloud_Dataproc_V1beta2_JobPlacement {
    get {return _storage._placement ?? Google_Cloud_Dataproc_V1beta2_JobPlacement()}
    set {_uniqueStorage()._placement = newValue}
  }
  /// Returns true if `placement` has been explicitly set.
  public var hasPlacement: Bool {return _storage._placement != nil}
  /// Clears the value of `placement`. Subsequent reads from it will return its default value.
  public mutating func clearPlacement() {_uniqueStorage()._placement = nil}

  /// Required. The application/framework-specific portion of the job.
  public var typeJob: OneOf_TypeJob? {
    get {return _storage._typeJob}
    set {_uniqueStorage()._typeJob = newValue}
  }

  /// Optional. Job is a Hadoop job.
  public var hadoopJob: Google_Cloud_Dataproc_V1beta2_HadoopJob {
    get {
      if case .hadoopJob(let v)? = _storage._typeJob {return v}
      return Google_Cloud_Dataproc_V1beta2_HadoopJob()
    }
    set {_uniqueStorage()._typeJob = .hadoopJob(newValue)}
  }

  /// Optional. Job is a Spark job.
  public var sparkJob: Google_Cloud_Dataproc_V1beta2_SparkJob {
    get {
      if case .sparkJob(let v)? = _storage._typeJob {return v}
      return Google_Cloud_Dataproc_V1beta2_SparkJob()
    }
    set {_uniqueStorage()._typeJob = .sparkJob(newValue)}
  }

  /// Optional. Job is a PySpark job.
  public var pysparkJob: Google_Cloud_Dataproc_V1beta2_PySparkJob {
    get {
      if case .pysparkJob(let v)? = _storage._typeJob {return v}
      return Google_Cloud_Dataproc_V1beta2_PySparkJob()
    }
    set {_uniqueStorage()._typeJob = .pysparkJob(newValue)}
  }

  /// Optional. Job is a Hive job.
  public var hiveJob: Google_Cloud_Dataproc_V1beta2_HiveJob {
    get {
      if case .hiveJob(let v)? = _storage._typeJob {return v}
      return Google_Cloud_Dataproc_V1beta2_HiveJob()
    }
    set {_uniqueStorage()._typeJob = .hiveJob(newValue)}
  }

  /// Optional. Job is a Pig job.
  public var pigJob: Google_Cloud_Dataproc_V1beta2_PigJob {
    get {
      if case .pigJob(let v)? = _storage._typeJob {return v}
      return Google_Cloud_Dataproc_V1beta2_PigJob()
    }
    set {_uniqueStorage()._typeJob = .pigJob(newValue)}
  }

  /// Optional. Job is a SparkR job.
  public var sparkRJob: Google_Cloud_Dataproc_V1beta2_SparkRJob {
    get {
      if case .sparkRJob(let v)? = _storage._typeJob {return v}
      return Google_Cloud_Dataproc_V1beta2_SparkRJob()
    }
    set {_uniqueStorage()._typeJob = .sparkRJob(newValue)}
  }

  /// Optional. Job is a SparkSql job.
  public var sparkSqlJob: Google_Cloud_Dataproc_V1beta2_SparkSqlJob {
    get {
      if case .sparkSqlJob(let v)? = _storage._typeJob {return v}
      return Google_Cloud_Dataproc_V1beta2_SparkSqlJob()
    }
    set {_uniqueStorage()._typeJob = .sparkSqlJob(newValue)}
  }

  /// Optional. Job is a Presto job.
  public var prestoJob: Google_Cloud_Dataproc_V1beta2_PrestoJob {
    get {
      if case .prestoJob(let v)? = _storage._typeJob {return v}
      return Google_Cloud_Dataproc_V1beta2_PrestoJob()
    }
    set {_uniqueStorage()._typeJob = .prestoJob(newValue)}
  }

  /// Output only. The job status. Additional application-specific
  /// status information may be contained in the <code>type_job</code>
  /// and <code>yarn_applications</code> fields.
  public var status: Google_Cloud_Dataproc_V1beta2_JobStatus {
    get {return _storage._status ?? Google_Cloud_Dataproc_V1beta2_JobStatus()}
    set {_uniqueStorage()._status = newValue}
  }
  /// Returns true if `status` has been explicitly set.
  public var hasStatus: Bool {return _storage._status != nil}
  /// Clears the value of `status`. Subsequent reads from it will return its default value.
  public mutating func clearStatus() {_uniqueStorage()._status = nil}

  /// Output only. The previous job status.
  public var statusHistory: [Google_Cloud_Dataproc_V1beta2_JobStatus] {
    get {return _storage._statusHistory}
    set {_uniqueStorage()._statusHistory = newValue}
  }

  /// Output only. The collection of YARN applications spun up by this job.
  ///
  /// **Beta** Feature: This report is available for testing purposes only. It
  /// may be changed before final release.
  public var yarnApplications: [Google_Cloud_Dataproc_V1beta2_YarnApplication] {
    get {return _storage._yarnApplications}
    set {_uniqueStorage()._yarnApplications = newValue}
  }

  /// Output only. The email address of the user submitting the job. For jobs
  /// submitted on the cluster, the address is <code>username@hostname</code>.
  public var submittedBy: String {
    get {return _storage._submittedBy}
    set {_uniqueStorage()._submittedBy = newValue}
  }

  /// Output only. A URI pointing to the location of the stdout of the job's
  /// driver program.
  public var driverOutputResourceUri: String {
    get {return _storage._driverOutputResourceUri}
    set {_uniqueStorage()._driverOutputResourceUri = newValue}
  }

  /// Output only. If present, the location of miscellaneous control files
  /// which may be used as part of job setup and handling. If not present,
  /// control files may be placed in the same location as `driver_output_uri`.
  public var driverControlFilesUri: String {
    get {return _storage._driverControlFilesUri}
    set {_uniqueStorage()._driverControlFilesUri = newValue}
  }

  /// Optional. The labels to associate with this job.
  /// Label **keys** must contain 1 to 63 characters, and must conform to
  /// [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
  /// Label **values** may be empty, but, if present, must contain 1 to 63
  /// characters, and must conform to [RFC
  /// 1035](https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be
  /// associated with a job.
  public var labels: Dictionary<String,String> {
    get {return _storage._labels}
    set {_uniqueStorage()._labels = newValue}
  }

  /// Optional. Job scheduling configuration.
  public var scheduling: Google_Cloud_Dataproc_V1beta2_JobScheduling {
    get {return _storage._scheduling ?? Google_Cloud_Dataproc_V1beta2_JobScheduling()}
    set {_uniqueStorage()._scheduling = newValue}
  }
  /// Returns true if `scheduling` has been explicitly set.
  public var hasScheduling: Bool {return _storage._scheduling != nil}
  /// Clears the value of `scheduling`. Subsequent reads from it will return its default value.
  public mutating func clearScheduling() {_uniqueStorage()._scheduling = nil}

  /// Output only. A UUID that uniquely identifies a job within the project
  /// over time. This is in contrast to a user-settable reference.job_id that
  /// may be reused over time.
  public var jobUuid: String {
    get {return _storage._jobUuid}
    set {_uniqueStorage()._jobUuid = newValue}
  }

  /// Output only. Indicates whether the job is completed. If the value is `false`,
  /// the job is still in progress. If `true`, the job is completed, and
  /// `status.state` field will indicate if it was successful, failed,
  /// or cancelled.
  public var done: Bool {
    get {return _storage._done}
    set {_uniqueStorage()._done = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Required. The application/framework-specific portion of the job.
  public enum OneOf_TypeJob: Equatable {
    /// Optional. Job is a Hadoop job.
    case hadoopJob(Google_Cloud_Dataproc_V1beta2_HadoopJob)
    /// Optional. Job is a Spark job.
    case sparkJob(Google_Cloud_Dataproc_V1beta2_SparkJob)
    /// Optional. Job is a PySpark job.
    case pysparkJob(Google_Cloud_Dataproc_V1beta2_PySparkJob)
    /// Optional. Job is a Hive job.
    case hiveJob(Google_Cloud_Dataproc_V1beta2_HiveJob)
    /// Optional. Job is a Pig job.
    case pigJob(Google_Cloud_Dataproc_V1beta2_PigJob)
    /// Optional. Job is a SparkR job.
    case sparkRJob(Google_Cloud_Dataproc_V1beta2_SparkRJob)
    /// Optional. Job is a SparkSql job.
    case sparkSqlJob(Google_Cloud_Dataproc_V1beta2_SparkSqlJob)
    /// Optional. Job is a Presto job.
    case prestoJob(Google_Cloud_Dataproc_V1beta2_PrestoJob)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_Job.OneOf_TypeJob, rhs: Google_Cloud_Dataproc_V1beta2_Job.OneOf_TypeJob) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.hadoopJob, .hadoopJob): return {
        guard case .hadoopJob(let l) = lhs, case .hadoopJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.sparkJob, .sparkJob): return {
        guard case .sparkJob(let l) = lhs, case .sparkJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.pysparkJob, .pysparkJob): return {
        guard case .pysparkJob(let l) = lhs, case .pysparkJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.hiveJob, .hiveJob): return {
        guard case .hiveJob(let l) = lhs, case .hiveJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.pigJob, .pigJob): return {
        guard case .pigJob(let l) = lhs, case .pigJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.sparkRJob, .sparkRJob): return {
        guard case .sparkRJob(let l) = lhs, case .sparkRJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.sparkSqlJob, .sparkSqlJob): return {
        guard case .sparkSqlJob(let l) = lhs, case .sparkSqlJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.prestoJob, .prestoJob): return {
        guard case .prestoJob(let l) = lhs, case .prestoJob(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Job scheduling options.
public struct Google_Cloud_Dataproc_V1beta2_JobScheduling {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. Maximum number of times per hour a driver may be restarted as
  /// a result of driver terminating with non-zero code before job is
  /// reported failed.
  ///
  /// A job may be reported as thrashing if driver exits with non-zero code
  /// 4 times within 10 minute window.
  ///
  /// Maximum value is 10.
  public var maxFailuresPerHour: Int32 = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Job Operation metadata.
public struct Google_Cloud_Dataproc_V1beta2_JobMetadata {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The job id.
  public var jobID: String = String()

  /// Output only. Most recent job status.
  public var status: Google_Cloud_Dataproc_V1beta2_JobStatus {
    get {return _status ?? Google_Cloud_Dataproc_V1beta2_JobStatus()}
    set {_status = newValue}
  }
  /// Returns true if `status` has been explicitly set.
  public var hasStatus: Bool {return self._status != nil}
  /// Clears the value of `status`. Subsequent reads from it will return its default value.
  public mutating func clearStatus() {self._status = nil}

  /// Output only. Operation type.
  public var operationType: String = String()

  /// Output only. Job submission time.
  public var startTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _startTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_startTime = newValue}
  }
  /// Returns true if `startTime` has been explicitly set.
  public var hasStartTime: Bool {return self._startTime != nil}
  /// Clears the value of `startTime`. Subsequent reads from it will return its default value.
  public mutating func clearStartTime() {self._startTime = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _status: Google_Cloud_Dataproc_V1beta2_JobStatus? = nil
  fileprivate var _startTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
}

/// A request to submit a job.
public struct Google_Cloud_Dataproc_V1beta2_SubmitJobRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The ID of the Google Cloud Platform project that the job
  /// belongs to.
  public var projectID: String = String()

  /// Required. The Dataproc region in which to handle the request.
  public var region: String = String()

  /// Required. The job resource.
  public var job: Google_Cloud_Dataproc_V1beta2_Job {
    get {return _job ?? Google_Cloud_Dataproc_V1beta2_Job()}
    set {_job = newValue}
  }
  /// Returns true if `job` has been explicitly set.
  public var hasJob: Bool {return self._job != nil}
  /// Clears the value of `job`. Subsequent reads from it will return its default value.
  public mutating func clearJob() {self._job = nil}

  /// Optional. A unique id used to identify the request. If the server
  /// receives two [SubmitJobRequest][google.cloud.dataproc.v1beta2.SubmitJobRequest] requests  with the same
  /// id, then the second request will be ignored and the
  /// first [Job][google.cloud.dataproc.v1beta2.Job] created and stored in the backend
  /// is returned.
  ///
  /// It is recommended to always set this value to a
  /// [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).
  ///
  /// The id must contain only letters (a-z, A-Z), numbers (0-9),
  /// underscores (_), and hyphens (-). The maximum length is 40 characters.
  public var requestID: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _job: Google_Cloud_Dataproc_V1beta2_Job? = nil
}

/// A request to get the resource representation for a job in a project.
public struct Google_Cloud_Dataproc_V1beta2_GetJobRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The ID of the Google Cloud Platform project that the job
  /// belongs to.
  public var projectID: String = String()

  /// Required. The Dataproc region in which to handle the request.
  public var region: String = String()

  /// Required. The job ID.
  public var jobID: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A request to list jobs in a project.
public struct Google_Cloud_Dataproc_V1beta2_ListJobsRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The ID of the Google Cloud Platform project that the job
  /// belongs to.
  public var projectID: String = String()

  /// Required. The Dataproc region in which to handle the request.
  public var region: String = String()

  /// Optional. The number of results to return in each response.
  public var pageSize: Int32 = 0

  /// Optional. The page token, returned by a previous call, to request the
  /// next page of results.
  public var pageToken: String = String()

  /// Optional. If set, the returned jobs list includes only jobs that were
  /// submitted to the named cluster.
  public var clusterName: String = String()

  /// Optional. Specifies enumerated categories of jobs to list.
  /// (default = match ALL jobs).
  ///
  /// If `filter` is provided, `jobStateMatcher` will be ignored.
  public var jobStateMatcher: Google_Cloud_Dataproc_V1beta2_ListJobsRequest.JobStateMatcher = .all

  /// Optional. A filter constraining the jobs to list. Filters are
  /// case-sensitive and have the following syntax:
  ///
  /// [field = value] AND [field [= value]] ...
  ///
  /// where **field** is `status.state` or `labels.[KEY]`, and `[KEY]` is a label
  /// key. **value** can be `*` to match all values.
  /// `status.state` can be either `ACTIVE` or `NON_ACTIVE`.
  /// Only the logical `AND` operator is supported; space-separated items are
  /// treated as having an implicit `AND` operator.
  ///
  /// Example filter:
  ///
  /// status.state = ACTIVE AND labels.env = staging AND labels.starred = *
  public var filter: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// A matcher that specifies categories of job states.
  public enum JobStateMatcher: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// Match all jobs, regardless of state.
    case all // = 0

    /// Only match jobs in non-terminal states: PENDING, RUNNING, or
    /// CANCEL_PENDING.
    case active // = 1

    /// Only match jobs in terminal states: CANCELLED, DONE, or ERROR.
    case nonActive // = 2
    case UNRECOGNIZED(Int)

    public init() {
      self = .all
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .all
      case 1: self = .active
      case 2: self = .nonActive
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .all: return 0
      case .active: return 1
      case .nonActive: return 2
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}
}

#if swift(>=4.2)

extension Google_Cloud_Dataproc_V1beta2_ListJobsRequest.JobStateMatcher: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dataproc_V1beta2_ListJobsRequest.JobStateMatcher] = [
    .all,
    .active,
    .nonActive,
  ]
}

#endif  // swift(>=4.2)

/// A request to update a job.
public struct Google_Cloud_Dataproc_V1beta2_UpdateJobRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The ID of the Google Cloud Platform project that the job
  /// belongs to.
  public var projectID: String = String()

  /// Required. The Dataproc region in which to handle the request.
  public var region: String = String()

  /// Required. The job ID.
  public var jobID: String = String()

  /// Required. The changes to the job.
  public var job: Google_Cloud_Dataproc_V1beta2_Job {
    get {return _job ?? Google_Cloud_Dataproc_V1beta2_Job()}
    set {_job = newValue}
  }
  /// Returns true if `job` has been explicitly set.
  public var hasJob: Bool {return self._job != nil}
  /// Clears the value of `job`. Subsequent reads from it will return its default value.
  public mutating func clearJob() {self._job = nil}

  /// Required. Specifies the path, relative to <code>Job</code>, of
  /// the field to update. For example, to update the labels of a Job the
  /// <code>update_mask</code> parameter would be specified as
  /// <code>labels</code>, and the `PATCH` request body would specify the new
  /// value. <strong>Note:</strong> Currently, <code>labels</code> is the only
  /// field that can be updated.
  public var updateMask: SwiftProtobuf.Google_Protobuf_FieldMask {
    get {return _updateMask ?? SwiftProtobuf.Google_Protobuf_FieldMask()}
    set {_updateMask = newValue}
  }
  /// Returns true if `updateMask` has been explicitly set.
  public var hasUpdateMask: Bool {return self._updateMask != nil}
  /// Clears the value of `updateMask`. Subsequent reads from it will return its default value.
  public mutating func clearUpdateMask() {self._updateMask = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _job: Google_Cloud_Dataproc_V1beta2_Job? = nil
  fileprivate var _updateMask: SwiftProtobuf.Google_Protobuf_FieldMask? = nil
}

/// A list of jobs in a project.
public struct Google_Cloud_Dataproc_V1beta2_ListJobsResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. Jobs list.
  public var jobs: [Google_Cloud_Dataproc_V1beta2_Job] = []

  /// Optional. This token is included in the response if there are more results
  /// to fetch. To fetch additional results, provide this value as the
  /// `page_token` in a subsequent <code>ListJobsRequest</code>.
  public var nextPageToken: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A request to cancel a job.
public struct Google_Cloud_Dataproc_V1beta2_CancelJobRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The ID of the Google Cloud Platform project that the job
  /// belongs to.
  public var projectID: String = String()

  /// Required. The Dataproc region in which to handle the request.
  public var region: String = String()

  /// Required. The job ID.
  public var jobID: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A request to delete a job.
public struct Google_Cloud_Dataproc_V1beta2_DeleteJobRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The ID of the Google Cloud Platform project that the job
  /// belongs to.
  public var projectID: String = String()

  /// Required. The Dataproc region in which to handle the request.
  public var region: String = String()

  /// Required. The job ID.
  public var jobID: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.dataproc.v1beta2"

extension Google_Cloud_Dataproc_V1beta2_LoggingConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LoggingConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .standard(proto: "driver_log_levels"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 2: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufEnumMap<SwiftProtobuf.ProtobufString,Google_Cloud_Dataproc_V1beta2_LoggingConfig.Level>.self, value: &self.driverLogLevels) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.driverLogLevels.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufEnumMap<SwiftProtobuf.ProtobufString,Google_Cloud_Dataproc_V1beta2_LoggingConfig.Level>.self, value: self.driverLogLevels, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_LoggingConfig, rhs: Google_Cloud_Dataproc_V1beta2_LoggingConfig) -> Bool {
    if lhs.driverLogLevels != rhs.driverLogLevels {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_LoggingConfig.Level: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "LEVEL_UNSPECIFIED"),
    1: .same(proto: "ALL"),
    2: .same(proto: "TRACE"),
    3: .same(proto: "DEBUG"),
    4: .same(proto: "INFO"),
    5: .same(proto: "WARN"),
    6: .same(proto: "ERROR"),
    7: .same(proto: "FATAL"),
    8: .same(proto: "OFF"),
  ]
}

extension Google_Cloud_Dataproc_V1beta2_HadoopJob: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".HadoopJob"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "main_jar_file_uri"),
    2: .standard(proto: "main_class"),
    3: .same(proto: "args"),
    4: .standard(proto: "jar_file_uris"),
    5: .standard(proto: "file_uris"),
    6: .standard(proto: "archive_uris"),
    7: .same(proto: "properties"),
    8: .standard(proto: "logging_config"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        if self.driver != nil {try decoder.handleConflictingOneOf()}
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {self.driver = .mainJarFileUri(v)}
      }()
      case 2: try {
        if self.driver != nil {try decoder.handleConflictingOneOf()}
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {self.driver = .mainClass(v)}
      }()
      case 3: try { try decoder.decodeRepeatedStringField(value: &self.args) }()
      case 4: try { try decoder.decodeRepeatedStringField(value: &self.jarFileUris) }()
      case 5: try { try decoder.decodeRepeatedStringField(value: &self.fileUris) }()
      case 6: try { try decoder.decodeRepeatedStringField(value: &self.archiveUris) }()
      case 7: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.properties) }()
      case 8: try { try decoder.decodeSingularMessageField(value: &self._loggingConfig) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.driver {
    case .mainJarFileUri?: try {
      guard case .mainJarFileUri(let v)? = self.driver else { preconditionFailure() }
      try visitor.visitSingularStringField(value: v, fieldNumber: 1)
    }()
    case .mainClass?: try {
      guard case .mainClass(let v)? = self.driver else { preconditionFailure() }
      try visitor.visitSingularStringField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    if !self.args.isEmpty {
      try visitor.visitRepeatedStringField(value: self.args, fieldNumber: 3)
    }
    if !self.jarFileUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.jarFileUris, fieldNumber: 4)
    }
    if !self.fileUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.fileUris, fieldNumber: 5)
    }
    if !self.archiveUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.archiveUris, fieldNumber: 6)
    }
    if !self.properties.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.properties, fieldNumber: 7)
    }
    if let v = self._loggingConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_HadoopJob, rhs: Google_Cloud_Dataproc_V1beta2_HadoopJob) -> Bool {
    if lhs.driver != rhs.driver {return false}
    if lhs.args != rhs.args {return false}
    if lhs.jarFileUris != rhs.jarFileUris {return false}
    if lhs.fileUris != rhs.fileUris {return false}
    if lhs.archiveUris != rhs.archiveUris {return false}
    if lhs.properties != rhs.properties {return false}
    if lhs._loggingConfig != rhs._loggingConfig {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_SparkJob: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SparkJob"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "main_jar_file_uri"),
    2: .standard(proto: "main_class"),
    3: .same(proto: "args"),
    4: .standard(proto: "jar_file_uris"),
    5: .standard(proto: "file_uris"),
    6: .standard(proto: "archive_uris"),
    7: .same(proto: "properties"),
    8: .standard(proto: "logging_config"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        if self.driver != nil {try decoder.handleConflictingOneOf()}
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {self.driver = .mainJarFileUri(v)}
      }()
      case 2: try {
        if self.driver != nil {try decoder.handleConflictingOneOf()}
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {self.driver = .mainClass(v)}
      }()
      case 3: try { try decoder.decodeRepeatedStringField(value: &self.args) }()
      case 4: try { try decoder.decodeRepeatedStringField(value: &self.jarFileUris) }()
      case 5: try { try decoder.decodeRepeatedStringField(value: &self.fileUris) }()
      case 6: try { try decoder.decodeRepeatedStringField(value: &self.archiveUris) }()
      case 7: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.properties) }()
      case 8: try { try decoder.decodeSingularMessageField(value: &self._loggingConfig) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.driver {
    case .mainJarFileUri?: try {
      guard case .mainJarFileUri(let v)? = self.driver else { preconditionFailure() }
      try visitor.visitSingularStringField(value: v, fieldNumber: 1)
    }()
    case .mainClass?: try {
      guard case .mainClass(let v)? = self.driver else { preconditionFailure() }
      try visitor.visitSingularStringField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    if !self.args.isEmpty {
      try visitor.visitRepeatedStringField(value: self.args, fieldNumber: 3)
    }
    if !self.jarFileUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.jarFileUris, fieldNumber: 4)
    }
    if !self.fileUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.fileUris, fieldNumber: 5)
    }
    if !self.archiveUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.archiveUris, fieldNumber: 6)
    }
    if !self.properties.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.properties, fieldNumber: 7)
    }
    if let v = self._loggingConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_SparkJob, rhs: Google_Cloud_Dataproc_V1beta2_SparkJob) -> Bool {
    if lhs.driver != rhs.driver {return false}
    if lhs.args != rhs.args {return false}
    if lhs.jarFileUris != rhs.jarFileUris {return false}
    if lhs.fileUris != rhs.fileUris {return false}
    if lhs.archiveUris != rhs.archiveUris {return false}
    if lhs.properties != rhs.properties {return false}
    if lhs._loggingConfig != rhs._loggingConfig {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_PySparkJob: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".PySparkJob"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "main_python_file_uri"),
    2: .same(proto: "args"),
    3: .standard(proto: "python_file_uris"),
    4: .standard(proto: "jar_file_uris"),
    5: .standard(proto: "file_uris"),
    6: .standard(proto: "archive_uris"),
    7: .same(proto: "properties"),
    8: .standard(proto: "logging_config"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.mainPythonFileUri) }()
      case 2: try { try decoder.decodeRepeatedStringField(value: &self.args) }()
      case 3: try { try decoder.decodeRepeatedStringField(value: &self.pythonFileUris) }()
      case 4: try { try decoder.decodeRepeatedStringField(value: &self.jarFileUris) }()
      case 5: try { try decoder.decodeRepeatedStringField(value: &self.fileUris) }()
      case 6: try { try decoder.decodeRepeatedStringField(value: &self.archiveUris) }()
      case 7: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.properties) }()
      case 8: try { try decoder.decodeSingularMessageField(value: &self._loggingConfig) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.mainPythonFileUri.isEmpty {
      try visitor.visitSingularStringField(value: self.mainPythonFileUri, fieldNumber: 1)
    }
    if !self.args.isEmpty {
      try visitor.visitRepeatedStringField(value: self.args, fieldNumber: 2)
    }
    if !self.pythonFileUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.pythonFileUris, fieldNumber: 3)
    }
    if !self.jarFileUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.jarFileUris, fieldNumber: 4)
    }
    if !self.fileUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.fileUris, fieldNumber: 5)
    }
    if !self.archiveUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.archiveUris, fieldNumber: 6)
    }
    if !self.properties.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.properties, fieldNumber: 7)
    }
    if let v = self._loggingConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_PySparkJob, rhs: Google_Cloud_Dataproc_V1beta2_PySparkJob) -> Bool {
    if lhs.mainPythonFileUri != rhs.mainPythonFileUri {return false}
    if lhs.args != rhs.args {return false}
    if lhs.pythonFileUris != rhs.pythonFileUris {return false}
    if lhs.jarFileUris != rhs.jarFileUris {return false}
    if lhs.fileUris != rhs.fileUris {return false}
    if lhs.archiveUris != rhs.archiveUris {return false}
    if lhs.properties != rhs.properties {return false}
    if lhs._loggingConfig != rhs._loggingConfig {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_QueryList: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".QueryList"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "queries"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedStringField(value: &self.queries) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.queries.isEmpty {
      try visitor.visitRepeatedStringField(value: self.queries, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_QueryList, rhs: Google_Cloud_Dataproc_V1beta2_QueryList) -> Bool {
    if lhs.queries != rhs.queries {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_HiveJob: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".HiveJob"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "query_file_uri"),
    2: .standard(proto: "query_list"),
    3: .standard(proto: "continue_on_failure"),
    4: .standard(proto: "script_variables"),
    5: .same(proto: "properties"),
    6: .standard(proto: "jar_file_uris"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        if self.queries != nil {try decoder.handleConflictingOneOf()}
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {self.queries = .queryFileUri(v)}
      }()
      case 2: try {
        var v: Google_Cloud_Dataproc_V1beta2_QueryList?
        if let current = self.queries {
          try decoder.handleConflictingOneOf()
          if case .queryList(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.queries = .queryList(v)}
      }()
      case 3: try { try decoder.decodeSingularBoolField(value: &self.continueOnFailure) }()
      case 4: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.scriptVariables) }()
      case 5: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.properties) }()
      case 6: try { try decoder.decodeRepeatedStringField(value: &self.jarFileUris) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.queries {
    case .queryFileUri?: try {
      guard case .queryFileUri(let v)? = self.queries else { preconditionFailure() }
      try visitor.visitSingularStringField(value: v, fieldNumber: 1)
    }()
    case .queryList?: try {
      guard case .queryList(let v)? = self.queries else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    if self.continueOnFailure != false {
      try visitor.visitSingularBoolField(value: self.continueOnFailure, fieldNumber: 3)
    }
    if !self.scriptVariables.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.scriptVariables, fieldNumber: 4)
    }
    if !self.properties.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.properties, fieldNumber: 5)
    }
    if !self.jarFileUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.jarFileUris, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_HiveJob, rhs: Google_Cloud_Dataproc_V1beta2_HiveJob) -> Bool {
    if lhs.queries != rhs.queries {return false}
    if lhs.continueOnFailure != rhs.continueOnFailure {return false}
    if lhs.scriptVariables != rhs.scriptVariables {return false}
    if lhs.properties != rhs.properties {return false}
    if lhs.jarFileUris != rhs.jarFileUris {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_SparkSqlJob: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SparkSqlJob"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "query_file_uri"),
    2: .standard(proto: "query_list"),
    3: .standard(proto: "script_variables"),
    4: .same(proto: "properties"),
    56: .standard(proto: "jar_file_uris"),
    6: .standard(proto: "logging_config"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        if self.queries != nil {try decoder.handleConflictingOneOf()}
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {self.queries = .queryFileUri(v)}
      }()
      case 2: try {
        var v: Google_Cloud_Dataproc_V1beta2_QueryList?
        if let current = self.queries {
          try decoder.handleConflictingOneOf()
          if case .queryList(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.queries = .queryList(v)}
      }()
      case 3: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.scriptVariables) }()
      case 4: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.properties) }()
      case 6: try { try decoder.decodeSingularMessageField(value: &self._loggingConfig) }()
      case 56: try { try decoder.decodeRepeatedStringField(value: &self.jarFileUris) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.queries {
    case .queryFileUri?: try {
      guard case .queryFileUri(let v)? = self.queries else { preconditionFailure() }
      try visitor.visitSingularStringField(value: v, fieldNumber: 1)
    }()
    case .queryList?: try {
      guard case .queryList(let v)? = self.queries else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    if !self.scriptVariables.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.scriptVariables, fieldNumber: 3)
    }
    if !self.properties.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.properties, fieldNumber: 4)
    }
    if let v = self._loggingConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
    }
    if !self.jarFileUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.jarFileUris, fieldNumber: 56)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_SparkSqlJob, rhs: Google_Cloud_Dataproc_V1beta2_SparkSqlJob) -> Bool {
    if lhs.queries != rhs.queries {return false}
    if lhs.scriptVariables != rhs.scriptVariables {return false}
    if lhs.properties != rhs.properties {return false}
    if lhs.jarFileUris != rhs.jarFileUris {return false}
    if lhs._loggingConfig != rhs._loggingConfig {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_PigJob: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".PigJob"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "query_file_uri"),
    2: .standard(proto: "query_list"),
    3: .standard(proto: "continue_on_failure"),
    4: .standard(proto: "script_variables"),
    5: .same(proto: "properties"),
    6: .standard(proto: "jar_file_uris"),
    7: .standard(proto: "logging_config"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        if self.queries != nil {try decoder.handleConflictingOneOf()}
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {self.queries = .queryFileUri(v)}
      }()
      case 2: try {
        var v: Google_Cloud_Dataproc_V1beta2_QueryList?
        if let current = self.queries {
          try decoder.handleConflictingOneOf()
          if case .queryList(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.queries = .queryList(v)}
      }()
      case 3: try { try decoder.decodeSingularBoolField(value: &self.continueOnFailure) }()
      case 4: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.scriptVariables) }()
      case 5: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.properties) }()
      case 6: try { try decoder.decodeRepeatedStringField(value: &self.jarFileUris) }()
      case 7: try { try decoder.decodeSingularMessageField(value: &self._loggingConfig) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.queries {
    case .queryFileUri?: try {
      guard case .queryFileUri(let v)? = self.queries else { preconditionFailure() }
      try visitor.visitSingularStringField(value: v, fieldNumber: 1)
    }()
    case .queryList?: try {
      guard case .queryList(let v)? = self.queries else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    if self.continueOnFailure != false {
      try visitor.visitSingularBoolField(value: self.continueOnFailure, fieldNumber: 3)
    }
    if !self.scriptVariables.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.scriptVariables, fieldNumber: 4)
    }
    if !self.properties.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.properties, fieldNumber: 5)
    }
    if !self.jarFileUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.jarFileUris, fieldNumber: 6)
    }
    if let v = self._loggingConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_PigJob, rhs: Google_Cloud_Dataproc_V1beta2_PigJob) -> Bool {
    if lhs.queries != rhs.queries {return false}
    if lhs.continueOnFailure != rhs.continueOnFailure {return false}
    if lhs.scriptVariables != rhs.scriptVariables {return false}
    if lhs.properties != rhs.properties {return false}
    if lhs.jarFileUris != rhs.jarFileUris {return false}
    if lhs._loggingConfig != rhs._loggingConfig {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_SparkRJob: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SparkRJob"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "main_r_file_uri"),
    2: .same(proto: "args"),
    3: .standard(proto: "file_uris"),
    4: .standard(proto: "archive_uris"),
    5: .same(proto: "properties"),
    6: .standard(proto: "logging_config"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.mainRFileUri) }()
      case 2: try { try decoder.decodeRepeatedStringField(value: &self.args) }()
      case 3: try { try decoder.decodeRepeatedStringField(value: &self.fileUris) }()
      case 4: try { try decoder.decodeRepeatedStringField(value: &self.archiveUris) }()
      case 5: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.properties) }()
      case 6: try { try decoder.decodeSingularMessageField(value: &self._loggingConfig) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.mainRFileUri.isEmpty {
      try visitor.visitSingularStringField(value: self.mainRFileUri, fieldNumber: 1)
    }
    if !self.args.isEmpty {
      try visitor.visitRepeatedStringField(value: self.args, fieldNumber: 2)
    }
    if !self.fileUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.fileUris, fieldNumber: 3)
    }
    if !self.archiveUris.isEmpty {
      try visitor.visitRepeatedStringField(value: self.archiveUris, fieldNumber: 4)
    }
    if !self.properties.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.properties, fieldNumber: 5)
    }
    if let v = self._loggingConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_SparkRJob, rhs: Google_Cloud_Dataproc_V1beta2_SparkRJob) -> Bool {
    if lhs.mainRFileUri != rhs.mainRFileUri {return false}
    if lhs.args != rhs.args {return false}
    if lhs.fileUris != rhs.fileUris {return false}
    if lhs.archiveUris != rhs.archiveUris {return false}
    if lhs.properties != rhs.properties {return false}
    if lhs._loggingConfig != rhs._loggingConfig {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_PrestoJob: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".PrestoJob"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "query_file_uri"),
    2: .standard(proto: "query_list"),
    3: .standard(proto: "continue_on_failure"),
    4: .standard(proto: "output_format"),
    5: .standard(proto: "client_tags"),
    6: .same(proto: "properties"),
    7: .standard(proto: "logging_config"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        if self.queries != nil {try decoder.handleConflictingOneOf()}
        var v: String?
        try decoder.decodeSingularStringField(value: &v)
        if let v = v {self.queries = .queryFileUri(v)}
      }()
      case 2: try {
        var v: Google_Cloud_Dataproc_V1beta2_QueryList?
        if let current = self.queries {
          try decoder.handleConflictingOneOf()
          if case .queryList(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.queries = .queryList(v)}
      }()
      case 3: try { try decoder.decodeSingularBoolField(value: &self.continueOnFailure) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.outputFormat) }()
      case 5: try { try decoder.decodeRepeatedStringField(value: &self.clientTags) }()
      case 6: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.properties) }()
      case 7: try { try decoder.decodeSingularMessageField(value: &self._loggingConfig) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.queries {
    case .queryFileUri?: try {
      guard case .queryFileUri(let v)? = self.queries else { preconditionFailure() }
      try visitor.visitSingularStringField(value: v, fieldNumber: 1)
    }()
    case .queryList?: try {
      guard case .queryList(let v)? = self.queries else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    if self.continueOnFailure != false {
      try visitor.visitSingularBoolField(value: self.continueOnFailure, fieldNumber: 3)
    }
    if !self.outputFormat.isEmpty {
      try visitor.visitSingularStringField(value: self.outputFormat, fieldNumber: 4)
    }
    if !self.clientTags.isEmpty {
      try visitor.visitRepeatedStringField(value: self.clientTags, fieldNumber: 5)
    }
    if !self.properties.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.properties, fieldNumber: 6)
    }
    if let v = self._loggingConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_PrestoJob, rhs: Google_Cloud_Dataproc_V1beta2_PrestoJob) -> Bool {
    if lhs.queries != rhs.queries {return false}
    if lhs.continueOnFailure != rhs.continueOnFailure {return false}
    if lhs.outputFormat != rhs.outputFormat {return false}
    if lhs.clientTags != rhs.clientTags {return false}
    if lhs.properties != rhs.properties {return false}
    if lhs._loggingConfig != rhs._loggingConfig {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_JobPlacement: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".JobPlacement"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "cluster_name"),
    2: .standard(proto: "cluster_uuid"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.clusterName) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.clusterUuid) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.clusterName.isEmpty {
      try visitor.visitSingularStringField(value: self.clusterName, fieldNumber: 1)
    }
    if !self.clusterUuid.isEmpty {
      try visitor.visitSingularStringField(value: self.clusterUuid, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_JobPlacement, rhs: Google_Cloud_Dataproc_V1beta2_JobPlacement) -> Bool {
    if lhs.clusterName != rhs.clusterName {return false}
    if lhs.clusterUuid != rhs.clusterUuid {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_JobStatus: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".JobStatus"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "state"),
    2: .same(proto: "details"),
    6: .standard(proto: "state_start_time"),
    7: .same(proto: "substate"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.state) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.details) }()
      case 6: try { try decoder.decodeSingularMessageField(value: &self._stateStartTime) }()
      case 7: try { try decoder.decodeSingularEnumField(value: &self.substate) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.state != .unspecified {
      try visitor.visitSingularEnumField(value: self.state, fieldNumber: 1)
    }
    if !self.details.isEmpty {
      try visitor.visitSingularStringField(value: self.details, fieldNumber: 2)
    }
    if let v = self._stateStartTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
    }
    if self.substate != .unspecified {
      try visitor.visitSingularEnumField(value: self.substate, fieldNumber: 7)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_JobStatus, rhs: Google_Cloud_Dataproc_V1beta2_JobStatus) -> Bool {
    if lhs.state != rhs.state {return false}
    if lhs.details != rhs.details {return false}
    if lhs._stateStartTime != rhs._stateStartTime {return false}
    if lhs.substate != rhs.substate {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_JobStatus.State: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "STATE_UNSPECIFIED"),
    1: .same(proto: "PENDING"),
    2: .same(proto: "RUNNING"),
    3: .same(proto: "CANCEL_PENDING"),
    4: .same(proto: "CANCELLED"),
    5: .same(proto: "DONE"),
    6: .same(proto: "ERROR"),
    7: .same(proto: "CANCEL_STARTED"),
    8: .same(proto: "SETUP_DONE"),
    9: .same(proto: "ATTEMPT_FAILURE"),
  ]
}

extension Google_Cloud_Dataproc_V1beta2_JobStatus.Substate: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "UNSPECIFIED"),
    1: .same(proto: "SUBMITTED"),
    2: .same(proto: "QUEUED"),
    3: .same(proto: "STALE_STATUS"),
  ]
}

extension Google_Cloud_Dataproc_V1beta2_JobReference: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".JobReference"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    2: .standard(proto: "job_id"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.jobID) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.jobID.isEmpty {
      try visitor.visitSingularStringField(value: self.jobID, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_JobReference, rhs: Google_Cloud_Dataproc_V1beta2_JobReference) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.jobID != rhs.jobID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_YarnApplication: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".YarnApplication"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .same(proto: "state"),
    3: .same(proto: "progress"),
    4: .standard(proto: "tracking_url"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeSingularEnumField(value: &self.state) }()
      case 3: try { try decoder.decodeSingularFloatField(value: &self.progress) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.trackingURL) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if self.state != .unspecified {
      try visitor.visitSingularEnumField(value: self.state, fieldNumber: 2)
    }
    if self.progress != 0 {
      try visitor.visitSingularFloatField(value: self.progress, fieldNumber: 3)
    }
    if !self.trackingURL.isEmpty {
      try visitor.visitSingularStringField(value: self.trackingURL, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_YarnApplication, rhs: Google_Cloud_Dataproc_V1beta2_YarnApplication) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.state != rhs.state {return false}
    if lhs.progress != rhs.progress {return false}
    if lhs.trackingURL != rhs.trackingURL {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_YarnApplication.State: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "STATE_UNSPECIFIED"),
    1: .same(proto: "NEW"),
    2: .same(proto: "NEW_SAVING"),
    3: .same(proto: "SUBMITTED"),
    4: .same(proto: "ACCEPTED"),
    5: .same(proto: "RUNNING"),
    6: .same(proto: "FINISHED"),
    7: .same(proto: "FAILED"),
    8: .same(proto: "KILLED"),
  ]
}

extension Google_Cloud_Dataproc_V1beta2_Job: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".Job"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "reference"),
    2: .same(proto: "placement"),
    3: .standard(proto: "hadoop_job"),
    4: .standard(proto: "spark_job"),
    5: .standard(proto: "pyspark_job"),
    6: .standard(proto: "hive_job"),
    7: .standard(proto: "pig_job"),
    21: .standard(proto: "spark_r_job"),
    12: .standard(proto: "spark_sql_job"),
    23: .standard(proto: "presto_job"),
    8: .same(proto: "status"),
    13: .standard(proto: "status_history"),
    9: .standard(proto: "yarn_applications"),
    10: .standard(proto: "submitted_by"),
    17: .standard(proto: "driver_output_resource_uri"),
    15: .standard(proto: "driver_control_files_uri"),
    18: .same(proto: "labels"),
    20: .same(proto: "scheduling"),
    22: .standard(proto: "job_uuid"),
    24: .same(proto: "done"),
  ]

  fileprivate class _StorageClass {
    var _reference: Google_Cloud_Dataproc_V1beta2_JobReference? = nil
    var _placement: Google_Cloud_Dataproc_V1beta2_JobPlacement? = nil
    var _typeJob: Google_Cloud_Dataproc_V1beta2_Job.OneOf_TypeJob?
    var _status: Google_Cloud_Dataproc_V1beta2_JobStatus? = nil
    var _statusHistory: [Google_Cloud_Dataproc_V1beta2_JobStatus] = []
    var _yarnApplications: [Google_Cloud_Dataproc_V1beta2_YarnApplication] = []
    var _submittedBy: String = String()
    var _driverOutputResourceUri: String = String()
    var _driverControlFilesUri: String = String()
    var _labels: Dictionary<String,String> = [:]
    var _scheduling: Google_Cloud_Dataproc_V1beta2_JobScheduling? = nil
    var _jobUuid: String = String()
    var _done: Bool = false

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _reference = source._reference
      _placement = source._placement
      _typeJob = source._typeJob
      _status = source._status
      _statusHistory = source._statusHistory
      _yarnApplications = source._yarnApplications
      _submittedBy = source._submittedBy
      _driverOutputResourceUri = source._driverOutputResourceUri
      _driverControlFilesUri = source._driverControlFilesUri
      _labels = source._labels
      _scheduling = source._scheduling
      _jobUuid = source._jobUuid
      _done = source._done
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularMessageField(value: &_storage._reference) }()
        case 2: try { try decoder.decodeSingularMessageField(value: &_storage._placement) }()
        case 3: try {
          var v: Google_Cloud_Dataproc_V1beta2_HadoopJob?
          if let current = _storage._typeJob {
            try decoder.handleConflictingOneOf()
            if case .hadoopJob(let m) = current {v = m}
          }
          try decoder.decodeSingularMessageField(value: &v)
          if let v = v {_storage._typeJob = .hadoopJob(v)}
        }()
        case 4: try {
          var v: Google_Cloud_Dataproc_V1beta2_SparkJob?
          if let current = _storage._typeJob {
            try decoder.handleConflictingOneOf()
            if case .sparkJob(let m) = current {v = m}
          }
          try decoder.decodeSingularMessageField(value: &v)
          if let v = v {_storage._typeJob = .sparkJob(v)}
        }()
        case 5: try {
          var v: Google_Cloud_Dataproc_V1beta2_PySparkJob?
          if let current = _storage._typeJob {
            try decoder.handleConflictingOneOf()
            if case .pysparkJob(let m) = current {v = m}
          }
          try decoder.decodeSingularMessageField(value: &v)
          if let v = v {_storage._typeJob = .pysparkJob(v)}
        }()
        case 6: try {
          var v: Google_Cloud_Dataproc_V1beta2_HiveJob?
          if let current = _storage._typeJob {
            try decoder.handleConflictingOneOf()
            if case .hiveJob(let m) = current {v = m}
          }
          try decoder.decodeSingularMessageField(value: &v)
          if let v = v {_storage._typeJob = .hiveJob(v)}
        }()
        case 7: try {
          var v: Google_Cloud_Dataproc_V1beta2_PigJob?
          if let current = _storage._typeJob {
            try decoder.handleConflictingOneOf()
            if case .pigJob(let m) = current {v = m}
          }
          try decoder.decodeSingularMessageField(value: &v)
          if let v = v {_storage._typeJob = .pigJob(v)}
        }()
        case 8: try { try decoder.decodeSingularMessageField(value: &_storage._status) }()
        case 9: try { try decoder.decodeRepeatedMessageField(value: &_storage._yarnApplications) }()
        case 10: try { try decoder.decodeSingularStringField(value: &_storage._submittedBy) }()
        case 12: try {
          var v: Google_Cloud_Dataproc_V1beta2_SparkSqlJob?
          if let current = _storage._typeJob {
            try decoder.handleConflictingOneOf()
            if case .sparkSqlJob(let m) = current {v = m}
          }
          try decoder.decodeSingularMessageField(value: &v)
          if let v = v {_storage._typeJob = .sparkSqlJob(v)}
        }()
        case 13: try { try decoder.decodeRepeatedMessageField(value: &_storage._statusHistory) }()
        case 15: try { try decoder.decodeSingularStringField(value: &_storage._driverControlFilesUri) }()
        case 17: try { try decoder.decodeSingularStringField(value: &_storage._driverOutputResourceUri) }()
        case 18: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &_storage._labels) }()
        case 20: try { try decoder.decodeSingularMessageField(value: &_storage._scheduling) }()
        case 21: try {
          var v: Google_Cloud_Dataproc_V1beta2_SparkRJob?
          if let current = _storage._typeJob {
            try decoder.handleConflictingOneOf()
            if case .sparkRJob(let m) = current {v = m}
          }
          try decoder.decodeSingularMessageField(value: &v)
          if let v = v {_storage._typeJob = .sparkRJob(v)}
        }()
        case 22: try { try decoder.decodeSingularStringField(value: &_storage._jobUuid) }()
        case 23: try {
          var v: Google_Cloud_Dataproc_V1beta2_PrestoJob?
          if let current = _storage._typeJob {
            try decoder.handleConflictingOneOf()
            if case .prestoJob(let m) = current {v = m}
          }
          try decoder.decodeSingularMessageField(value: &v)
          if let v = v {_storage._typeJob = .prestoJob(v)}
        }()
        case 24: try { try decoder.decodeSingularBoolField(value: &_storage._done) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if let v = _storage._reference {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      }
      if let v = _storage._placement {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      }
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch _storage._typeJob {
      case .hadoopJob?: try {
        guard case .hadoopJob(let v)? = _storage._typeJob else { preconditionFailure() }
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      }()
      case .sparkJob?: try {
        guard case .sparkJob(let v)? = _storage._typeJob else { preconditionFailure() }
        try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
      }()
      case .pysparkJob?: try {
        guard case .pysparkJob(let v)? = _storage._typeJob else { preconditionFailure() }
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      }()
      case .hiveJob?: try {
        guard case .hiveJob(let v)? = _storage._typeJob else { preconditionFailure() }
        try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
      }()
      case .pigJob?: try {
        guard case .pigJob(let v)? = _storage._typeJob else { preconditionFailure() }
        try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
      }()
      default: break
      }
      if let v = _storage._status {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
      }
      if !_storage._yarnApplications.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._yarnApplications, fieldNumber: 9)
      }
      if !_storage._submittedBy.isEmpty {
        try visitor.visitSingularStringField(value: _storage._submittedBy, fieldNumber: 10)
      }
      if case .sparkSqlJob(let v)? = _storage._typeJob {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 12)
      }
      if !_storage._statusHistory.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._statusHistory, fieldNumber: 13)
      }
      if !_storage._driverControlFilesUri.isEmpty {
        try visitor.visitSingularStringField(value: _storage._driverControlFilesUri, fieldNumber: 15)
      }
      if !_storage._driverOutputResourceUri.isEmpty {
        try visitor.visitSingularStringField(value: _storage._driverOutputResourceUri, fieldNumber: 17)
      }
      if !_storage._labels.isEmpty {
        try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: _storage._labels, fieldNumber: 18)
      }
      if let v = _storage._scheduling {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 20)
      }
      if case .sparkRJob(let v)? = _storage._typeJob {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 21)
      }
      if !_storage._jobUuid.isEmpty {
        try visitor.visitSingularStringField(value: _storage._jobUuid, fieldNumber: 22)
      }
      if case .prestoJob(let v)? = _storage._typeJob {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 23)
      }
      if _storage._done != false {
        try visitor.visitSingularBoolField(value: _storage._done, fieldNumber: 24)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_Job, rhs: Google_Cloud_Dataproc_V1beta2_Job) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._reference != rhs_storage._reference {return false}
        if _storage._placement != rhs_storage._placement {return false}
        if _storage._typeJob != rhs_storage._typeJob {return false}
        if _storage._status != rhs_storage._status {return false}
        if _storage._statusHistory != rhs_storage._statusHistory {return false}
        if _storage._yarnApplications != rhs_storage._yarnApplications {return false}
        if _storage._submittedBy != rhs_storage._submittedBy {return false}
        if _storage._driverOutputResourceUri != rhs_storage._driverOutputResourceUri {return false}
        if _storage._driverControlFilesUri != rhs_storage._driverControlFilesUri {return false}
        if _storage._labels != rhs_storage._labels {return false}
        if _storage._scheduling != rhs_storage._scheduling {return false}
        if _storage._jobUuid != rhs_storage._jobUuid {return false}
        if _storage._done != rhs_storage._done {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_JobScheduling: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".JobScheduling"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "max_failures_per_hour"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularInt32Field(value: &self.maxFailuresPerHour) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.maxFailuresPerHour != 0 {
      try visitor.visitSingularInt32Field(value: self.maxFailuresPerHour, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_JobScheduling, rhs: Google_Cloud_Dataproc_V1beta2_JobScheduling) -> Bool {
    if lhs.maxFailuresPerHour != rhs.maxFailuresPerHour {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_JobMetadata: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".JobMetadata"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "job_id"),
    2: .same(proto: "status"),
    3: .standard(proto: "operation_type"),
    4: .standard(proto: "start_time"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.jobID) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._status) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.operationType) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._startTime) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.jobID.isEmpty {
      try visitor.visitSingularStringField(value: self.jobID, fieldNumber: 1)
    }
    if let v = self._status {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if !self.operationType.isEmpty {
      try visitor.visitSingularStringField(value: self.operationType, fieldNumber: 3)
    }
    if let v = self._startTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_JobMetadata, rhs: Google_Cloud_Dataproc_V1beta2_JobMetadata) -> Bool {
    if lhs.jobID != rhs.jobID {return false}
    if lhs._status != rhs._status {return false}
    if lhs.operationType != rhs.operationType {return false}
    if lhs._startTime != rhs._startTime {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_SubmitJobRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SubmitJobRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    3: .same(proto: "region"),
    2: .same(proto: "job"),
    4: .standard(proto: "request_id"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._job) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.region) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.requestID) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if let v = self._job {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if !self.region.isEmpty {
      try visitor.visitSingularStringField(value: self.region, fieldNumber: 3)
    }
    if !self.requestID.isEmpty {
      try visitor.visitSingularStringField(value: self.requestID, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_SubmitJobRequest, rhs: Google_Cloud_Dataproc_V1beta2_SubmitJobRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.region != rhs.region {return false}
    if lhs._job != rhs._job {return false}
    if lhs.requestID != rhs.requestID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_GetJobRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".GetJobRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    3: .same(proto: "region"),
    2: .standard(proto: "job_id"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.jobID) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.region) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.jobID.isEmpty {
      try visitor.visitSingularStringField(value: self.jobID, fieldNumber: 2)
    }
    if !self.region.isEmpty {
      try visitor.visitSingularStringField(value: self.region, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_GetJobRequest, rhs: Google_Cloud_Dataproc_V1beta2_GetJobRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.region != rhs.region {return false}
    if lhs.jobID != rhs.jobID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ListJobsRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ListJobsRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    6: .same(proto: "region"),
    2: .standard(proto: "page_size"),
    3: .standard(proto: "page_token"),
    4: .standard(proto: "cluster_name"),
    5: .standard(proto: "job_state_matcher"),
    7: .same(proto: "filter"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.pageSize) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.pageToken) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.clusterName) }()
      case 5: try { try decoder.decodeSingularEnumField(value: &self.jobStateMatcher) }()
      case 6: try { try decoder.decodeSingularStringField(value: &self.region) }()
      case 7: try { try decoder.decodeSingularStringField(value: &self.filter) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if self.pageSize != 0 {
      try visitor.visitSingularInt32Field(value: self.pageSize, fieldNumber: 2)
    }
    if !self.pageToken.isEmpty {
      try visitor.visitSingularStringField(value: self.pageToken, fieldNumber: 3)
    }
    if !self.clusterName.isEmpty {
      try visitor.visitSingularStringField(value: self.clusterName, fieldNumber: 4)
    }
    if self.jobStateMatcher != .all {
      try visitor.visitSingularEnumField(value: self.jobStateMatcher, fieldNumber: 5)
    }
    if !self.region.isEmpty {
      try visitor.visitSingularStringField(value: self.region, fieldNumber: 6)
    }
    if !self.filter.isEmpty {
      try visitor.visitSingularStringField(value: self.filter, fieldNumber: 7)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ListJobsRequest, rhs: Google_Cloud_Dataproc_V1beta2_ListJobsRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.region != rhs.region {return false}
    if lhs.pageSize != rhs.pageSize {return false}
    if lhs.pageToken != rhs.pageToken {return false}
    if lhs.clusterName != rhs.clusterName {return false}
    if lhs.jobStateMatcher != rhs.jobStateMatcher {return false}
    if lhs.filter != rhs.filter {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ListJobsRequest.JobStateMatcher: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "ALL"),
    1: .same(proto: "ACTIVE"),
    2: .same(proto: "NON_ACTIVE"),
  ]
}

extension Google_Cloud_Dataproc_V1beta2_UpdateJobRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".UpdateJobRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    2: .same(proto: "region"),
    3: .standard(proto: "job_id"),
    4: .same(proto: "job"),
    5: .standard(proto: "update_mask"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.region) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.jobID) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._job) }()
      case 5: try { try decoder.decodeSingularMessageField(value: &self._updateMask) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.region.isEmpty {
      try visitor.visitSingularStringField(value: self.region, fieldNumber: 2)
    }
    if !self.jobID.isEmpty {
      try visitor.visitSingularStringField(value: self.jobID, fieldNumber: 3)
    }
    if let v = self._job {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    if let v = self._updateMask {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_UpdateJobRequest, rhs: Google_Cloud_Dataproc_V1beta2_UpdateJobRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.region != rhs.region {return false}
    if lhs.jobID != rhs.jobID {return false}
    if lhs._job != rhs._job {return false}
    if lhs._updateMask != rhs._updateMask {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ListJobsResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ListJobsResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "jobs"),
    2: .standard(proto: "next_page_token"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.jobs) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.nextPageToken) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.jobs.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.jobs, fieldNumber: 1)
    }
    if !self.nextPageToken.isEmpty {
      try visitor.visitSingularStringField(value: self.nextPageToken, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ListJobsResponse, rhs: Google_Cloud_Dataproc_V1beta2_ListJobsResponse) -> Bool {
    if lhs.jobs != rhs.jobs {return false}
    if lhs.nextPageToken != rhs.nextPageToken {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_CancelJobRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".CancelJobRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    3: .same(proto: "region"),
    2: .standard(proto: "job_id"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.jobID) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.region) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.jobID.isEmpty {
      try visitor.visitSingularStringField(value: self.jobID, fieldNumber: 2)
    }
    if !self.region.isEmpty {
      try visitor.visitSingularStringField(value: self.region, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_CancelJobRequest, rhs: Google_Cloud_Dataproc_V1beta2_CancelJobRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.region != rhs.region {return false}
    if lhs.jobID != rhs.jobID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_DeleteJobRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".DeleteJobRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    3: .same(proto: "region"),
    2: .standard(proto: "job_id"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.jobID) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.region) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.jobID.isEmpty {
      try visitor.visitSingularStringField(value: self.jobID, fieldNumber: 2)
    }
    if !self.region.isEmpty {
      try visitor.visitSingularStringField(value: self.region, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_DeleteJobRequest, rhs: Google_Cloud_Dataproc_V1beta2_DeleteJobRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.region != rhs.region {return false}
    if lhs.jobID != rhs.jobID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
