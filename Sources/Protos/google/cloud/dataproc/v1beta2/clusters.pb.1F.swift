// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/dataproc/v1beta2/clusters.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2020 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Describes the identifying information, config, and status of
/// a cluster of Compute Engine instances.
public struct Google_Cloud_Dataproc_V1beta2_Cluster {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The Google Cloud Platform project ID that the cluster belongs to.
  public var projectID: String = String()

  /// Required. The cluster name. Cluster names within a project must be
  /// unique. Names of deleted clusters can be reused.
  public var clusterName: String = String()

  /// Required. The cluster config. Note that Dataproc may set
  /// default values, and values may change when clusters are updated.
  public var config: Google_Cloud_Dataproc_V1beta2_ClusterConfig {
    get {return _config ?? Google_Cloud_Dataproc_V1beta2_ClusterConfig()}
    set {_config = newValue}
  }
  /// Returns true if `config` has been explicitly set.
  public var hasConfig: Bool {return self._config != nil}
  /// Clears the value of `config`. Subsequent reads from it will return its default value.
  public mutating func clearConfig() {self._config = nil}

  /// Optional. The labels to associate with this cluster.
  /// Label **keys** must contain 1 to 63 characters, and must conform to
  /// [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
  /// Label **values** may be empty, but, if present, must contain 1 to 63
  /// characters, and must conform to [RFC
  /// 1035](https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be
  /// associated with a cluster.
  public var labels: Dictionary<String,String> = [:]

  /// Output only. Cluster status.
  public var status: Google_Cloud_Dataproc_V1beta2_ClusterStatus {
    get {return _status ?? Google_Cloud_Dataproc_V1beta2_ClusterStatus()}
    set {_status = newValue}
  }
  /// Returns true if `status` has been explicitly set.
  public var hasStatus: Bool {return self._status != nil}
  /// Clears the value of `status`. Subsequent reads from it will return its default value.
  public mutating func clearStatus() {self._status = nil}

  /// Output only. The previous cluster status.
  public var statusHistory: [Google_Cloud_Dataproc_V1beta2_ClusterStatus] = []

  /// Output only. A cluster UUID (Unique Universal Identifier). Dataproc
  /// generates this value when it creates the cluster.
  public var clusterUuid: String = String()

  /// Output only. Contains cluster daemon metrics such as HDFS and YARN stats.
  ///
  /// **Beta Feature**: This report is available for testing purposes only. It
  /// may be changed before final release.
  public var metrics: Google_Cloud_Dataproc_V1beta2_ClusterMetrics {
    get {return _metrics ?? Google_Cloud_Dataproc_V1beta2_ClusterMetrics()}
    set {_metrics = newValue}
  }
  /// Returns true if `metrics` has been explicitly set.
  public var hasMetrics: Bool {return self._metrics != nil}
  /// Clears the value of `metrics`. Subsequent reads from it will return its default value.
  public mutating func clearMetrics() {self._metrics = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _config: Google_Cloud_Dataproc_V1beta2_ClusterConfig? = nil
  fileprivate var _status: Google_Cloud_Dataproc_V1beta2_ClusterStatus? = nil
  fileprivate var _metrics: Google_Cloud_Dataproc_V1beta2_ClusterMetrics? = nil
}

/// The cluster config.
public struct Google_Cloud_Dataproc_V1beta2_ClusterConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. A Cloud Storage bucket used to stage job
  /// dependencies, config files, and job driver console output.
  /// If you do not specify a staging bucket, Cloud
  /// Dataproc will determine a Cloud Storage location (US,
  /// ASIA, or EU) for your cluster's staging bucket according to the
  /// Compute Engine zone where your cluster is deployed, and then create
  /// and manage this project-level, per-location bucket (see
  /// [Dataproc staging
  /// bucket](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
  public var configBucket: String {
    get {return _storage._configBucket}
    set {_uniqueStorage()._configBucket = newValue}
  }

  /// Optional. The shared Compute Engine config settings for
  /// all instances in a cluster.
  public var gceClusterConfig: Google_Cloud_Dataproc_V1beta2_GceClusterConfig {
    get {return _storage._gceClusterConfig ?? Google_Cloud_Dataproc_V1beta2_GceClusterConfig()}
    set {_uniqueStorage()._gceClusterConfig = newValue}
  }
  /// Returns true if `gceClusterConfig` has been explicitly set.
  public var hasGceClusterConfig: Bool {return _storage._gceClusterConfig != nil}
  /// Clears the value of `gceClusterConfig`. Subsequent reads from it will return its default value.
  public mutating func clearGceClusterConfig() {_uniqueStorage()._gceClusterConfig = nil}

  /// Optional. The Compute Engine config settings for
  /// the master instance in a cluster.
  public var masterConfig: Google_Cloud_Dataproc_V1beta2_InstanceGroupConfig {
    get {return _storage._masterConfig ?? Google_Cloud_Dataproc_V1beta2_InstanceGroupConfig()}
    set {_uniqueStorage()._masterConfig = newValue}
  }
  /// Returns true if `masterConfig` has been explicitly set.
  public var hasMasterConfig: Bool {return _storage._masterConfig != nil}
  /// Clears the value of `masterConfig`. Subsequent reads from it will return its default value.
  public mutating func clearMasterConfig() {_uniqueStorage()._masterConfig = nil}

  /// Optional. The Compute Engine config settings for
  /// worker instances in a cluster.
  public var workerConfig: Google_Cloud_Dataproc_V1beta2_InstanceGroupConfig {
    get {return _storage._workerConfig ?? Google_Cloud_Dataproc_V1beta2_InstanceGroupConfig()}
    set {_uniqueStorage()._workerConfig = newValue}
  }
  /// Returns true if `workerConfig` has been explicitly set.
  public var hasWorkerConfig: Bool {return _storage._workerConfig != nil}
  /// Clears the value of `workerConfig`. Subsequent reads from it will return its default value.
  public mutating func clearWorkerConfig() {_uniqueStorage()._workerConfig = nil}

  /// Optional. The Compute Engine config settings for
  /// additional worker instances in a cluster.
  public var secondaryWorkerConfig: Google_Cloud_Dataproc_V1beta2_InstanceGroupConfig {
    get {return _storage._secondaryWorkerConfig ?? Google_Cloud_Dataproc_V1beta2_InstanceGroupConfig()}
    set {_uniqueStorage()._secondaryWorkerConfig = newValue}
  }
  /// Returns true if `secondaryWorkerConfig` has been explicitly set.
  public var hasSecondaryWorkerConfig: Bool {return _storage._secondaryWorkerConfig != nil}
  /// Clears the value of `secondaryWorkerConfig`. Subsequent reads from it will return its default value.
  public mutating func clearSecondaryWorkerConfig() {_uniqueStorage()._secondaryWorkerConfig = nil}

  /// Optional. The config settings for software inside the cluster.
  public var softwareConfig: Google_Cloud_Dataproc_V1beta2_SoftwareConfig {
    get {return _storage._softwareConfig ?? Google_Cloud_Dataproc_V1beta2_SoftwareConfig()}
    set {_uniqueStorage()._softwareConfig = newValue}
  }
  /// Returns true if `softwareConfig` has been explicitly set.
  public var hasSoftwareConfig: Bool {return _storage._softwareConfig != nil}
  /// Clears the value of `softwareConfig`. Subsequent reads from it will return its default value.
  public mutating func clearSoftwareConfig() {_uniqueStorage()._softwareConfig = nil}

  /// Optional. The config setting for auto delete cluster schedule.
  public var lifecycleConfig: Google_Cloud_Dataproc_V1beta2_LifecycleConfig {
    get {return _storage._lifecycleConfig ?? Google_Cloud_Dataproc_V1beta2_LifecycleConfig()}
    set {_uniqueStorage()._lifecycleConfig = newValue}
  }
  /// Returns true if `lifecycleConfig` has been explicitly set.
  public var hasLifecycleConfig: Bool {return _storage._lifecycleConfig != nil}
  /// Clears the value of `lifecycleConfig`. Subsequent reads from it will return its default value.
  public mutating func clearLifecycleConfig() {_uniqueStorage()._lifecycleConfig = nil}

  /// Optional. Commands to execute on each node after config is
  /// completed. By default, executables are run on master and all worker nodes.
  /// You can test a node's <code>role</code> metadata to run an executable on
  /// a master or worker node, as shown below using `curl` (you can also use
  /// `wget`):
  ///
  ///     ROLE=$(curl -H Metadata-Flavor:Google
  ///     http://metadata/computeMetadata/v1beta2/instance/attributes/dataproc-role)
  ///     if [[ "${ROLE}" == 'Master' ]]; then
  ///       ... master specific actions ...
  ///     else
  ///       ... worker specific actions ...
  ///     fi
  public var initializationActions: [Google_Cloud_Dataproc_V1beta2_NodeInitializationAction] {
    get {return _storage._initializationActions}
    set {_uniqueStorage()._initializationActions = newValue}
  }

  /// Optional. Encryption settings for the cluster.
  public var encryptionConfig: Google_Cloud_Dataproc_V1beta2_EncryptionConfig {
    get {return _storage._encryptionConfig ?? Google_Cloud_Dataproc_V1beta2_EncryptionConfig()}
    set {_uniqueStorage()._encryptionConfig = newValue}
  }
  /// Returns true if `encryptionConfig` has been explicitly set.
  public var hasEncryptionConfig: Bool {return _storage._encryptionConfig != nil}
  /// Clears the value of `encryptionConfig`. Subsequent reads from it will return its default value.
  public mutating func clearEncryptionConfig() {_uniqueStorage()._encryptionConfig = nil}

  /// Optional. Autoscaling config for the policy associated with the cluster.
  /// Cluster does not autoscale if this field is unset.
  public var autoscalingConfig: Google_Cloud_Dataproc_V1beta2_AutoscalingConfig {
    get {return _storage._autoscalingConfig ?? Google_Cloud_Dataproc_V1beta2_AutoscalingConfig()}
    set {_uniqueStorage()._autoscalingConfig = newValue}
  }
  /// Returns true if `autoscalingConfig` has been explicitly set.
  public var hasAutoscalingConfig: Bool {return _storage._autoscalingConfig != nil}
  /// Clears the value of `autoscalingConfig`. Subsequent reads from it will return its default value.
  public mutating func clearAutoscalingConfig() {_uniqueStorage()._autoscalingConfig = nil}

  /// Optional. Port/endpoint configuration for this cluster
  public var endpointConfig: Google_Cloud_Dataproc_V1beta2_EndpointConfig {
    get {return _storage._endpointConfig ?? Google_Cloud_Dataproc_V1beta2_EndpointConfig()}
    set {_uniqueStorage()._endpointConfig = newValue}
  }
  /// Returns true if `endpointConfig` has been explicitly set.
  public var hasEndpointConfig: Bool {return _storage._endpointConfig != nil}
  /// Clears the value of `endpointConfig`. Subsequent reads from it will return its default value.
  public mutating func clearEndpointConfig() {_uniqueStorage()._endpointConfig = nil}

  /// Optional. Security related configuration.
  public var securityConfig: Google_Cloud_Dataproc_V1beta2_SecurityConfig {
    get {return _storage._securityConfig ?? Google_Cloud_Dataproc_V1beta2_SecurityConfig()}
    set {_uniqueStorage()._securityConfig = newValue}
  }
  /// Returns true if `securityConfig` has been explicitly set.
  public var hasSecurityConfig: Bool {return _storage._securityConfig != nil}
  /// Clears the value of `securityConfig`. Subsequent reads from it will return its default value.
  public mutating func clearSecurityConfig() {_uniqueStorage()._securityConfig = nil}

  /// Optional. The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes.
  /// Setting this is considered mutually exclusive with Compute Engine-based
  /// options such as `gce_cluster_config`, `master_config`, `worker_config`,
  /// `secondary_worker_config`, and `autoscaling_config`.
  public var gkeClusterConfig: Google_Cloud_Dataproc_V1beta2_GkeClusterConfig {
    get {return _storage._gkeClusterConfig ?? Google_Cloud_Dataproc_V1beta2_GkeClusterConfig()}
    set {_uniqueStorage()._gkeClusterConfig = newValue}
  }
  /// Returns true if `gkeClusterConfig` has been explicitly set.
  public var hasGkeClusterConfig: Bool {return _storage._gkeClusterConfig != nil}
  /// Clears the value of `gkeClusterConfig`. Subsequent reads from it will return its default value.
  public mutating func clearGkeClusterConfig() {_uniqueStorage()._gkeClusterConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// The GKE config for this cluster.
public struct Google_Cloud_Dataproc_V1beta2_GkeClusterConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. A target for the deployment.
  public var namespacedGkeDeploymentTarget: Google_Cloud_Dataproc_V1beta2_GkeClusterConfig.NamespacedGkeDeploymentTarget {
    get {return _namespacedGkeDeploymentTarget ?? Google_Cloud_Dataproc_V1beta2_GkeClusterConfig.NamespacedGkeDeploymentTarget()}
    set {_namespacedGkeDeploymentTarget = newValue}
  }
  /// Returns true if `namespacedGkeDeploymentTarget` has been explicitly set.
  public var hasNamespacedGkeDeploymentTarget: Bool {return self._namespacedGkeDeploymentTarget != nil}
  /// Clears the value of `namespacedGkeDeploymentTarget`. Subsequent reads from it will return its default value.
  public mutating func clearNamespacedGkeDeploymentTarget() {self._namespacedGkeDeploymentTarget = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// A full, namespace-isolated deployment target for an existing GKE cluster.
  public struct NamespacedGkeDeploymentTarget {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Optional. The target GKE cluster to deploy to.
    /// Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
    public var targetGkeCluster: String = String()

    /// Optional. A namespace within the GKE cluster to deploy into.
    public var clusterNamespace: String = String()

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    public init() {}
  }

  public init() {}

  fileprivate var _namespacedGkeDeploymentTarget: Google_Cloud_Dataproc_V1beta2_GkeClusterConfig.NamespacedGkeDeploymentTarget? = nil
}

/// Endpoint config for this cluster
public struct Google_Cloud_Dataproc_V1beta2_EndpointConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The map of port descriptions to URLs. Will only be populated
  /// if enable_http_port_access is true.
  public var httpPorts: Dictionary<String,String> = [:]

  /// Optional. If true, enable http access to specific ports on the cluster
  /// from external sources. Defaults to false.
  public var enableHTTPPortAccess: Bool = false

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Autoscaling Policy config associated with the cluster.
public struct Google_Cloud_Dataproc_V1beta2_AutoscalingConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. The autoscaling policy used by the cluster.
  ///
  /// Only resource names including projectid and location (region) are valid.
  /// Examples:
  ///
  /// * `https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`
  /// * `projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`
  ///
  /// Note that the policy must be in the same project and Dataproc region.
  public var policyUri: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Encryption settings for the cluster.
public struct Google_Cloud_Dataproc_V1beta2_EncryptionConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. The Cloud KMS key name to use for PD disk encryption for all
  /// instances in the cluster.
  public var gcePdKmsKeyName: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Common config settings for resources of Compute Engine cluster
/// instances, applicable to all instances in the cluster.
public struct Google_Cloud_Dataproc_V1beta2_GceClusterConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. The zone where the Compute Engine cluster will be located.
  /// On a create request, it is required in the "global" region. If omitted
  /// in a non-global Dataproc region, the service will pick a zone in the
  /// corresponding Compute Engine region. On a get request, zone will always be
  /// present.
  ///
  /// A full URL, partial URI, or short name are valid. Examples:
  ///
  /// * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]`
  /// * `projects/[project_id]/zones/[zone]`
  /// * `us-central1-f`
  public var zoneUri: String = String()

  /// Optional. The Compute Engine network to be used for machine
  /// communications. Cannot be specified with subnetwork_uri. If neither
  /// `network_uri` nor `subnetwork_uri` is specified, the "default" network of
  /// the project is used, if it exists. Cannot be a "Custom Subnet Network" (see
  /// [Using Subnetworks](https://cloud.google.com/compute/docs/subnetworks) for
  /// more information).
  ///
  /// A full URL, partial URI, or short name are valid. Examples:
  ///
  /// * `https://www.googleapis.com/compute/v1/projects/[project_id]/regions/global/default`
  /// * `projects/[project_id]/regions/global/default`
  /// * `default`
  public var networkUri: String = String()

  /// Optional. The Compute Engine subnetwork to be used for machine
  /// communications. Cannot be specified with network_uri.
  ///
  /// A full URL, partial URI, or short name are valid. Examples:
  ///
  /// * `https://www.googleapis.com/compute/v1/projects/[project_id]/regions/us-east1/subnetworks/sub0`
  /// * `projects/[project_id]/regions/us-east1/subnetworks/sub0`
  /// * `sub0`
  public var subnetworkUri: String = String()

  /// Optional. If true, all instances in the cluster will only have internal IP
  /// addresses. By default, clusters are not restricted to internal IP
  /// addresses, and will have ephemeral external IP addresses assigned to each
  /// instance. This `internal_ip_only` restriction can only be enabled for
  /// subnetwork enabled networks, and all off-cluster dependencies must be
  /// configured to be accessible without external IP addresses.
  public var internalIpOnly: Bool = false

  /// Optional. The [Dataproc service
  /// account](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_cloud_dataproc)
  /// (also see [VM Data Plane
  /// identity](https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity))
  /// used by Dataproc cluster VM instances to access Google Cloud Platform
  /// services.
  ///
  /// If not specified, the
  /// [Compute Engine default service
  /// account](https://cloud.google.com/compute/docs/access/service-accounts#default_service_account)
  /// is used.
  public var serviceAccount: String = String()

  /// Optional. The URIs of service account scopes to be included in
  /// Compute Engine instances. The following base set of scopes is always
  /// included:
  ///
  /// * https://www.googleapis.com/auth/cloud.useraccounts.readonly
  /// * https://www.googleapis.com/auth/devstorage.read_write
  /// * https://www.googleapis.com/auth/logging.write
  ///
  /// If no scopes are specified, the following defaults are also provided:
  ///
  /// * https://www.googleapis.com/auth/bigquery
  /// * https://www.googleapis.com/auth/bigtable.admin.table
  /// * https://www.googleapis.com/auth/bigtable.data
  /// * https://www.googleapis.com/auth/devstorage.full_control
  public var serviceAccountScopes: [String] = []

  /// The Compute Engine tags to add to all instances (see [Tagging
  /// instances](https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
  public var tags: [String] = []

  /// The Compute Engine metadata entries to add to all instances (see
  /// [Project and instance
  /// metadata](https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
  public var metadata: Dictionary<String,String> = [:]

  /// Optional. Reservation Affinity for consuming Zonal reservation.
  public var reservationAffinity: Google_Cloud_Dataproc_V1beta2_ReservationAffinity {
    get {return _reservationAffinity ?? Google_Cloud_Dataproc_V1beta2_ReservationAffinity()}
    set {_reservationAffinity = newValue}
  }
  /// Returns true if `reservationAffinity` has been explicitly set.
  public var hasReservationAffinity: Bool {return self._reservationAffinity != nil}
  /// Clears the value of `reservationAffinity`. Subsequent reads from it will return its default value.
  public mutating func clearReservationAffinity() {self._reservationAffinity = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _reservationAffinity: Google_Cloud_Dataproc_V1beta2_ReservationAffinity? = nil
}

/// The config settings for Compute Engine resources in
/// an instance group, such as a master or worker group.
public struct Google_Cloud_Dataproc_V1beta2_InstanceGroupConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. The number of VM instances in the instance group.
  /// For master instance groups, must be set to 1.
  public var numInstances: Int32 = 0

  /// Output only. The list of instance names. Dataproc derives the names
  /// from `cluster_name`, `num_instances`, and the instance group.
  public var instanceNames: [String] = []

  /// Optional. The Compute Engine image resource used for cluster instances.
  ///
  /// The URI can represent an image or image family.
  ///
  /// Image examples:
  ///
  /// * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id]`
  /// * `projects/[project_id]/global/images/[image-id]`
  /// * `image-id`
  ///
  /// Image family examples. Dataproc will use the most recent
  /// image from the family:
  ///
  /// * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name]`
  /// * `projects/[project_id]/global/images/family/[custom-image-family-name]`
  ///
  /// If the URI is unspecified, it will be inferred from
  /// `SoftwareConfig.image_version` or the system default.
  public var imageUri: String = String()

  /// Optional. The Compute Engine machine type used for cluster instances.
  ///
  /// A full URL, partial URI, or short name are valid. Examples:
  ///
  /// * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2`
  /// * `projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2`
  /// * `n1-standard-2`
  ///
  /// **Auto Zone Exception**: If you are using the Dataproc
  /// [Auto Zone
  /// Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
  /// feature, you must use the short name of the machine type
  /// resource, for example, `n1-standard-2`.
  public var machineTypeUri: String = String()

  /// Optional. Disk option config settings.
  public var diskConfig: Google_Cloud_Dataproc_V1beta2_DiskConfig {
    get {return _diskConfig ?? Google_Cloud_Dataproc_V1beta2_DiskConfig()}
    set {_diskConfig = newValue}
  }
  /// Returns true if `diskConfig` has been explicitly set.
  public var hasDiskConfig: Bool {return self._diskConfig != nil}
  /// Clears the value of `diskConfig`. Subsequent reads from it will return its default value.
  public mutating func clearDiskConfig() {self._diskConfig = nil}

  /// Output only. Specifies that this instance group contains preemptible
  /// instances.
  public var isPreemptible: Bool = false

  /// Output only. The config for Compute Engine Instance Group
  /// Manager that manages this group.
  /// This is only used for preemptible instance groups.
  public var managedGroupConfig: Google_Cloud_Dataproc_V1beta2_ManagedGroupConfig {
    get {return _managedGroupConfig ?? Google_Cloud_Dataproc_V1beta2_ManagedGroupConfig()}
    set {_managedGroupConfig = newValue}
  }
  /// Returns true if `managedGroupConfig` has been explicitly set.
  public var hasManagedGroupConfig: Bool {return self._managedGroupConfig != nil}
  /// Clears the value of `managedGroupConfig`. Subsequent reads from it will return its default value.
  public mutating func clearManagedGroupConfig() {self._managedGroupConfig = nil}

  /// Optional. The Compute Engine accelerator configuration for these
  /// instances.
  public var accelerators: [Google_Cloud_Dataproc_V1beta2_AcceleratorConfig] = []

  /// Specifies the minimum cpu platform for the Instance Group.
  /// See [Dataproc -> Minimum CPU
  /// Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
  public var minCpuPlatform: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _diskConfig: Google_Cloud_Dataproc_V1beta2_DiskConfig? = nil
  fileprivate var _managedGroupConfig: Google_Cloud_Dataproc_V1beta2_ManagedGroupConfig? = nil
}

/// Specifies the resources used to actively manage an instance group.
public struct Google_Cloud_Dataproc_V1beta2_ManagedGroupConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The name of the Instance Template used for the Managed
  /// Instance Group.
  public var instanceTemplateName: String = String()

  /// Output only. The name of the Instance Group Manager for this group.
  public var instanceGroupManagerName: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Specifies the type and number of accelerator cards attached to the instances
/// of an instance group (see [GPUs on Compute
/// Engine](https://cloud.google.com/compute/docs/gpus/)).
public struct Google_Cloud_Dataproc_V1beta2_AcceleratorConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Full URL, partial URI, or short name of the accelerator type resource to
  /// expose to this instance. See
  /// [Compute Engine
  /// AcceleratorTypes](https://cloud.google.com/compute/docs/reference/beta/acceleratorTypes)
  ///
  /// Examples
  /// * `https://www.googleapis.com/compute/beta/projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80`
  /// * `projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80`
  /// * `nvidia-tesla-k80`
  ///
  /// **Auto Zone Exception**: If you are using the Dataproc
  /// [Auto Zone
  /// Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
  /// feature, you must use the short name of the accelerator type
  /// resource, for example, `nvidia-tesla-k80`.
  public var acceleratorTypeUri: String = String()

  /// The number of the accelerator cards of this type exposed to this instance.
  public var acceleratorCount: Int32 = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Specifies the config of disk options for a group of VM instances.
public struct Google_Cloud_Dataproc_V1beta2_DiskConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. Type of the boot disk (default is "pd-standard").
  /// Valid values: "pd-ssd" (Persistent Disk Solid State Drive) or
  /// "pd-standard" (Persistent Disk Hard Disk Drive).
  public var bootDiskType: String = String()

  /// Optional. Size in GB of the boot disk (default is 500GB).
  public var bootDiskSizeGb: Int32 = 0

  /// Number of attached SSDs, from 0 to 4 (default is 0).
  /// If SSDs are not attached, the boot disk is used to store runtime logs and
  /// [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data.
  /// If one or more SSDs are attached, this runtime bulk
  /// data is spread across them, and the boot disk contains only basic
  /// config and installed binaries.
  public var numLocalSsds: Int32 = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Specifies the cluster auto-delete schedule configuration.
public struct Google_Cloud_Dataproc_V1beta2_LifecycleConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. The duration to keep the cluster alive while idling (when no jobs
  /// are running). Passing this threshold will cause the cluster to be
  /// deleted. Minimum value is 10 minutes; maximum value is 14 days (see JSON
  /// representation of
  /// [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json).
  public var idleDeleteTtl: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _idleDeleteTtl ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_idleDeleteTtl = newValue}
  }
  /// Returns true if `idleDeleteTtl` has been explicitly set.
  public var hasIdleDeleteTtl: Bool {return self._idleDeleteTtl != nil}
  /// Clears the value of `idleDeleteTtl`. Subsequent reads from it will return its default value.
  public mutating func clearIdleDeleteTtl() {self._idleDeleteTtl = nil}

  /// Either the exact time the cluster should be deleted at or
  /// the cluster maximum age.
  public var ttl: Google_Cloud_Dataproc_V1beta2_LifecycleConfig.OneOf_Ttl? = nil

  /// Optional. The time when cluster will be auto-deleted. (see JSON representation of
  /// [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).
  public var autoDeleteTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {
      if case .autoDeleteTime(let v)? = ttl {return v}
      return SwiftProtobuf.Google_Protobuf_Timestamp()
    }
    set {ttl = .autoDeleteTime(newValue)}
  }

  /// Optional. The lifetime duration of cluster. The cluster will be
  /// auto-deleted at the end of this period. Minimum value is 10 minutes;
  /// maximum value is 14 days (see JSON representation of
  /// [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
  public var autoDeleteTtl: SwiftProtobuf.Google_Protobuf_Duration {
    get {
      if case .autoDeleteTtl(let v)? = ttl {return v}
      return SwiftProtobuf.Google_Protobuf_Duration()
    }
    set {ttl = .autoDeleteTtl(newValue)}
  }

  /// Output only. The time when cluster became idle (most recent job finished)
  /// and became eligible for deletion due to idleness (see JSON representation
  /// of
  /// [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).
  public var idleStartTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _idleStartTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_idleStartTime = newValue}
  }
  /// Returns true if `idleStartTime` has been explicitly set.
  public var hasIdleStartTime: Bool {return self._idleStartTime != nil}
  /// Clears the value of `idleStartTime`. Subsequent reads from it will return its default value.
  public mutating func clearIdleStartTime() {self._idleStartTime = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Either the exact time the cluster should be deleted at or
  /// the cluster maximum age.
  public enum OneOf_Ttl: Equatable {
    /// Optional. The time when cluster will be auto-deleted. (see JSON representation of
    /// [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).
    case autoDeleteTime(SwiftProtobuf.Google_Protobuf_Timestamp)
    /// Optional. The lifetime duration of cluster. The cluster will be
    /// auto-deleted at the end of this period. Minimum value is 10 minutes;
    /// maximum value is 14 days (see JSON representation of
    /// [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
    case autoDeleteTtl(SwiftProtobuf.Google_Protobuf_Duration)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_LifecycleConfig.OneOf_Ttl, rhs: Google_Cloud_Dataproc_V1beta2_LifecycleConfig.OneOf_Ttl) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.autoDeleteTime, .autoDeleteTime): return {
        guard case .autoDeleteTime(let l) = lhs, case .autoDeleteTime(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.autoDeleteTtl, .autoDeleteTtl): return {
        guard case .autoDeleteTtl(let l) = lhs, case .autoDeleteTtl(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}

  fileprivate var _idleDeleteTtl: SwiftProtobuf.Google_Protobuf_Duration? = nil
  fileprivate var _idleStartTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
}

/// Security related configuration, including encryption, Kerberos, etc.
public struct Google_Cloud_Dataproc_V1beta2_SecurityConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Kerberos related configuration.
  public var kerberosConfig: Google_Cloud_Dataproc_V1beta2_KerberosConfig {
    get {return _kerberosConfig ?? Google_Cloud_Dataproc_V1beta2_KerberosConfig()}
    set {_kerberosConfig = newValue}
  }
  /// Returns true if `kerberosConfig` has been explicitly set.
  public var hasKerberosConfig: Bool {return self._kerberosConfig != nil}
  /// Clears the value of `kerberosConfig`. Subsequent reads from it will return its default value.
  public mutating func clearKerberosConfig() {self._kerberosConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _kerberosConfig: Google_Cloud_Dataproc_V1beta2_KerberosConfig? = nil
}

/// Specifies Kerberos related configuration.
public struct Google_Cloud_Dataproc_V1beta2_KerberosConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set
  /// this field to true to enable Kerberos on a cluster.
  public var enableKerberos: Bool = false

  /// Required. The Cloud Storage URI of a KMS encrypted file containing the root
  /// principal password.
  public var rootPrincipalPasswordUri: String = String()

  /// Required. The uri of the KMS key used to encrypt various sensitive
  /// files.
  public var kmsKeyUri: String = String()

  /// Optional. The Cloud Storage URI of the keystore file used for SSL
  /// encryption. If not provided, Dataproc will provide a self-signed
  /// certificate.
  public var keystoreUri: String = String()

  /// Optional. The Cloud Storage URI of the truststore file used for SSL
  /// encryption. If not provided, Dataproc will provide a self-signed
  /// certificate.
  public var truststoreUri: String = String()

  /// Optional. The Cloud Storage URI of a KMS encrypted file containing the
  /// password to the user provided keystore. For the self-signed certificate,
  /// this password is generated by Dataproc.
  public var keystorePasswordUri: String = String()

  /// Optional. The Cloud Storage URI of a KMS encrypted file containing the
  /// password to the user provided key. For the self-signed certificate, this
  /// password is generated by Dataproc.
  public var keyPasswordUri: String = String()

  /// Optional. The Cloud Storage URI of a KMS encrypted file containing the
  /// password to the user provided truststore. For the self-signed certificate,
  /// this password is generated by Dataproc.
  public var truststorePasswordUri: String = String()

  /// Optional. The remote realm the Dataproc on-cluster KDC will trust, should
  /// the user enable cross realm trust.
  public var crossRealmTrustRealm: String = String()

  /// Optional. The KDC (IP or hostname) for the remote trusted realm in a cross
  /// realm trust relationship.
  public var crossRealmTrustKdc: String = String()

  /// Optional. The admin server (IP or hostname) for the remote trusted realm in
  /// a cross realm trust relationship.
  public var crossRealmTrustAdminServer: String = String()

  /// Optional. The Cloud Storage URI of a KMS encrypted file containing the
  /// shared password between the on-cluster Kerberos realm and the remote
  /// trusted realm, in a cross realm trust relationship.
  public var crossRealmTrustSharedPasswordUri: String = String()

  /// Optional. The Cloud Storage URI of a KMS encrypted file containing the
  /// master key of the KDC database.
  public var kdcDbKeyUri: String = String()

  /// Optional. The lifetime of the ticket granting ticket, in hours.
  /// If not specified, or user specifies 0, then default value 10
  /// will be used.
  public var tgtLifetimeHours: Int32 = 0

  /// Optional. The name of the on-cluster Kerberos realm.
  /// If not specified, the uppercased domain of hostnames will be the realm.
  public var realm: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Specifies an executable to run on a fully configured node and a
/// timeout period for executable completion.
public struct Google_Cloud_Dataproc_V1beta2_NodeInitializationAction {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Cloud Storage URI of executable file.
  public var executableFile: String = String()

  /// Optional. Amount of time executable has to complete. Default is
  /// 10 minutes (see JSON representation of
  /// [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
  ///
  /// Cluster creation fails with an explanatory error message (the
  /// name of the executable that caused the error and the exceeded timeout
  /// period) if the executable is not completed at end of the timeout period.
  public var executionTimeout: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _executionTimeout ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_executionTimeout = newValue}
  }
  /// Returns true if `executionTimeout` has been explicitly set.
  public var hasExecutionTimeout: Bool {return self._executionTimeout != nil}
  /// Clears the value of `executionTimeout`. Subsequent reads from it will return its default value.
  public mutating func clearExecutionTimeout() {self._executionTimeout = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _executionTimeout: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// The status of a cluster and its instances.
public struct Google_Cloud_Dataproc_V1beta2_ClusterStatus {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The cluster's state.
  public var state: Google_Cloud_Dataproc_V1beta2_ClusterStatus.State = .unknown

  /// Output only. Optional details of cluster's state.
  public var detail: String = String()

  /// Output only. Time when this state was entered (see JSON representation of
  /// [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).
  public var stateStartTime: SwiftProtobuf.Google_Protobuf_Timestamp {
    get {return _stateStartTime ?? SwiftProtobuf.Google_Protobuf_Timestamp()}
    set {_stateStartTime = newValue}
  }
  /// Returns true if `stateStartTime` has been explicitly set.
  public var hasStateStartTime: Bool {return self._stateStartTime != nil}
  /// Clears the value of `stateStartTime`. Subsequent reads from it will return its default value.
  public mutating func clearStateStartTime() {self._stateStartTime = nil}

  /// Output only. Additional state information that includes
  /// status reported by the agent.
  public var substate: Google_Cloud_Dataproc_V1beta2_ClusterStatus.Substate = .unspecified

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The cluster state.
  public enum State: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// The cluster state is unknown.
    case unknown // = 0

    /// The cluster is being created and set up. It is not ready for use.
    case creating // = 1

    /// The cluster is currently running and healthy. It is ready for use.
    case running // = 2

    /// The cluster encountered an error. It is not ready for use.
    case error // = 3

    /// The cluster is being deleted. It cannot be used.
    case deleting // = 4

    /// The cluster is being updated. It continues to accept and process jobs.
    case updating // = 5

    /// The cluster is being stopped. It cannot be used.
    case stopping // = 6

    /// The cluster is currently stopped. It is not ready for use.
    case stopped // = 7

    /// The cluster is being started. It is not ready for use.
    case starting // = 8
    case UNRECOGNIZED(Int)

    public init() {
      self = .unknown
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unknown
      case 1: self = .creating
      case 2: self = .running
      case 3: self = .error
      case 4: self = .deleting
      case 5: self = .updating
      case 6: self = .stopping
      case 7: self = .stopped
      case 8: self = .starting
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unknown: return 0
      case .creating: return 1
      case .running: return 2
      case .error: return 3
      case .deleting: return 4
      case .updating: return 5
      case .stopping: return 6
      case .stopped: return 7
      case .starting: return 8
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  /// The cluster substate.
  public enum Substate: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// The cluster substate is unknown.
    case unspecified // = 0

    /// The cluster is known to be in an unhealthy state
    /// (for example, critical daemons are not running or HDFS capacity is
    /// exhausted).
    ///
    /// Applies to RUNNING state.
    case unhealthy // = 1

    /// The agent-reported status is out of date (may occur if
    /// Dataproc loses communication with Agent).
    ///
    /// Applies to RUNNING state.
    case staleStatus // = 2
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .unhealthy
      case 2: self = .staleStatus
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .unhealthy: return 1
      case .staleStatus: return 2
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}

  fileprivate var _stateStartTime: SwiftProtobuf.Google_Protobuf_Timestamp? = nil
}

#if swift(>=4.2)

extension Google_Cloud_Dataproc_V1beta2_ClusterStatus.State: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dataproc_V1beta2_ClusterStatus.State] = [
    .unknown,
    .creating,
    .running,
    .error,
    .deleting,
    .updating,
    .stopping,
    .stopped,
    .starting,
  ]
}

extension Google_Cloud_Dataproc_V1beta2_ClusterStatus.Substate: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dataproc_V1beta2_ClusterStatus.Substate] = [
    .unspecified,
    .unhealthy,
    .staleStatus,
  ]
}

#endif  // swift(>=4.2)

/// Specifies the selection and config of software inside the cluster.
public struct Google_Cloud_Dataproc_V1beta2_SoftwareConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. The version of software inside the cluster. It must be one of the
  /// supported [Dataproc
  /// Versions](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_cloud_dataproc_versions),
  /// such as "1.2" (including a subminor version, such as "1.2.29"), or the
  /// ["preview"
  /// version](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions).
  /// If unspecified, it defaults to the latest Debian version.
  public var imageVersion: String = String()

  /// Optional. The properties to set on daemon config files.
  ///
  /// Property keys are specified in `prefix:property` format, for example
  /// `core:hadoop.tmp.dir`. The following are supported prefixes
  /// and their mappings:
  ///
  /// * capacity-scheduler: `capacity-scheduler.xml`
  /// * core:   `core-site.xml`
  /// * distcp: `distcp-default.xml`
  /// * hdfs:   `hdfs-site.xml`
  /// * hive:   `hive-site.xml`
  /// * mapred: `mapred-site.xml`
  /// * pig:    `pig.properties`
  /// * spark:  `spark-defaults.conf`
  /// * yarn:   `yarn-site.xml`
  ///
  /// For more information, see [Cluster
  /// properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
  public var properties: Dictionary<String,String> = [:]

  /// The set of optional components to activate on the cluster.
  public var optionalComponents: [Google_Cloud_Dataproc_V1beta2_Component] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Contains cluster daemon metrics, such as HDFS and YARN stats.
///
/// **Beta Feature**: This report is available for testing purposes only. It may
/// be changed before final release.
public struct Google_Cloud_Dataproc_V1beta2_ClusterMetrics {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The HDFS metrics.
  public var hdfsMetrics: Dictionary<String,Int64> = [:]

  /// The YARN metrics.
  public var yarnMetrics: Dictionary<String,Int64> = [:]

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A request to create a cluster.
public struct Google_Cloud_Dataproc_V1beta2_CreateClusterRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The ID of the Google Cloud Platform project that the cluster
  /// belongs to.
  public var projectID: String = String()

  /// Required. The Dataproc region in which to handle the request.
  public var region: String = String()

  /// Required. The cluster to create.
  public var cluster: Google_Cloud_Dataproc_V1beta2_Cluster {
    get {return _cluster ?? Google_Cloud_Dataproc_V1beta2_Cluster()}
    set {_cluster = newValue}
  }
  /// Returns true if `cluster` has been explicitly set.
  public var hasCluster: Bool {return self._cluster != nil}
  /// Clears the value of `cluster`. Subsequent reads from it will return its default value.
  public mutating func clearCluster() {self._cluster = nil}

  /// Optional. A unique id used to identify the request. If the server
  /// receives two [CreateClusterRequest][google.cloud.dataproc.v1beta2.CreateClusterRequest] requests  with the same
  /// id, then the second request will be ignored and the
  /// first [google.longrunning.Operation][google.longrunning.Operation] created and stored in the backend
  /// is returned.
  ///
  /// It is recommended to always set this value to a
  /// [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).
  ///
  /// The id must contain only letters (a-z, A-Z), numbers (0-9),
  /// underscores (_), and hyphens (-). The maximum length is 40 characters.
  public var requestID: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _cluster: Google_Cloud_Dataproc_V1beta2_Cluster? = nil
}

/// A request to update a cluster.
public struct Google_Cloud_Dataproc_V1beta2_UpdateClusterRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The ID of the Google Cloud Platform project the
  /// cluster belongs to.
  public var projectID: String {
    get {return _storage._projectID}
    set {_uniqueStorage()._projectID = newValue}
  }

  /// Required. The Dataproc region in which to handle the request.
  public var region: String {
    get {return _storage._region}
    set {_uniqueStorage()._region = newValue}
  }

  /// Required. The cluster name.
  public var clusterName: String {
    get {return _storage._clusterName}
    set {_uniqueStorage()._clusterName = newValue}
  }

  /// Required. The changes to the cluster.
  public var cluster: Google_Cloud_Dataproc_V1beta2_Cluster {
    get {return _storage._cluster ?? Google_Cloud_Dataproc_V1beta2_Cluster()}
    set {_uniqueStorage()._cluster = newValue}
  }
  /// Returns true if `cluster` has been explicitly set.
  public var hasCluster: Bool {return _storage._cluster != nil}
  /// Clears the value of `cluster`. Subsequent reads from it will return its default value.
  public mutating func clearCluster() {_uniqueStorage()._cluster = nil}

  /// Optional. Timeout for graceful YARN decomissioning. Graceful
  /// decommissioning allows removing nodes from the cluster without
  /// interrupting jobs in progress. Timeout specifies how long to wait for jobs
  /// in progress to finish before forcefully removing nodes (and potentially
  /// interrupting jobs). Default timeout is 0 (for forceful decommission), and
  /// the maximum allowed timeout is 1 day (see JSON representation of
  /// [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
  ///
  /// Only supported on Dataproc image versions 1.2 and higher.
  public var gracefulDecommissionTimeout: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _storage._gracefulDecommissionTimeout ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_uniqueStorage()._gracefulDecommissionTimeout = newValue}
  }
  /// Returns true if `gracefulDecommissionTimeout` has been explicitly set.
  public var hasGracefulDecommissionTimeout: Bool {return _storage._gracefulDecommissionTimeout != nil}
  /// Clears the value of `gracefulDecommissionTimeout`. Subsequent reads from it will return its default value.
  public mutating func clearGracefulDecommissionTimeout() {_uniqueStorage()._gracefulDecommissionTimeout = nil}

  /// Required. Specifies the path, relative to `Cluster`, of
  /// the field to update. For example, to change the number of workers
  /// in a cluster to 5, the `update_mask` parameter would be
  /// specified as `config.worker_config.num_instances`,
  /// and the `PATCH` request body would specify the new value, as follows:
  ///
  ///     {
  ///       "config":{
  ///         "workerConfig":{
  ///           "numInstances":"5"
  ///         }
  ///       }
  ///     }
  ///
  /// Similarly, to change the number of preemptible workers in a cluster to 5,
  /// the `update_mask` parameter would be
  /// `config.secondary_worker_config.num_instances`, and the `PATCH` request
  /// body would be set as follows:
  ///
  ///     {
  ///       "config":{
  ///         "secondaryWorkerConfig":{
  ///           "numInstances":"5"
  ///         }
  ///       }
  ///     }
  /// <strong>Note:</strong> currently only the following fields can be updated:
  ///
  /// <table>
  /// <tr>
  /// <td><strong>Mask</strong></td><td><strong>Purpose</strong></td>
  /// </tr>
  /// <tr>
  /// <td>labels</td><td>Updates labels</td>
  /// </tr>
  /// <tr>
  /// <td>config.worker_config.num_instances</td><td>Resize primary worker
  /// group</td>
  /// </tr>
  /// <tr>
  /// <td>config.secondary_worker_config.num_instances</td><td>Resize secondary
  /// worker group</td>
  /// </tr>
  /// <tr>
  /// <td>config.lifecycle_config.auto_delete_ttl</td><td>Reset MAX TTL
  /// duration</td>
  /// </tr>
  /// <tr>
  /// <td>config.lifecycle_config.auto_delete_time</td><td>Update MAX TTL
  /// deletion timestamp</td>
  /// </tr>
  /// <tr>
  /// <td>config.lifecycle_config.idle_delete_ttl</td><td>Update Idle TTL
  /// duration</td>
  /// </tr>
  /// <tr>
  /// <td>config.autoscaling_config.policy_uri</td><td>Use, stop using, or change
  /// autoscaling policies</td>
  /// </tr>
  /// </table>
  public var updateMask: SwiftProtobuf.Google_Protobuf_FieldMask {
    get {return _storage._updateMask ?? SwiftProtobuf.Google_Protobuf_FieldMask()}
    set {_uniqueStorage()._updateMask = newValue}
  }
  /// Returns true if `updateMask` has been explicitly set.
  public var hasUpdateMask: Bool {return _storage._updateMask != nil}
  /// Clears the value of `updateMask`. Subsequent reads from it will return its default value.
  public mutating func clearUpdateMask() {_uniqueStorage()._updateMask = nil}

  /// Optional. A unique id used to identify the request. If the server
  /// receives two [UpdateClusterRequest][google.cloud.dataproc.v1beta2.UpdateClusterRequest] requests  with the same
  /// id, then the second request will be ignored and the
  /// first [google.longrunning.Operation][google.longrunning.Operation] created and stored in the
  /// backend is returned.
  ///
  /// It is recommended to always set this value to a
  /// [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).
  ///
  /// The id must contain only letters (a-z, A-Z), numbers (0-9),
  /// underscores (_), and hyphens (-). The maximum length is 40 characters.
  public var requestID: String {
    get {return _storage._requestID}
    set {_uniqueStorage()._requestID = newValue}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// A request to delete a cluster.
public struct Google_Cloud_Dataproc_V1beta2_DeleteClusterRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The ID of the Google Cloud Platform project that the cluster
  /// belongs to.
  public var projectID: String = String()

  /// Required. The Dataproc region in which to handle the request.
  public var region: String = String()

  /// Required. The cluster name.
  public var clusterName: String = String()

  /// Optional. Specifying the `cluster_uuid` means the RPC should fail
  /// (with error NOT_FOUND) if cluster with specified UUID does not exist.
  public var clusterUuid: String = String()

  /// Optional. A unique id used to identify the request. If the server
  /// receives two [DeleteClusterRequest][google.cloud.dataproc.v1beta2.DeleteClusterRequest] requests  with the same
  /// id, then the second request will be ignored and the
  /// first [google.longrunning.Operation][google.longrunning.Operation] created and stored in the
  /// backend is returned.
  ///
  /// It is recommended to always set this value to a
  /// [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).
  ///
  /// The id must contain only letters (a-z, A-Z), numbers (0-9),
  /// underscores (_), and hyphens (-). The maximum length is 40 characters.
  public var requestID: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Request to get the resource representation for a cluster in a project.
public struct Google_Cloud_Dataproc_V1beta2_GetClusterRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The ID of the Google Cloud Platform project that the cluster
  /// belongs to.
  public var projectID: String = String()

  /// Required. The Dataproc region in which to handle the request.
  public var region: String = String()

  /// Required. The cluster name.
  public var clusterName: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A request to list the clusters in a project.
public struct Google_Cloud_Dataproc_V1beta2_ListClustersRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The ID of the Google Cloud Platform project that the cluster
  /// belongs to.
  public var projectID: String = String()

  /// Required. The Dataproc region in which to handle the request.
  public var region: String = String()

  /// Optional.  A filter constraining the clusters to list. Filters are
  /// case-sensitive and have the following syntax:
  ///
  /// field = value [AND [field = value]] ...
  ///
  /// where **field** is one of `status.state`, `clusterName`, or `labels.[KEY]`,
  /// and `[KEY]` is a label key. **value** can be `*` to match all values.
  /// `status.state` can be one of the following: `ACTIVE`, `INACTIVE`,
  /// `CREATING`, `RUNNING`, `ERROR`, `DELETING`, or `UPDATING`. `ACTIVE`
  /// contains the `CREATING`, `UPDATING`, and `RUNNING` states. `INACTIVE`
  /// contains the `DELETING` and `ERROR` states.
  /// `clusterName` is the name of the cluster provided at creation time.
  /// Only the logical `AND` operator is supported; space-separated items are
  /// treated as having an implicit `AND` operator.
  ///
  /// Example filter:
  ///
  /// status.state = ACTIVE AND clusterName = mycluster
  /// AND labels.env = staging AND labels.starred = *
  public var filter: String = String()

  /// Optional. The standard List page size.
  public var pageSize: Int32 = 0

  /// Optional. The standard List page token.
  public var pageToken: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// The list of all clusters in a project.
public struct Google_Cloud_Dataproc_V1beta2_ListClustersResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The clusters in the project.
  public var clusters: [Google_Cloud_Dataproc_V1beta2_Cluster] = []

  /// Output only. This token is included in the response if there are more
  /// results to fetch. To fetch additional results, provide this value as the
  /// `page_token` in a subsequent <code>ListClustersRequest</code>.
  public var nextPageToken: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// A request to collect cluster diagnostic information.
public struct Google_Cloud_Dataproc_V1beta2_DiagnoseClusterRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. The ID of the Google Cloud Platform project that the cluster
  /// belongs to.
  public var projectID: String = String()

  /// Required. The Dataproc region in which to handle the request.
  public var region: String = String()

  /// Required. The cluster name.
  public var clusterName: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// The location of diagnostic output.
public struct Google_Cloud_Dataproc_V1beta2_DiagnoseClusterResults {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Output only. The Cloud Storage URI of the diagnostic output.
  /// The output report is a plain text file with a summary of collected
  /// diagnostics.
  public var outputUri: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Reservation Affinity for consuming Zonal reservation.
public struct Google_Cloud_Dataproc_V1beta2_ReservationAffinity {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. Type of reservation to consume
  public var consumeReservationType: Google_Cloud_Dataproc_V1beta2_ReservationAffinity.TypeEnum = .unspecified

  /// Optional. Corresponds to the label key of reservation resource.
  public var key: String = String()

  /// Optional. Corresponds to the label values of reservation resource.
  public var values: [String] = []

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Indicates whether to consume capacity from an reservation or not.
  public enum TypeEnum: SwiftProtobuf.Enum {
    public typealias RawValue = Int
    case unspecified // = 0

    /// Do not consume from any allocated capacity.
    case noReservation // = 1

    /// Consume any reservation available.
    case anyReservation // = 2

    /// Must consume from a specific reservation. Must specify key value fields
    /// for specifying the reservations.
    case specificReservation // = 3
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .noReservation
      case 2: self = .anyReservation
      case 3: self = .specificReservation
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .noReservation: return 1
      case .anyReservation: return 2
      case .specificReservation: return 3
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}
}

#if swift(>=4.2)

extension Google_Cloud_Dataproc_V1beta2_ReservationAffinity.TypeEnum: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dataproc_V1beta2_ReservationAffinity.TypeEnum] = [
    .unspecified,
    .noReservation,
    .anyReservation,
    .specificReservation,
  ]
}

#endif  // swift(>=4.2)

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.dataproc.v1beta2"

extension Google_Cloud_Dataproc_V1beta2_Cluster: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".Cluster"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    2: .standard(proto: "cluster_name"),
    3: .same(proto: "config"),
    8: .same(proto: "labels"),
    4: .same(proto: "status"),
    7: .standard(proto: "status_history"),
    6: .standard(proto: "cluster_uuid"),
    9: .same(proto: "metrics"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.clusterName) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._config) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._status) }()
      case 6: try { try decoder.decodeSingularStringField(value: &self.clusterUuid) }()
      case 7: try { try decoder.decodeRepeatedMessageField(value: &self.statusHistory) }()
      case 8: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.labels) }()
      case 9: try { try decoder.decodeSingularMessageField(value: &self._metrics) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.clusterName.isEmpty {
      try visitor.visitSingularStringField(value: self.clusterName, fieldNumber: 2)
    }
    if let v = self._config {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if let v = self._status {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    if !self.clusterUuid.isEmpty {
      try visitor.visitSingularStringField(value: self.clusterUuid, fieldNumber: 6)
    }
    if !self.statusHistory.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.statusHistory, fieldNumber: 7)
    }
    if !self.labels.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.labels, fieldNumber: 8)
    }
    if let v = self._metrics {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_Cluster, rhs: Google_Cloud_Dataproc_V1beta2_Cluster) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.clusterName != rhs.clusterName {return false}
    if lhs._config != rhs._config {return false}
    if lhs.labels != rhs.labels {return false}
    if lhs._status != rhs._status {return false}
    if lhs.statusHistory != rhs.statusHistory {return false}
    if lhs.clusterUuid != rhs.clusterUuid {return false}
    if lhs._metrics != rhs._metrics {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ClusterConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ClusterConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "config_bucket"),
    8: .standard(proto: "gce_cluster_config"),
    9: .standard(proto: "master_config"),
    10: .standard(proto: "worker_config"),
    12: .standard(proto: "secondary_worker_config"),
    13: .standard(proto: "software_config"),
    14: .standard(proto: "lifecycle_config"),
    11: .standard(proto: "initialization_actions"),
    15: .standard(proto: "encryption_config"),
    16: .standard(proto: "autoscaling_config"),
    17: .standard(proto: "endpoint_config"),
    18: .standard(proto: "security_config"),
    19: .standard(proto: "gke_cluster_config"),
  ]

  fileprivate class _StorageClass {
    var _configBucket: String = String()
    var _gceClusterConfig: Google_Cloud_Dataproc_V1beta2_GceClusterConfig? = nil
    var _masterConfig: Google_Cloud_Dataproc_V1beta2_InstanceGroupConfig? = nil
    var _workerConfig: Google_Cloud_Dataproc_V1beta2_InstanceGroupConfig? = nil
    var _secondaryWorkerConfig: Google_Cloud_Dataproc_V1beta2_InstanceGroupConfig? = nil
    var _softwareConfig: Google_Cloud_Dataproc_V1beta2_SoftwareConfig? = nil
    var _lifecycleConfig: Google_Cloud_Dataproc_V1beta2_LifecycleConfig? = nil
    var _initializationActions: [Google_Cloud_Dataproc_V1beta2_NodeInitializationAction] = []
    var _encryptionConfig: Google_Cloud_Dataproc_V1beta2_EncryptionConfig? = nil
    var _autoscalingConfig: Google_Cloud_Dataproc_V1beta2_AutoscalingConfig? = nil
    var _endpointConfig: Google_Cloud_Dataproc_V1beta2_EndpointConfig? = nil
    var _securityConfig: Google_Cloud_Dataproc_V1beta2_SecurityConfig? = nil
    var _gkeClusterConfig: Google_Cloud_Dataproc_V1beta2_GkeClusterConfig? = nil

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _configBucket = source._configBucket
      _gceClusterConfig = source._gceClusterConfig
      _masterConfig = source._masterConfig
      _workerConfig = source._workerConfig
      _secondaryWorkerConfig = source._secondaryWorkerConfig
      _softwareConfig = source._softwareConfig
      _lifecycleConfig = source._lifecycleConfig
      _initializationActions = source._initializationActions
      _encryptionConfig = source._encryptionConfig
      _autoscalingConfig = source._autoscalingConfig
      _endpointConfig = source._endpointConfig
      _securityConfig = source._securityConfig
      _gkeClusterConfig = source._gkeClusterConfig
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularStringField(value: &_storage._configBucket) }()
        case 8: try { try decoder.decodeSingularMessageField(value: &_storage._gceClusterConfig) }()
        case 9: try { try decoder.decodeSingularMessageField(value: &_storage._masterConfig) }()
        case 10: try { try decoder.decodeSingularMessageField(value: &_storage._workerConfig) }()
        case 11: try { try decoder.decodeRepeatedMessageField(value: &_storage._initializationActions) }()
        case 12: try { try decoder.decodeSingularMessageField(value: &_storage._secondaryWorkerConfig) }()
        case 13: try { try decoder.decodeSingularMessageField(value: &_storage._softwareConfig) }()
        case 14: try { try decoder.decodeSingularMessageField(value: &_storage._lifecycleConfig) }()
        case 15: try { try decoder.decodeSingularMessageField(value: &_storage._encryptionConfig) }()
        case 16: try { try decoder.decodeSingularMessageField(value: &_storage._autoscalingConfig) }()
        case 17: try { try decoder.decodeSingularMessageField(value: &_storage._endpointConfig) }()
        case 18: try { try decoder.decodeSingularMessageField(value: &_storage._securityConfig) }()
        case 19: try { try decoder.decodeSingularMessageField(value: &_storage._gkeClusterConfig) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if !_storage._configBucket.isEmpty {
        try visitor.visitSingularStringField(value: _storage._configBucket, fieldNumber: 1)
      }
      if let v = _storage._gceClusterConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
      }
      if let v = _storage._masterConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
      }
      if let v = _storage._workerConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 10)
      }
      if !_storage._initializationActions.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._initializationActions, fieldNumber: 11)
      }
      if let v = _storage._secondaryWorkerConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 12)
      }
      if let v = _storage._softwareConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 13)
      }
      if let v = _storage._lifecycleConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 14)
      }
      if let v = _storage._encryptionConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 15)
      }
      if let v = _storage._autoscalingConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 16)
      }
      if let v = _storage._endpointConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 17)
      }
      if let v = _storage._securityConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 18)
      }
      if let v = _storage._gkeClusterConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 19)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ClusterConfig, rhs: Google_Cloud_Dataproc_V1beta2_ClusterConfig) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._configBucket != rhs_storage._configBucket {return false}
        if _storage._gceClusterConfig != rhs_storage._gceClusterConfig {return false}
        if _storage._masterConfig != rhs_storage._masterConfig {return false}
        if _storage._workerConfig != rhs_storage._workerConfig {return false}
        if _storage._secondaryWorkerConfig != rhs_storage._secondaryWorkerConfig {return false}
        if _storage._softwareConfig != rhs_storage._softwareConfig {return false}
        if _storage._lifecycleConfig != rhs_storage._lifecycleConfig {return false}
        if _storage._initializationActions != rhs_storage._initializationActions {return false}
        if _storage._encryptionConfig != rhs_storage._encryptionConfig {return false}
        if _storage._autoscalingConfig != rhs_storage._autoscalingConfig {return false}
        if _storage._endpointConfig != rhs_storage._endpointConfig {return false}
        if _storage._securityConfig != rhs_storage._securityConfig {return false}
        if _storage._gkeClusterConfig != rhs_storage._gkeClusterConfig {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_GkeClusterConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".GkeClusterConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "namespaced_gke_deployment_target"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._namespacedGkeDeploymentTarget) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._namespacedGkeDeploymentTarget {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_GkeClusterConfig, rhs: Google_Cloud_Dataproc_V1beta2_GkeClusterConfig) -> Bool {
    if lhs._namespacedGkeDeploymentTarget != rhs._namespacedGkeDeploymentTarget {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_GkeClusterConfig.NamespacedGkeDeploymentTarget: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Dataproc_V1beta2_GkeClusterConfig.protoMessageName + ".NamespacedGkeDeploymentTarget"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "target_gke_cluster"),
    2: .standard(proto: "cluster_namespace"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.targetGkeCluster) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.clusterNamespace) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.targetGkeCluster.isEmpty {
      try visitor.visitSingularStringField(value: self.targetGkeCluster, fieldNumber: 1)
    }
    if !self.clusterNamespace.isEmpty {
      try visitor.visitSingularStringField(value: self.clusterNamespace, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_GkeClusterConfig.NamespacedGkeDeploymentTarget, rhs: Google_Cloud_Dataproc_V1beta2_GkeClusterConfig.NamespacedGkeDeploymentTarget) -> Bool {
    if lhs.targetGkeCluster != rhs.targetGkeCluster {return false}
    if lhs.clusterNamespace != rhs.clusterNamespace {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_EndpointConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".EndpointConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "http_ports"),
    2: .standard(proto: "enable_http_port_access"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.httpPorts) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.enableHTTPPortAccess) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.httpPorts.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.httpPorts, fieldNumber: 1)
    }
    if self.enableHTTPPortAccess != false {
      try visitor.visitSingularBoolField(value: self.enableHTTPPortAccess, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_EndpointConfig, rhs: Google_Cloud_Dataproc_V1beta2_EndpointConfig) -> Bool {
    if lhs.httpPorts != rhs.httpPorts {return false}
    if lhs.enableHTTPPortAccess != rhs.enableHTTPPortAccess {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_AutoscalingConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AutoscalingConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "policy_uri"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.policyUri) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.policyUri.isEmpty {
      try visitor.visitSingularStringField(value: self.policyUri, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_AutoscalingConfig, rhs: Google_Cloud_Dataproc_V1beta2_AutoscalingConfig) -> Bool {
    if lhs.policyUri != rhs.policyUri {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_EncryptionConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".EncryptionConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "gce_pd_kms_key_name"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.gcePdKmsKeyName) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.gcePdKmsKeyName.isEmpty {
      try visitor.visitSingularStringField(value: self.gcePdKmsKeyName, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_EncryptionConfig, rhs: Google_Cloud_Dataproc_V1beta2_EncryptionConfig) -> Bool {
    if lhs.gcePdKmsKeyName != rhs.gcePdKmsKeyName {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_GceClusterConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".GceClusterConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "zone_uri"),
    2: .standard(proto: "network_uri"),
    6: .standard(proto: "subnetwork_uri"),
    7: .standard(proto: "internal_ip_only"),
    8: .standard(proto: "service_account"),
    3: .standard(proto: "service_account_scopes"),
    4: .same(proto: "tags"),
    5: .same(proto: "metadata"),
    11: .standard(proto: "reservation_affinity"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.zoneUri) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.networkUri) }()
      case 3: try { try decoder.decodeRepeatedStringField(value: &self.serviceAccountScopes) }()
      case 4: try { try decoder.decodeRepeatedStringField(value: &self.tags) }()
      case 5: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.metadata) }()
      case 6: try { try decoder.decodeSingularStringField(value: &self.subnetworkUri) }()
      case 7: try { try decoder.decodeSingularBoolField(value: &self.internalIpOnly) }()
      case 8: try { try decoder.decodeSingularStringField(value: &self.serviceAccount) }()
      case 11: try { try decoder.decodeSingularMessageField(value: &self._reservationAffinity) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.zoneUri.isEmpty {
      try visitor.visitSingularStringField(value: self.zoneUri, fieldNumber: 1)
    }
    if !self.networkUri.isEmpty {
      try visitor.visitSingularStringField(value: self.networkUri, fieldNumber: 2)
    }
    if !self.serviceAccountScopes.isEmpty {
      try visitor.visitRepeatedStringField(value: self.serviceAccountScopes, fieldNumber: 3)
    }
    if !self.tags.isEmpty {
      try visitor.visitRepeatedStringField(value: self.tags, fieldNumber: 4)
    }
    if !self.metadata.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.metadata, fieldNumber: 5)
    }
    if !self.subnetworkUri.isEmpty {
      try visitor.visitSingularStringField(value: self.subnetworkUri, fieldNumber: 6)
    }
    if self.internalIpOnly != false {
      try visitor.visitSingularBoolField(value: self.internalIpOnly, fieldNumber: 7)
    }
    if !self.serviceAccount.isEmpty {
      try visitor.visitSingularStringField(value: self.serviceAccount, fieldNumber: 8)
    }
    if let v = self._reservationAffinity {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 11)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_GceClusterConfig, rhs: Google_Cloud_Dataproc_V1beta2_GceClusterConfig) -> Bool {
    if lhs.zoneUri != rhs.zoneUri {return false}
    if lhs.networkUri != rhs.networkUri {return false}
    if lhs.subnetworkUri != rhs.subnetworkUri {return false}
    if lhs.internalIpOnly != rhs.internalIpOnly {return false}
    if lhs.serviceAccount != rhs.serviceAccount {return false}
    if lhs.serviceAccountScopes != rhs.serviceAccountScopes {return false}
    if lhs.tags != rhs.tags {return false}
    if lhs.metadata != rhs.metadata {return false}
    if lhs._reservationAffinity != rhs._reservationAffinity {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_InstanceGroupConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".InstanceGroupConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "num_instances"),
    2: .standard(proto: "instance_names"),
    3: .standard(proto: "image_uri"),
    4: .standard(proto: "machine_type_uri"),
    5: .standard(proto: "disk_config"),
    6: .standard(proto: "is_preemptible"),
    7: .standard(proto: "managed_group_config"),
    8: .same(proto: "accelerators"),
    9: .standard(proto: "min_cpu_platform"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularInt32Field(value: &self.numInstances) }()
      case 2: try { try decoder.decodeRepeatedStringField(value: &self.instanceNames) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.imageUri) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.machineTypeUri) }()
      case 5: try { try decoder.decodeSingularMessageField(value: &self._diskConfig) }()
      case 6: try { try decoder.decodeSingularBoolField(value: &self.isPreemptible) }()
      case 7: try { try decoder.decodeSingularMessageField(value: &self._managedGroupConfig) }()
      case 8: try { try decoder.decodeRepeatedMessageField(value: &self.accelerators) }()
      case 9: try { try decoder.decodeSingularStringField(value: &self.minCpuPlatform) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.numInstances != 0 {
      try visitor.visitSingularInt32Field(value: self.numInstances, fieldNumber: 1)
    }
    if !self.instanceNames.isEmpty {
      try visitor.visitRepeatedStringField(value: self.instanceNames, fieldNumber: 2)
    }
    if !self.imageUri.isEmpty {
      try visitor.visitSingularStringField(value: self.imageUri, fieldNumber: 3)
    }
    if !self.machineTypeUri.isEmpty {
      try visitor.visitSingularStringField(value: self.machineTypeUri, fieldNumber: 4)
    }
    if let v = self._diskConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    }
    if self.isPreemptible != false {
      try visitor.visitSingularBoolField(value: self.isPreemptible, fieldNumber: 6)
    }
    if let v = self._managedGroupConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 7)
    }
    if !self.accelerators.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.accelerators, fieldNumber: 8)
    }
    if !self.minCpuPlatform.isEmpty {
      try visitor.visitSingularStringField(value: self.minCpuPlatform, fieldNumber: 9)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_InstanceGroupConfig, rhs: Google_Cloud_Dataproc_V1beta2_InstanceGroupConfig) -> Bool {
    if lhs.numInstances != rhs.numInstances {return false}
    if lhs.instanceNames != rhs.instanceNames {return false}
    if lhs.imageUri != rhs.imageUri {return false}
    if lhs.machineTypeUri != rhs.machineTypeUri {return false}
    if lhs._diskConfig != rhs._diskConfig {return false}
    if lhs.isPreemptible != rhs.isPreemptible {return false}
    if lhs._managedGroupConfig != rhs._managedGroupConfig {return false}
    if lhs.accelerators != rhs.accelerators {return false}
    if lhs.minCpuPlatform != rhs.minCpuPlatform {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ManagedGroupConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ManagedGroupConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "instance_template_name"),
    2: .standard(proto: "instance_group_manager_name"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.instanceTemplateName) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.instanceGroupManagerName) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.instanceTemplateName.isEmpty {
      try visitor.visitSingularStringField(value: self.instanceTemplateName, fieldNumber: 1)
    }
    if !self.instanceGroupManagerName.isEmpty {
      try visitor.visitSingularStringField(value: self.instanceGroupManagerName, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ManagedGroupConfig, rhs: Google_Cloud_Dataproc_V1beta2_ManagedGroupConfig) -> Bool {
    if lhs.instanceTemplateName != rhs.instanceTemplateName {return false}
    if lhs.instanceGroupManagerName != rhs.instanceGroupManagerName {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_AcceleratorConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AcceleratorConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "accelerator_type_uri"),
    2: .standard(proto: "accelerator_count"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.acceleratorTypeUri) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.acceleratorCount) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.acceleratorTypeUri.isEmpty {
      try visitor.visitSingularStringField(value: self.acceleratorTypeUri, fieldNumber: 1)
    }
    if self.acceleratorCount != 0 {
      try visitor.visitSingularInt32Field(value: self.acceleratorCount, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_AcceleratorConfig, rhs: Google_Cloud_Dataproc_V1beta2_AcceleratorConfig) -> Bool {
    if lhs.acceleratorTypeUri != rhs.acceleratorTypeUri {return false}
    if lhs.acceleratorCount != rhs.acceleratorCount {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_DiskConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".DiskConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    3: .standard(proto: "boot_disk_type"),
    1: .standard(proto: "boot_disk_size_gb"),
    2: .standard(proto: "num_local_ssds"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularInt32Field(value: &self.bootDiskSizeGb) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.numLocalSsds) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.bootDiskType) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.bootDiskSizeGb != 0 {
      try visitor.visitSingularInt32Field(value: self.bootDiskSizeGb, fieldNumber: 1)
    }
    if self.numLocalSsds != 0 {
      try visitor.visitSingularInt32Field(value: self.numLocalSsds, fieldNumber: 2)
    }
    if !self.bootDiskType.isEmpty {
      try visitor.visitSingularStringField(value: self.bootDiskType, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_DiskConfig, rhs: Google_Cloud_Dataproc_V1beta2_DiskConfig) -> Bool {
    if lhs.bootDiskType != rhs.bootDiskType {return false}
    if lhs.bootDiskSizeGb != rhs.bootDiskSizeGb {return false}
    if lhs.numLocalSsds != rhs.numLocalSsds {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_LifecycleConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".LifecycleConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "idle_delete_ttl"),
    2: .standard(proto: "auto_delete_time"),
    3: .standard(proto: "auto_delete_ttl"),
    4: .standard(proto: "idle_start_time"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._idleDeleteTtl) }()
      case 2: try {
        var v: SwiftProtobuf.Google_Protobuf_Timestamp?
        if let current = self.ttl {
          try decoder.handleConflictingOneOf()
          if case .autoDeleteTime(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.ttl = .autoDeleteTime(v)}
      }()
      case 3: try {
        var v: SwiftProtobuf.Google_Protobuf_Duration?
        if let current = self.ttl {
          try decoder.handleConflictingOneOf()
          if case .autoDeleteTtl(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.ttl = .autoDeleteTtl(v)}
      }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._idleStartTime) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._idleDeleteTtl {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.ttl {
    case .autoDeleteTime?: try {
      guard case .autoDeleteTime(let v)? = self.ttl else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }()
    case .autoDeleteTtl?: try {
      guard case .autoDeleteTtl(let v)? = self.ttl else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }()
    case nil: break
    }
    if let v = self._idleStartTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_LifecycleConfig, rhs: Google_Cloud_Dataproc_V1beta2_LifecycleConfig) -> Bool {
    if lhs._idleDeleteTtl != rhs._idleDeleteTtl {return false}
    if lhs.ttl != rhs.ttl {return false}
    if lhs._idleStartTime != rhs._idleStartTime {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_SecurityConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SecurityConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "kerberos_config"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._kerberosConfig) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._kerberosConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_SecurityConfig, rhs: Google_Cloud_Dataproc_V1beta2_SecurityConfig) -> Bool {
    if lhs._kerberosConfig != rhs._kerberosConfig {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_KerberosConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".KerberosConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "enable_kerberos"),
    2: .standard(proto: "root_principal_password_uri"),
    3: .standard(proto: "kms_key_uri"),
    4: .standard(proto: "keystore_uri"),
    5: .standard(proto: "truststore_uri"),
    6: .standard(proto: "keystore_password_uri"),
    7: .standard(proto: "key_password_uri"),
    8: .standard(proto: "truststore_password_uri"),
    9: .standard(proto: "cross_realm_trust_realm"),
    10: .standard(proto: "cross_realm_trust_kdc"),
    11: .standard(proto: "cross_realm_trust_admin_server"),
    12: .standard(proto: "cross_realm_trust_shared_password_uri"),
    13: .standard(proto: "kdc_db_key_uri"),
    14: .standard(proto: "tgt_lifetime_hours"),
    15: .same(proto: "realm"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularBoolField(value: &self.enableKerberos) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.rootPrincipalPasswordUri) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.kmsKeyUri) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.keystoreUri) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.truststoreUri) }()
      case 6: try { try decoder.decodeSingularStringField(value: &self.keystorePasswordUri) }()
      case 7: try { try decoder.decodeSingularStringField(value: &self.keyPasswordUri) }()
      case 8: try { try decoder.decodeSingularStringField(value: &self.truststorePasswordUri) }()
      case 9: try { try decoder.decodeSingularStringField(value: &self.crossRealmTrustRealm) }()
      case 10: try { try decoder.decodeSingularStringField(value: &self.crossRealmTrustKdc) }()
      case 11: try { try decoder.decodeSingularStringField(value: &self.crossRealmTrustAdminServer) }()
      case 12: try { try decoder.decodeSingularStringField(value: &self.crossRealmTrustSharedPasswordUri) }()
      case 13: try { try decoder.decodeSingularStringField(value: &self.kdcDbKeyUri) }()
      case 14: try { try decoder.decodeSingularInt32Field(value: &self.tgtLifetimeHours) }()
      case 15: try { try decoder.decodeSingularStringField(value: &self.realm) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.enableKerberos != false {
      try visitor.visitSingularBoolField(value: self.enableKerberos, fieldNumber: 1)
    }
    if !self.rootPrincipalPasswordUri.isEmpty {
      try visitor.visitSingularStringField(value: self.rootPrincipalPasswordUri, fieldNumber: 2)
    }
    if !self.kmsKeyUri.isEmpty {
      try visitor.visitSingularStringField(value: self.kmsKeyUri, fieldNumber: 3)
    }
    if !self.keystoreUri.isEmpty {
      try visitor.visitSingularStringField(value: self.keystoreUri, fieldNumber: 4)
    }
    if !self.truststoreUri.isEmpty {
      try visitor.visitSingularStringField(value: self.truststoreUri, fieldNumber: 5)
    }
    if !self.keystorePasswordUri.isEmpty {
      try visitor.visitSingularStringField(value: self.keystorePasswordUri, fieldNumber: 6)
    }
    if !self.keyPasswordUri.isEmpty {
      try visitor.visitSingularStringField(value: self.keyPasswordUri, fieldNumber: 7)
    }
    if !self.truststorePasswordUri.isEmpty {
      try visitor.visitSingularStringField(value: self.truststorePasswordUri, fieldNumber: 8)
    }
    if !self.crossRealmTrustRealm.isEmpty {
      try visitor.visitSingularStringField(value: self.crossRealmTrustRealm, fieldNumber: 9)
    }
    if !self.crossRealmTrustKdc.isEmpty {
      try visitor.visitSingularStringField(value: self.crossRealmTrustKdc, fieldNumber: 10)
    }
    if !self.crossRealmTrustAdminServer.isEmpty {
      try visitor.visitSingularStringField(value: self.crossRealmTrustAdminServer, fieldNumber: 11)
    }
    if !self.crossRealmTrustSharedPasswordUri.isEmpty {
      try visitor.visitSingularStringField(value: self.crossRealmTrustSharedPasswordUri, fieldNumber: 12)
    }
    if !self.kdcDbKeyUri.isEmpty {
      try visitor.visitSingularStringField(value: self.kdcDbKeyUri, fieldNumber: 13)
    }
    if self.tgtLifetimeHours != 0 {
      try visitor.visitSingularInt32Field(value: self.tgtLifetimeHours, fieldNumber: 14)
    }
    if !self.realm.isEmpty {
      try visitor.visitSingularStringField(value: self.realm, fieldNumber: 15)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_KerberosConfig, rhs: Google_Cloud_Dataproc_V1beta2_KerberosConfig) -> Bool {
    if lhs.enableKerberos != rhs.enableKerberos {return false}
    if lhs.rootPrincipalPasswordUri != rhs.rootPrincipalPasswordUri {return false}
    if lhs.kmsKeyUri != rhs.kmsKeyUri {return false}
    if lhs.keystoreUri != rhs.keystoreUri {return false}
    if lhs.truststoreUri != rhs.truststoreUri {return false}
    if lhs.keystorePasswordUri != rhs.keystorePasswordUri {return false}
    if lhs.keyPasswordUri != rhs.keyPasswordUri {return false}
    if lhs.truststorePasswordUri != rhs.truststorePasswordUri {return false}
    if lhs.crossRealmTrustRealm != rhs.crossRealmTrustRealm {return false}
    if lhs.crossRealmTrustKdc != rhs.crossRealmTrustKdc {return false}
    if lhs.crossRealmTrustAdminServer != rhs.crossRealmTrustAdminServer {return false}
    if lhs.crossRealmTrustSharedPasswordUri != rhs.crossRealmTrustSharedPasswordUri {return false}
    if lhs.kdcDbKeyUri != rhs.kdcDbKeyUri {return false}
    if lhs.tgtLifetimeHours != rhs.tgtLifetimeHours {return false}
    if lhs.realm != rhs.realm {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_NodeInitializationAction: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".NodeInitializationAction"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "executable_file"),
    2: .standard(proto: "execution_timeout"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.executableFile) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._executionTimeout) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.executableFile.isEmpty {
      try visitor.visitSingularStringField(value: self.executableFile, fieldNumber: 1)
    }
    if let v = self._executionTimeout {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_NodeInitializationAction, rhs: Google_Cloud_Dataproc_V1beta2_NodeInitializationAction) -> Bool {
    if lhs.executableFile != rhs.executableFile {return false}
    if lhs._executionTimeout != rhs._executionTimeout {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ClusterStatus: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ClusterStatus"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "state"),
    2: .same(proto: "detail"),
    3: .standard(proto: "state_start_time"),
    4: .same(proto: "substate"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.state) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.detail) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._stateStartTime) }()
      case 4: try { try decoder.decodeSingularEnumField(value: &self.substate) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.state != .unknown {
      try visitor.visitSingularEnumField(value: self.state, fieldNumber: 1)
    }
    if !self.detail.isEmpty {
      try visitor.visitSingularStringField(value: self.detail, fieldNumber: 2)
    }
    if let v = self._stateStartTime {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if self.substate != .unspecified {
      try visitor.visitSingularEnumField(value: self.substate, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ClusterStatus, rhs: Google_Cloud_Dataproc_V1beta2_ClusterStatus) -> Bool {
    if lhs.state != rhs.state {return false}
    if lhs.detail != rhs.detail {return false}
    if lhs._stateStartTime != rhs._stateStartTime {return false}
    if lhs.substate != rhs.substate {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ClusterStatus.State: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "UNKNOWN"),
    1: .same(proto: "CREATING"),
    2: .same(proto: "RUNNING"),
    3: .same(proto: "ERROR"),
    4: .same(proto: "DELETING"),
    5: .same(proto: "UPDATING"),
    6: .same(proto: "STOPPING"),
    7: .same(proto: "STOPPED"),
    8: .same(proto: "STARTING"),
  ]
}

extension Google_Cloud_Dataproc_V1beta2_ClusterStatus.Substate: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "UNSPECIFIED"),
    1: .same(proto: "UNHEALTHY"),
    2: .same(proto: "STALE_STATUS"),
  ]
}

extension Google_Cloud_Dataproc_V1beta2_SoftwareConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SoftwareConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "image_version"),
    2: .same(proto: "properties"),
    3: .standard(proto: "optional_components"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.imageVersion) }()
      case 2: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.properties) }()
      case 3: try { try decoder.decodeRepeatedEnumField(value: &self.optionalComponents) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.imageVersion.isEmpty {
      try visitor.visitSingularStringField(value: self.imageVersion, fieldNumber: 1)
    }
    if !self.properties.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.properties, fieldNumber: 2)
    }
    if !self.optionalComponents.isEmpty {
      try visitor.visitPackedEnumField(value: self.optionalComponents, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_SoftwareConfig, rhs: Google_Cloud_Dataproc_V1beta2_SoftwareConfig) -> Bool {
    if lhs.imageVersion != rhs.imageVersion {return false}
    if lhs.properties != rhs.properties {return false}
    if lhs.optionalComponents != rhs.optionalComponents {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ClusterMetrics: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ClusterMetrics"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "hdfs_metrics"),
    2: .standard(proto: "yarn_metrics"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufInt64>.self, value: &self.hdfsMetrics) }()
      case 2: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufInt64>.self, value: &self.yarnMetrics) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.hdfsMetrics.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufInt64>.self, value: self.hdfsMetrics, fieldNumber: 1)
    }
    if !self.yarnMetrics.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufInt64>.self, value: self.yarnMetrics, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ClusterMetrics, rhs: Google_Cloud_Dataproc_V1beta2_ClusterMetrics) -> Bool {
    if lhs.hdfsMetrics != rhs.hdfsMetrics {return false}
    if lhs.yarnMetrics != rhs.yarnMetrics {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_CreateClusterRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".CreateClusterRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    3: .same(proto: "region"),
    2: .same(proto: "cluster"),
    4: .standard(proto: "request_id"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._cluster) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.region) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.requestID) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if let v = self._cluster {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if !self.region.isEmpty {
      try visitor.visitSingularStringField(value: self.region, fieldNumber: 3)
    }
    if !self.requestID.isEmpty {
      try visitor.visitSingularStringField(value: self.requestID, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_CreateClusterRequest, rhs: Google_Cloud_Dataproc_V1beta2_CreateClusterRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.region != rhs.region {return false}
    if lhs._cluster != rhs._cluster {return false}
    if lhs.requestID != rhs.requestID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_UpdateClusterRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".UpdateClusterRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    5: .same(proto: "region"),
    2: .standard(proto: "cluster_name"),
    3: .same(proto: "cluster"),
    6: .standard(proto: "graceful_decommission_timeout"),
    4: .standard(proto: "update_mask"),
    7: .standard(proto: "request_id"),
  ]

  fileprivate class _StorageClass {
    var _projectID: String = String()
    var _region: String = String()
    var _clusterName: String = String()
    var _cluster: Google_Cloud_Dataproc_V1beta2_Cluster? = nil
    var _gracefulDecommissionTimeout: SwiftProtobuf.Google_Protobuf_Duration? = nil
    var _updateMask: SwiftProtobuf.Google_Protobuf_FieldMask? = nil
    var _requestID: String = String()

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _projectID = source._projectID
      _region = source._region
      _clusterName = source._clusterName
      _cluster = source._cluster
      _gracefulDecommissionTimeout = source._gracefulDecommissionTimeout
      _updateMask = source._updateMask
      _requestID = source._requestID
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularStringField(value: &_storage._projectID) }()
        case 2: try { try decoder.decodeSingularStringField(value: &_storage._clusterName) }()
        case 3: try { try decoder.decodeSingularMessageField(value: &_storage._cluster) }()
        case 4: try { try decoder.decodeSingularMessageField(value: &_storage._updateMask) }()
        case 5: try { try decoder.decodeSingularStringField(value: &_storage._region) }()
        case 6: try { try decoder.decodeSingularMessageField(value: &_storage._gracefulDecommissionTimeout) }()
        case 7: try { try decoder.decodeSingularStringField(value: &_storage._requestID) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if !_storage._projectID.isEmpty {
        try visitor.visitSingularStringField(value: _storage._projectID, fieldNumber: 1)
      }
      if !_storage._clusterName.isEmpty {
        try visitor.visitSingularStringField(value: _storage._clusterName, fieldNumber: 2)
      }
      if let v = _storage._cluster {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      }
      if let v = _storage._updateMask {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
      }
      if !_storage._region.isEmpty {
        try visitor.visitSingularStringField(value: _storage._region, fieldNumber: 5)
      }
      if let v = _storage._gracefulDecommissionTimeout {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
      }
      if !_storage._requestID.isEmpty {
        try visitor.visitSingularStringField(value: _storage._requestID, fieldNumber: 7)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_UpdateClusterRequest, rhs: Google_Cloud_Dataproc_V1beta2_UpdateClusterRequest) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._projectID != rhs_storage._projectID {return false}
        if _storage._region != rhs_storage._region {return false}
        if _storage._clusterName != rhs_storage._clusterName {return false}
        if _storage._cluster != rhs_storage._cluster {return false}
        if _storage._gracefulDecommissionTimeout != rhs_storage._gracefulDecommissionTimeout {return false}
        if _storage._updateMask != rhs_storage._updateMask {return false}
        if _storage._requestID != rhs_storage._requestID {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_DeleteClusterRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".DeleteClusterRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    3: .same(proto: "region"),
    2: .standard(proto: "cluster_name"),
    4: .standard(proto: "cluster_uuid"),
    5: .standard(proto: "request_id"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.clusterName) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.region) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.clusterUuid) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.requestID) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.clusterName.isEmpty {
      try visitor.visitSingularStringField(value: self.clusterName, fieldNumber: 2)
    }
    if !self.region.isEmpty {
      try visitor.visitSingularStringField(value: self.region, fieldNumber: 3)
    }
    if !self.clusterUuid.isEmpty {
      try visitor.visitSingularStringField(value: self.clusterUuid, fieldNumber: 4)
    }
    if !self.requestID.isEmpty {
      try visitor.visitSingularStringField(value: self.requestID, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_DeleteClusterRequest, rhs: Google_Cloud_Dataproc_V1beta2_DeleteClusterRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.region != rhs.region {return false}
    if lhs.clusterName != rhs.clusterName {return false}
    if lhs.clusterUuid != rhs.clusterUuid {return false}
    if lhs.requestID != rhs.requestID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_GetClusterRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".GetClusterRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    3: .same(proto: "region"),
    2: .standard(proto: "cluster_name"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.clusterName) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.region) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.clusterName.isEmpty {
      try visitor.visitSingularStringField(value: self.clusterName, fieldNumber: 2)
    }
    if !self.region.isEmpty {
      try visitor.visitSingularStringField(value: self.region, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_GetClusterRequest, rhs: Google_Cloud_Dataproc_V1beta2_GetClusterRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.region != rhs.region {return false}
    if lhs.clusterName != rhs.clusterName {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ListClustersRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ListClustersRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    4: .same(proto: "region"),
    5: .same(proto: "filter"),
    2: .standard(proto: "page_size"),
    3: .standard(proto: "page_token"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.pageSize) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.pageToken) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.region) }()
      case 5: try { try decoder.decodeSingularStringField(value: &self.filter) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if self.pageSize != 0 {
      try visitor.visitSingularInt32Field(value: self.pageSize, fieldNumber: 2)
    }
    if !self.pageToken.isEmpty {
      try visitor.visitSingularStringField(value: self.pageToken, fieldNumber: 3)
    }
    if !self.region.isEmpty {
      try visitor.visitSingularStringField(value: self.region, fieldNumber: 4)
    }
    if !self.filter.isEmpty {
      try visitor.visitSingularStringField(value: self.filter, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ListClustersRequest, rhs: Google_Cloud_Dataproc_V1beta2_ListClustersRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.region != rhs.region {return false}
    if lhs.filter != rhs.filter {return false}
    if lhs.pageSize != rhs.pageSize {return false}
    if lhs.pageToken != rhs.pageToken {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ListClustersResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ListClustersResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "clusters"),
    2: .standard(proto: "next_page_token"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.clusters) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.nextPageToken) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.clusters.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.clusters, fieldNumber: 1)
    }
    if !self.nextPageToken.isEmpty {
      try visitor.visitSingularStringField(value: self.nextPageToken, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ListClustersResponse, rhs: Google_Cloud_Dataproc_V1beta2_ListClustersResponse) -> Bool {
    if lhs.clusters != rhs.clusters {return false}
    if lhs.nextPageToken != rhs.nextPageToken {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_DiagnoseClusterRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".DiagnoseClusterRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "project_id"),
    3: .same(proto: "region"),
    2: .standard(proto: "cluster_name"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.projectID) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.clusterName) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.region) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.projectID.isEmpty {
      try visitor.visitSingularStringField(value: self.projectID, fieldNumber: 1)
    }
    if !self.clusterName.isEmpty {
      try visitor.visitSingularStringField(value: self.clusterName, fieldNumber: 2)
    }
    if !self.region.isEmpty {
      try visitor.visitSingularStringField(value: self.region, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_DiagnoseClusterRequest, rhs: Google_Cloud_Dataproc_V1beta2_DiagnoseClusterRequest) -> Bool {
    if lhs.projectID != rhs.projectID {return false}
    if lhs.region != rhs.region {return false}
    if lhs.clusterName != rhs.clusterName {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_DiagnoseClusterResults: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".DiagnoseClusterResults"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "output_uri"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.outputUri) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.outputUri.isEmpty {
      try visitor.visitSingularStringField(value: self.outputUri, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_DiagnoseClusterResults, rhs: Google_Cloud_Dataproc_V1beta2_DiagnoseClusterResults) -> Bool {
    if lhs.outputUri != rhs.outputUri {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ReservationAffinity: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ReservationAffinity"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "consume_reservation_type"),
    2: .same(proto: "key"),
    3: .same(proto: "values"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.consumeReservationType) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.key) }()
      case 3: try { try decoder.decodeRepeatedStringField(value: &self.values) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.consumeReservationType != .unspecified {
      try visitor.visitSingularEnumField(value: self.consumeReservationType, fieldNumber: 1)
    }
    if !self.key.isEmpty {
      try visitor.visitSingularStringField(value: self.key, fieldNumber: 2)
    }
    if !self.values.isEmpty {
      try visitor.visitRepeatedStringField(value: self.values, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_V1beta2_ReservationAffinity, rhs: Google_Cloud_Dataproc_V1beta2_ReservationAffinity) -> Bool {
    if lhs.consumeReservationType != rhs.consumeReservationType {return false}
    if lhs.key != rhs.key {return false}
    if lhs.values != rhs.values {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_V1beta2_ReservationAffinity.TypeEnum: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "TYPE_UNSPECIFIED"),
    1: .same(proto: "NO_RESERVATION"),
    2: .same(proto: "ANY_RESERVATION"),
    3: .same(proto: "SPECIFIC_RESERVATION"),
  ]
}
