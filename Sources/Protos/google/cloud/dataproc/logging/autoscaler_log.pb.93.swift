// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/dataproc/logging/autoscaler_log.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2020 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// The Autoscaler state.
public enum Google_Cloud_Dataproc_Logging_AutoscalerState: SwiftProtobuf.Enum {
  public typealias RawValue = Int
  case unspecified // = 0

  /// The Autoscaler is sleeping and waiting for the next update.
  case cooldown // = 1

  /// The Autoscaler is in the process of calculating its recommendation on
  /// whether to scale the cluster, and if so, how to autoscale.
  case recommending // = 6

  /// The Autoscaler is scaling the cluster.
  case scaling // = 2

  /// The Autoscaler has stopped.
  case stopped // = 3

  /// The Autoscaler has failed.
  case failed // = 4

  /// The Autoscaler is initializing.
  case initializing // = 5
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .cooldown
    case 2: self = .scaling
    case 3: self = .stopped
    case 4: self = .failed
    case 5: self = .initializing
    case 6: self = .recommending
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .cooldown: return 1
    case .scaling: return 2
    case .stopped: return 3
    case .failed: return 4
    case .initializing: return 5
    case .recommending: return 6
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Dataproc_Logging_AutoscalerState: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dataproc_Logging_AutoscalerState] = [
    .unspecified,
    .cooldown,
    .recommending,
    .scaling,
    .stopped,
    .failed,
    .initializing,
  ]
}

#endif  // swift(>=4.2)

/// The Autoscaling decision type.
public enum Google_Cloud_Dataproc_Logging_ScalingDecisionType: SwiftProtobuf.Enum {
  public typealias RawValue = Int
  case unspecified // = 0

  /// Increase the number of primary and/or secondary workers.
  case scaleUp // = 1

  /// Decrease the number of primary and/or secondary workers.
  case scaleDown // = 2

  /// Not changing the number of primary or secondary workers.
  case noScale // = 3

  /// Scale the primary and secondary worker groups in different directions.
  case mixed // = 4
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .scaleUp
    case 2: self = .scaleDown
    case 3: self = .noScale
    case 4: self = .mixed
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .scaleUp: return 1
    case .scaleDown: return 2
    case .noScale: return 3
    case .mixed: return 4
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Dataproc_Logging_ScalingDecisionType: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dataproc_Logging_ScalingDecisionType] = [
    .unspecified,
    .scaleUp,
    .scaleDown,
    .noScale,
    .mixed,
  ]
}

#endif  // swift(>=4.2)

public enum Google_Cloud_Dataproc_Logging_ConstrainingFactor: SwiftProtobuf.Enum {
  public typealias RawValue = Int
  case unspecified // = 0

  /// The project does not have sufficient regional, global, and or preemptible
  /// quota to allocate a new VM.
  case scalingCappedDueToLackOfQuota // = 1

  /// All worker groups have reached maximum size. This message will not be
  /// issued if one group reached maximum size, but workers were able to be
  /// allocated to another group.
  case reachedMaximumClusterSize // = 2

  /// All worker groups have reached minimum size. This message will not be
  /// issued if workers were able to be removed from another group that had not
  /// reached minimum size.
  case reachedMinimumClusterSize // = 3
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .scalingCappedDueToLackOfQuota
    case 2: self = .reachedMaximumClusterSize
    case 3: self = .reachedMinimumClusterSize
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .scalingCappedDueToLackOfQuota: return 1
    case .reachedMaximumClusterSize: return 2
    case .reachedMinimumClusterSize: return 3
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Dataproc_Logging_ConstrainingFactor: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dataproc_Logging_ConstrainingFactor] = [
    .unspecified,
    .scalingCappedDueToLackOfQuota,
    .reachedMaximumClusterSize,
    .reachedMinimumClusterSize,
  ]
}

#endif  // swift(>=4.2)

/// The short version of cluster configuration for Cloud logging.
public struct Google_Cloud_Dataproc_Logging_ClusterSize {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The number of primary workers in the cluster.
  public var primaryWorkerCount: Int32 = 0

  /// The number of secondary workers in the cluster.
  public var secondaryWorkerCount: Int32 = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// The main proto that will be converted to JSON format and then written to
/// Logging.
public struct Google_Cloud_Dataproc_Logging_AutoscalerLog {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The current Autoscaler status.
  public var status: Google_Cloud_Dataproc_Logging_AutoscalerStatus {
    get {return _storage._status ?? Google_Cloud_Dataproc_Logging_AutoscalerStatus()}
    set {_uniqueStorage()._status = newValue}
  }
  /// Returns true if `status` has been explicitly set.
  public var hasStatus: Bool {return _storage._status != nil}
  /// Clears the value of `status`. Subsequent reads from it will return its default value.
  public mutating func clearStatus() {_uniqueStorage()._status = nil}

  /// Optional. The autoscaling recommendation including its inputs, outputs,
  /// scaling decision, and detailed explanation.
  public var recommendation: Google_Cloud_Dataproc_Logging_AutoscalerRecommendation {
    get {return _storage._recommendation ?? Google_Cloud_Dataproc_Logging_AutoscalerRecommendation()}
    set {_uniqueStorage()._recommendation = newValue}
  }
  /// Returns true if `recommendation` has been explicitly set.
  public var hasRecommendation: Bool {return _storage._recommendation != nil}
  /// Clears the value of `recommendation`. Subsequent reads from it will return its default value.
  public mutating func clearRecommendation() {_uniqueStorage()._recommendation = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// The Autoscaler's status, including its state and other details.
public struct Google_Cloud_Dataproc_Logging_AutoscalerStatus {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The high-level Autoscaler state.
  public var state: Google_Cloud_Dataproc_Logging_AutoscalerState = .unspecified

  /// The detailed description of Autoscaler status.
  public var details: String = String()

  /// The cluster update operation ID.
  public var updateClusterOperationID: String = String()

  /// Error message from an Autoscaler exception, if any.
  public var error: String = String()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// The inputs, outputs, and detailed explanation of the Autoscaling
/// recommendation.
public struct Google_Cloud_Dataproc_Logging_AutoscalerRecommendation {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The autoscaling algorithm inputs.
  public var inputs: Google_Cloud_Dataproc_Logging_AutoscalerRecommendation.Inputs {
    get {return _inputs ?? Google_Cloud_Dataproc_Logging_AutoscalerRecommendation.Inputs()}
    set {_inputs = newValue}
  }
  /// Returns true if `inputs` has been explicitly set.
  public var hasInputs: Bool {return self._inputs != nil}
  /// Clears the value of `inputs`. Subsequent reads from it will return its default value.
  public mutating func clearInputs() {self._inputs = nil}

  /// The algorithm outputs for the recommended cluster size.
  public var outputs: Google_Cloud_Dataproc_Logging_AutoscalerRecommendation.Outputs {
    get {return _outputs ?? Google_Cloud_Dataproc_Logging_AutoscalerRecommendation.Outputs()}
    set {_outputs = newValue}
  }
  /// Returns true if `outputs` has been explicitly set.
  public var hasOutputs: Bool {return self._outputs != nil}
  /// Clears the value of `outputs`. Subsequent reads from it will return its default value.
  public mutating func clearOutputs() {self._outputs = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// The input values for the Autoscaling recommendation alogirthm.
  public struct Inputs {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// The metrics collected by the Dataproc agent running on the cluster.
    /// For example, {"avg-yarn-pending-memory": "1040 MB"}
    public var clusterMetrics: Dictionary<String,String> = [:]

    /// The cluster configuration before updating the cluster.
    public var currentClusterSize: Google_Cloud_Dataproc_Logging_ClusterSize {
      get {return _currentClusterSize ?? Google_Cloud_Dataproc_Logging_ClusterSize()}
      set {_currentClusterSize = newValue}
    }
    /// Returns true if `currentClusterSize` has been explicitly set.
    public var hasCurrentClusterSize: Bool {return self._currentClusterSize != nil}
    /// Clears the value of `currentClusterSize`. Subsequent reads from it will return its default value.
    public mutating func clearCurrentClusterSize() {self._currentClusterSize = nil}

    /// The minimum worker counts for each instance group.
    public var minWorkerCounts: Google_Cloud_Dataproc_Logging_ClusterSize {
      get {return _minWorkerCounts ?? Google_Cloud_Dataproc_Logging_ClusterSize()}
      set {_minWorkerCounts = newValue}
    }
    /// Returns true if `minWorkerCounts` has been explicitly set.
    public var hasMinWorkerCounts: Bool {return self._minWorkerCounts != nil}
    /// Clears the value of `minWorkerCounts`. Subsequent reads from it will return its default value.
    public mutating func clearMinWorkerCounts() {self._minWorkerCounts = nil}

    /// The maximum worker counts for each instance group.
    public var maxWorkerCounts: Google_Cloud_Dataproc_Logging_ClusterSize {
      get {return _maxWorkerCounts ?? Google_Cloud_Dataproc_Logging_ClusterSize()}
      set {_maxWorkerCounts = newValue}
    }
    /// Returns true if `maxWorkerCounts` has been explicitly set.
    public var hasMaxWorkerCounts: Bool {return self._maxWorkerCounts != nil}
    /// Clears the value of `maxWorkerCounts`. Subsequent reads from it will return its default value.
    public mutating func clearMaxWorkerCounts() {self._maxWorkerCounts = nil}

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    public init() {}

    fileprivate var _currentClusterSize: Google_Cloud_Dataproc_Logging_ClusterSize? = nil
    fileprivate var _minWorkerCounts: Google_Cloud_Dataproc_Logging_ClusterSize? = nil
    fileprivate var _maxWorkerCounts: Google_Cloud_Dataproc_Logging_ClusterSize? = nil
  }

  /// Autoscaler recommendations.
  public struct Outputs {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// The high-level autoscaling decision, such as SCALE_UP, SCALE_DOWN,
    /// NO_OP.
    public var decision: Google_Cloud_Dataproc_Logging_ScalingDecisionType = .unspecified

    /// The recommended cluster size.
    public var recommendedClusterSize: Google_Cloud_Dataproc_Logging_ClusterSize {
      get {return _recommendedClusterSize ?? Google_Cloud_Dataproc_Logging_ClusterSize()}
      set {_recommendedClusterSize = newValue}
    }
    /// Returns true if `recommendedClusterSize` has been explicitly set.
    public var hasRecommendedClusterSize: Bool {return self._recommendedClusterSize != nil}
    /// Clears the value of `recommendedClusterSize`. Subsequent reads from it will return its default value.
    public mutating func clearRecommendedClusterSize() {self._recommendedClusterSize = nil}

    /// The graceful decommission timeout for downscaling operations.
    public var gracefulDecommissionTimeout: SwiftProtobuf.Google_Protobuf_Duration {
      get {return _gracefulDecommissionTimeout ?? SwiftProtobuf.Google_Protobuf_Duration()}
      set {_gracefulDecommissionTimeout = newValue}
    }
    /// Returns true if `gracefulDecommissionTimeout` has been explicitly set.
    public var hasGracefulDecommissionTimeout: Bool {return self._gracefulDecommissionTimeout != nil}
    /// Clears the value of `gracefulDecommissionTimeout`. Subsequent reads from it will return its default value.
    public mutating func clearGracefulDecommissionTimeout() {self._gracefulDecommissionTimeout = nil}

    /// Reasons why the Autoscaler didn't add or remove more workers.
    public var constraintsReached: [Google_Cloud_Dataproc_Logging_ConstrainingFactor] = []

    /// Less significant recommendations that are not included in the
    /// `AutoscalerStatus.details` message.
    public var additionalRecommendationDetails: [String] = []

    /// A unique id for this recommendation that should be included when opening
    /// a support ticket.
    public var recommendationID: String = String()

    public var unknownFields = SwiftProtobuf.UnknownStorage()

    public init() {}

    fileprivate var _recommendedClusterSize: Google_Cloud_Dataproc_Logging_ClusterSize? = nil
    fileprivate var _gracefulDecommissionTimeout: SwiftProtobuf.Google_Protobuf_Duration? = nil
  }

  public init() {}

  fileprivate var _inputs: Google_Cloud_Dataproc_Logging_AutoscalerRecommendation.Inputs? = nil
  fileprivate var _outputs: Google_Cloud_Dataproc_Logging_AutoscalerRecommendation.Outputs? = nil
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.dataproc.logging"

extension Google_Cloud_Dataproc_Logging_AutoscalerState: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "AUTOSCALER_STATE_UNSPECIFIED"),
    1: .same(proto: "COOLDOWN"),
    2: .same(proto: "SCALING"),
    3: .same(proto: "STOPPED"),
    4: .same(proto: "FAILED"),
    5: .same(proto: "INITIALIZING"),
    6: .same(proto: "RECOMMENDING"),
  ]
}

extension Google_Cloud_Dataproc_Logging_ScalingDecisionType: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "SCALING_DECISION_TYPE_UNSPECIFIED"),
    1: .same(proto: "SCALE_UP"),
    2: .same(proto: "SCALE_DOWN"),
    3: .same(proto: "NO_SCALE"),
    4: .same(proto: "MIXED"),
  ]
}

extension Google_Cloud_Dataproc_Logging_ConstrainingFactor: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "CONSTRAINING_FACTOR_UNSPECIFIED"),
    1: .same(proto: "SCALING_CAPPED_DUE_TO_LACK_OF_QUOTA"),
    2: .same(proto: "REACHED_MAXIMUM_CLUSTER_SIZE"),
    3: .same(proto: "REACHED_MINIMUM_CLUSTER_SIZE"),
  ]
}

extension Google_Cloud_Dataproc_Logging_ClusterSize: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ClusterSize"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "primary_worker_count"),
    2: .standard(proto: "secondary_worker_count"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularInt32Field(value: &self.primaryWorkerCount) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.secondaryWorkerCount) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.primaryWorkerCount != 0 {
      try visitor.visitSingularInt32Field(value: self.primaryWorkerCount, fieldNumber: 1)
    }
    if self.secondaryWorkerCount != 0 {
      try visitor.visitSingularInt32Field(value: self.secondaryWorkerCount, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_Logging_ClusterSize, rhs: Google_Cloud_Dataproc_Logging_ClusterSize) -> Bool {
    if lhs.primaryWorkerCount != rhs.primaryWorkerCount {return false}
    if lhs.secondaryWorkerCount != rhs.secondaryWorkerCount {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_Logging_AutoscalerLog: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AutoscalerLog"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "status"),
    2: .same(proto: "recommendation"),
  ]

  fileprivate class _StorageClass {
    var _status: Google_Cloud_Dataproc_Logging_AutoscalerStatus? = nil
    var _recommendation: Google_Cloud_Dataproc_Logging_AutoscalerRecommendation? = nil

    static let defaultInstance = _StorageClass()

    private init() {}

    init(copying source: _StorageClass) {
      _status = source._status
      _recommendation = source._recommendation
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularMessageField(value: &_storage._status) }()
        case 2: try { try decoder.decodeSingularMessageField(value: &_storage._recommendation) }()
        default: break
        }
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      if let v = _storage._status {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
      }
      if let v = _storage._recommendation {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_Logging_AutoscalerLog, rhs: Google_Cloud_Dataproc_Logging_AutoscalerLog) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._status != rhs_storage._status {return false}
        if _storage._recommendation != rhs_storage._recommendation {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_Logging_AutoscalerStatus: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AutoscalerStatus"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "state"),
    2: .same(proto: "details"),
    3: .standard(proto: "update_cluster_operation_id"),
    4: .same(proto: "error"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.state) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.details) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.updateClusterOperationID) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.error) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.state != .unspecified {
      try visitor.visitSingularEnumField(value: self.state, fieldNumber: 1)
    }
    if !self.details.isEmpty {
      try visitor.visitSingularStringField(value: self.details, fieldNumber: 2)
    }
    if !self.updateClusterOperationID.isEmpty {
      try visitor.visitSingularStringField(value: self.updateClusterOperationID, fieldNumber: 3)
    }
    if !self.error.isEmpty {
      try visitor.visitSingularStringField(value: self.error, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_Logging_AutoscalerStatus, rhs: Google_Cloud_Dataproc_Logging_AutoscalerStatus) -> Bool {
    if lhs.state != rhs.state {return false}
    if lhs.details != rhs.details {return false}
    if lhs.updateClusterOperationID != rhs.updateClusterOperationID {return false}
    if lhs.error != rhs.error {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_Logging_AutoscalerRecommendation: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AutoscalerRecommendation"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "inputs"),
    2: .same(proto: "outputs"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._inputs) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._outputs) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._inputs {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._outputs {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_Logging_AutoscalerRecommendation, rhs: Google_Cloud_Dataproc_Logging_AutoscalerRecommendation) -> Bool {
    if lhs._inputs != rhs._inputs {return false}
    if lhs._outputs != rhs._outputs {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_Logging_AutoscalerRecommendation.Inputs: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Dataproc_Logging_AutoscalerRecommendation.protoMessageName + ".Inputs"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "cluster_metrics"),
    2: .standard(proto: "current_cluster_size"),
    3: .standard(proto: "min_worker_counts"),
    4: .standard(proto: "max_worker_counts"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &self.clusterMetrics) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._currentClusterSize) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._minWorkerCounts) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._maxWorkerCounts) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.clusterMetrics.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: self.clusterMetrics, fieldNumber: 1)
    }
    if let v = self._currentClusterSize {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if let v = self._minWorkerCounts {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if let v = self._maxWorkerCounts {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_Logging_AutoscalerRecommendation.Inputs, rhs: Google_Cloud_Dataproc_Logging_AutoscalerRecommendation.Inputs) -> Bool {
    if lhs.clusterMetrics != rhs.clusterMetrics {return false}
    if lhs._currentClusterSize != rhs._currentClusterSize {return false}
    if lhs._minWorkerCounts != rhs._minWorkerCounts {return false}
    if lhs._maxWorkerCounts != rhs._maxWorkerCounts {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dataproc_Logging_AutoscalerRecommendation.Outputs: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = Google_Cloud_Dataproc_Logging_AutoscalerRecommendation.protoMessageName + ".Outputs"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "decision"),
    2: .standard(proto: "recommended_cluster_size"),
    3: .standard(proto: "graceful_decommission_timeout"),
    4: .standard(proto: "constraints_reached"),
    5: .standard(proto: "additional_recommendation_details"),
    6: .standard(proto: "recommendation_id"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.decision) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._recommendedClusterSize) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._gracefulDecommissionTimeout) }()
      case 4: try { try decoder.decodeRepeatedEnumField(value: &self.constraintsReached) }()
      case 5: try { try decoder.decodeRepeatedStringField(value: &self.additionalRecommendationDetails) }()
      case 6: try { try decoder.decodeSingularStringField(value: &self.recommendationID) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.decision != .unspecified {
      try visitor.visitSingularEnumField(value: self.decision, fieldNumber: 1)
    }
    if let v = self._recommendedClusterSize {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if let v = self._gracefulDecommissionTimeout {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    if !self.constraintsReached.isEmpty {
      try visitor.visitPackedEnumField(value: self.constraintsReached, fieldNumber: 4)
    }
    if !self.additionalRecommendationDetails.isEmpty {
      try visitor.visitRepeatedStringField(value: self.additionalRecommendationDetails, fieldNumber: 5)
    }
    if !self.recommendationID.isEmpty {
      try visitor.visitSingularStringField(value: self.recommendationID, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dataproc_Logging_AutoscalerRecommendation.Outputs, rhs: Google_Cloud_Dataproc_Logging_AutoscalerRecommendation.Outputs) -> Bool {
    if lhs.decision != rhs.decision {return false}
    if lhs._recommendedClusterSize != rhs._recommendedClusterSize {return false}
    if lhs._gracefulDecommissionTimeout != rhs._gracefulDecommissionTimeout {return false}
    if lhs.constraintsReached != rhs.constraintsReached {return false}
    if lhs.additionalRecommendationDetails != rhs.additionalRecommendationDetails {return false}
    if lhs.recommendationID != rhs.recommendationID {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
