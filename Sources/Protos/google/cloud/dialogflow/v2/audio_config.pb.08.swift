// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/cloud/dialogflow/v2/audio_config.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2020 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Audio encoding of the audio content sent in the conversational query request.
/// Refer to the
/// [Cloud Speech API
/// documentation](https://cloud.google.com/speech-to-text/docs/basics) for more
/// details.
public enum Google_Cloud_Dialogflow_V2_AudioEncoding: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Not specified.
  case unspecified // = 0

  /// Uncompressed 16-bit signed little-endian samples (Linear PCM).
  case linear16 // = 1

  /// [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
  /// Codec) is the recommended encoding because it is lossless (therefore
  /// recognition is not compromised) and requires only about half the
  /// bandwidth of `LINEAR16`. `FLAC` stream encoding supports 16-bit and
  /// 24-bit samples, however, not all fields in `STREAMINFO` are supported.
  case flac // = 2

  /// 8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.
  case mulaw // = 3

  /// Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.
  case amr // = 4

  /// Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.
  case amrWb // = 5

  /// Opus encoded audio frames in Ogg container
  /// ([OggOpus](https://wiki.xiph.org/OggOpus)).
  /// `sample_rate_hertz` must be 16000.
  case oggOpus // = 6

  /// Although the use of lossy encodings is not recommended, if a very low
  /// bitrate encoding is required, `OGG_OPUS` is highly preferred over
  /// Speex encoding. The [Speex](https://speex.org/) encoding supported by
  /// Dialogflow API has a header byte in each block, as in MIME type
  /// `audio/x-speex-with-header-byte`.
  /// It is a variant of the RTP Speex encoding defined in
  /// [RFC 5574](https://tools.ietf.org/html/rfc5574).
  /// The stream is a sequence of blocks, one block per RTP packet. Each block
  /// starts with a byte containing the length of the block, in bytes, followed
  /// by one or more frames of Speex data, padded to an integral number of
  /// bytes (octets) as specified in RFC 5574. In other words, each RTP header
  /// is replaced with a single byte containing the block length. Only Speex
  /// wideband is supported. `sample_rate_hertz` must be 16000.
  case speexWithHeaderByte // = 7
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .linear16
    case 2: self = .flac
    case 3: self = .mulaw
    case 4: self = .amr
    case 5: self = .amrWb
    case 6: self = .oggOpus
    case 7: self = .speexWithHeaderByte
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .linear16: return 1
    case .flac: return 2
    case .mulaw: return 3
    case .amr: return 4
    case .amrWb: return 5
    case .oggOpus: return 6
    case .speexWithHeaderByte: return 7
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Dialogflow_V2_AudioEncoding: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dialogflow_V2_AudioEncoding] = [
    .unspecified,
    .linear16,
    .flac,
    .mulaw,
    .amr,
    .amrWb,
    .oggOpus,
    .speexWithHeaderByte,
  ]
}

#endif  // swift(>=4.2)

/// Variant of the specified [Speech model][google.cloud.dialogflow.v2.InputAudioConfig.model] to use.
///
/// See the [Cloud Speech
/// documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
/// for which models have different variants. For example, the "phone_call" model
/// has both a standard and an enhanced variant. When you use an enhanced model,
/// you will generally receive higher quality results than for a standard model.
public enum Google_Cloud_Dialogflow_V2_SpeechModelVariant: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// No model variant specified. In this case Dialogflow defaults to
  /// USE_BEST_AVAILABLE.
  case unspecified // = 0

  /// Use the best available variant of the [Speech
  /// model][InputAudioConfig.model] that the caller is eligible for.
  ///
  /// Please see the [Dialogflow
  /// docs](https://cloud.google.com/dialogflow/docs/data-logging) for
  /// how to make your project eligible for enhanced models.
  case useBestAvailable // = 1

  /// Use standard model variant even if an enhanced model is available.  See the
  /// [Cloud Speech
  /// documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
  /// for details about enhanced models.
  case useStandard // = 2

  /// Use an enhanced model variant:
  ///
  /// * If an enhanced variant does not exist for the given
  ///   [model][google.cloud.dialogflow.v2.InputAudioConfig.model] and request language, Dialogflow falls
  ///   back to the standard variant.
  ///
  ///   The [Cloud Speech
  ///   documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
  ///   describes which models have enhanced variants.
  ///
  /// * If the API caller isn't eligible for enhanced models, Dialogflow returns
  ///   an error. Please see the [Dialogflow
  ///   docs](https://cloud.google.com/dialogflow/docs/data-logging)
  ///   for how to make your project eligible.
  case useEnhanced // = 3
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .useBestAvailable
    case 2: self = .useStandard
    case 3: self = .useEnhanced
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .useBestAvailable: return 1
    case .useStandard: return 2
    case .useEnhanced: return 3
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Dialogflow_V2_SpeechModelVariant: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dialogflow_V2_SpeechModelVariant] = [
    .unspecified,
    .useBestAvailable,
    .useStandard,
    .useEnhanced,
  ]
}

#endif  // swift(>=4.2)

/// Gender of the voice as described in
/// [SSML voice element](https://www.w3.org/TR/speech-synthesis11/#edef_voice).
public enum Google_Cloud_Dialogflow_V2_SsmlVoiceGender: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// An unspecified gender, which means that the client doesn't care which
  /// gender the selected voice will have.
  case unspecified // = 0

  /// A male voice.
  case male // = 1

  /// A female voice.
  case female // = 2

  /// A gender-neutral voice.
  case neutral // = 3
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .male
    case 2: self = .female
    case 3: self = .neutral
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .male: return 1
    case .female: return 2
    case .neutral: return 3
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Dialogflow_V2_SsmlVoiceGender: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dialogflow_V2_SsmlVoiceGender] = [
    .unspecified,
    .male,
    .female,
    .neutral,
  ]
}

#endif  // swift(>=4.2)

/// Audio encoding of the output audio format in Text-To-Speech.
public enum Google_Cloud_Dialogflow_V2_OutputAudioEncoding: SwiftProtobuf.Enum {
  public typealias RawValue = Int

  /// Not specified.
  case unspecified // = 0

  /// Uncompressed 16-bit signed little-endian samples (Linear PCM).
  /// Audio content returned as LINEAR16 also contains a WAV header.
  case linear16 // = 1

  /// MP3 audio at 32kbps.
  case mp3 // = 2

  /// Opus encoded audio wrapped in an ogg container. The result will be a
  /// file which can be played natively on Android, and in browsers (at least
  /// Chrome and Firefox). The quality of the encoding is considerably higher
  /// than MP3 while using approximately the same bitrate.
  case oggOpus // = 3
  case UNRECOGNIZED(Int)

  public init() {
    self = .unspecified
  }

  public init?(rawValue: Int) {
    switch rawValue {
    case 0: self = .unspecified
    case 1: self = .linear16
    case 2: self = .mp3
    case 3: self = .oggOpus
    default: self = .UNRECOGNIZED(rawValue)
    }
  }

  public var rawValue: Int {
    switch self {
    case .unspecified: return 0
    case .linear16: return 1
    case .mp3: return 2
    case .oggOpus: return 3
    case .UNRECOGNIZED(let i): return i
    }
  }

}

#if swift(>=4.2)

extension Google_Cloud_Dialogflow_V2_OutputAudioEncoding: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Cloud_Dialogflow_V2_OutputAudioEncoding] = [
    .unspecified,
    .linear16,
    .mp3,
    .oggOpus,
  ]
}

#endif  // swift(>=4.2)

/// Hints for the speech recognizer to help with recognition in a specific
/// conversation state.
public struct Google_Cloud_Dialogflow_V2_SpeechContext {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. A list of strings containing words and phrases that the speech
  /// recognizer should recognize with higher likelihood.
  ///
  /// This list can be used to:
  ///
  /// * improve accuracy for words and phrases you expect the user to say,
  ///   e.g. typical commands for your Dialogflow agent
  /// * add additional words to the speech recognizer vocabulary
  /// * ...
  ///
  /// See the [Cloud Speech
  /// documentation](https://cloud.google.com/speech-to-text/quotas) for usage
  /// limits.
  public var phrases: [String] = []

  /// Optional. Boost for this context compared to other contexts:
  ///
  /// * If the boost is positive, Dialogflow will increase the probability that
  ///   the phrases in this context are recognized over similar sounding phrases.
  /// * If the boost is unspecified or non-positive, Dialogflow will not apply
  ///   any boost.
  ///
  /// Dialogflow recommends that you use boosts in the range (0, 20] and that you
  /// find a value that fits your use case with binary search.
  public var boost: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Information for a word recognized by the speech recognizer.
public struct Google_Cloud_Dialogflow_V2_SpeechWordInfo {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The word this info is for.
  public var word: String = String()

  /// Time offset relative to the beginning of the audio that corresponds to the
  /// start of the spoken word. This is an experimental feature and the accuracy
  /// of the time offset can vary.
  public var startOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _startOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_startOffset = newValue}
  }
  /// Returns true if `startOffset` has been explicitly set.
  public var hasStartOffset: Bool {return self._startOffset != nil}
  /// Clears the value of `startOffset`. Subsequent reads from it will return its default value.
  public mutating func clearStartOffset() {self._startOffset = nil}

  /// Time offset relative to the beginning of the audio that corresponds to the
  /// end of the spoken word. This is an experimental feature and the accuracy of
  /// the time offset can vary.
  public var endOffset: SwiftProtobuf.Google_Protobuf_Duration {
    get {return _endOffset ?? SwiftProtobuf.Google_Protobuf_Duration()}
    set {_endOffset = newValue}
  }
  /// Returns true if `endOffset` has been explicitly set.
  public var hasEndOffset: Bool {return self._endOffset != nil}
  /// Clears the value of `endOffset`. Subsequent reads from it will return its default value.
  public mutating func clearEndOffset() {self._endOffset = nil}

  /// The Speech confidence between 0.0 and 1.0 for this word. A higher number
  /// indicates an estimated greater likelihood that the recognized word is
  /// correct. The default of 0.0 is a sentinel value indicating that confidence
  /// was not set.
  ///
  /// This field is not guaranteed to be fully stable over time for the same
  /// audio input. Users should also not rely on it to always be provided.
  public var confidence: Float = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _startOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
  fileprivate var _endOffset: SwiftProtobuf.Google_Protobuf_Duration? = nil
}

/// Instructs the speech recognizer how to process the audio content.
public struct Google_Cloud_Dialogflow_V2_InputAudioConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Audio encoding of the audio content to process.
  public var audioEncoding: Google_Cloud_Dialogflow_V2_AudioEncoding = .unspecified

  /// Required. Sample rate (in Hertz) of the audio content sent in the query.
  /// Refer to
  /// [Cloud Speech API
  /// documentation](https://cloud.google.com/speech-to-text/docs/basics) for
  /// more details.
  public var sampleRateHertz: Int32 = 0

  /// Required. The language of the supplied audio. Dialogflow does not do
  /// translations. See [Language
  /// Support](https://cloud.google.com/dialogflow/docs/reference/language)
  /// for a list of the currently supported language codes. Note that queries in
  /// the same session do not necessarily need to specify the same language.
  public var languageCode: String = String()

  /// If `true`, Dialogflow returns [SpeechWordInfo][google.cloud.dialogflow.v2.SpeechWordInfo] in
  /// [StreamingRecognitionResult][google.cloud.dialogflow.v2.StreamingRecognitionResult] with information about the recognized speech
  /// words, e.g. start and end time offsets. If false or unspecified, Speech
  /// doesn't return any word-level information.
  public var enableWordInfo: Bool = false

  /// A list of strings containing words and phrases that the speech
  /// recognizer should recognize with higher likelihood.
  ///
  /// See [the Cloud Speech
  /// documentation](https://cloud.google.com/speech-to-text/docs/basics#phrase-hints)
  /// for more details.
  ///
  /// This field is deprecated. Please use [speech_contexts]() instead. If you
  /// specify both [phrase_hints]() and [speech_contexts](), Dialogflow will
  /// treat the [phrase_hints]() as a single additional [SpeechContext]().
  public var phraseHints: [String] = []

  /// Context information to assist speech recognition.
  ///
  /// See [the Cloud Speech
  /// documentation](https://cloud.google.com/speech-to-text/docs/basics#phrase-hints)
  /// for more details.
  public var speechContexts: [Google_Cloud_Dialogflow_V2_SpeechContext] = []

  /// Which Speech model to select for the given request. Select the
  /// model best suited to your domain to get best results. If a model is not
  /// explicitly specified, then we auto-select a model based on the parameters
  /// in the InputAudioConfig.
  /// If enhanced speech model is enabled for the agent and an enhanced
  /// version of the specified model for the language does not exist, then the
  /// speech is recognized using the standard version of the specified model.
  /// Refer to
  /// [Cloud Speech API
  /// documentation](https://cloud.google.com/speech-to-text/docs/basics#select-model)
  /// for more details.
  public var model: String = String()

  /// Which variant of the [Speech model][google.cloud.dialogflow.v2.InputAudioConfig.model] to use.
  public var modelVariant: Google_Cloud_Dialogflow_V2_SpeechModelVariant = .unspecified

  /// If `false` (default), recognition does not cease until the
  /// client closes the stream.
  /// If `true`, the recognizer will detect a single spoken utterance in input
  /// audio. Recognition ceases when it detects the audio's voice has
  /// stopped or paused. In this case, once a detected intent is received, the
  /// client should close the stream and start a new request with a new stream as
  /// needed.
  /// Note: This setting is relevant only for streaming methods.
  /// Note: When specified, InputAudioConfig.single_utterance takes precedence
  /// over StreamingDetectIntentRequest.single_utterance.
  public var singleUtterance: Bool = false

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Description of which voice to use for speech synthesis.
public struct Google_Cloud_Dialogflow_V2_VoiceSelectionParams {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. The name of the voice. If not set, the service will choose a
  /// voice based on the other parameters such as language_code and
  /// [ssml_gender][google.cloud.dialogflow.v2.VoiceSelectionParams.ssml_gender].
  public var name: String = String()

  /// Optional. The preferred gender of the voice. If not set, the service will
  /// choose a voice based on the other parameters such as language_code and
  /// [name][google.cloud.dialogflow.v2.VoiceSelectionParams.name]. Note that this is only a preference, not requirement. If a
  /// voice of the appropriate gender is not available, the synthesizer should
  /// substitute a voice with a different gender rather than failing the request.
  public var ssmlGender: Google_Cloud_Dialogflow_V2_SsmlVoiceGender = .unspecified

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// Configuration of how speech should be synthesized.
public struct Google_Cloud_Dialogflow_V2_SynthesizeSpeechConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Optional. Speaking rate/speed, in the range [0.25, 4.0]. 1.0 is the normal
  /// native speed supported by the specific voice. 2.0 is twice as fast, and
  /// 0.5 is half as fast. If unset(0.0), defaults to the native 1.0 speed. Any
  /// other values < 0.25 or > 4.0 will return an error.
  public var speakingRate: Double = 0

  /// Optional. Speaking pitch, in the range [-20.0, 20.0]. 20 means increase 20
  /// semitones from the original pitch. -20 means decrease 20 semitones from the
  /// original pitch.
  public var pitch: Double = 0

  /// Optional. Volume gain (in dB) of the normal native volume supported by the
  /// specific voice, in the range [-96.0, 16.0]. If unset, or set to a value of
  /// 0.0 (dB), will play at normal native signal amplitude. A value of -6.0 (dB)
  /// will play at approximately half the amplitude of the normal native signal
  /// amplitude. A value of +6.0 (dB) will play at approximately twice the
  /// amplitude of the normal native signal amplitude. We strongly recommend not
  /// to exceed +10 (dB) as there's usually no effective increase in loudness for
  /// any value greater than that.
  public var volumeGainDb: Double = 0

  /// Optional. An identifier which selects 'audio effects' profiles that are
  /// applied on (post synthesized) text to speech. Effects are applied on top of
  /// each other in the order they are given.
  public var effectsProfileID: [String] = []

  /// Optional. The desired voice of the synthesized audio.
  public var voice: Google_Cloud_Dialogflow_V2_VoiceSelectionParams {
    get {return _voice ?? Google_Cloud_Dialogflow_V2_VoiceSelectionParams()}
    set {_voice = newValue}
  }
  /// Returns true if `voice` has been explicitly set.
  public var hasVoice: Bool {return self._voice != nil}
  /// Clears the value of `voice`. Subsequent reads from it will return its default value.
  public mutating func clearVoice() {self._voice = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _voice: Google_Cloud_Dialogflow_V2_VoiceSelectionParams? = nil
}

/// Instructs the speech synthesizer on how to generate the output audio content.
/// If this audio config is supplied in a request, it overrides all existing
/// text-to-speech settings applied to the agent.
public struct Google_Cloud_Dialogflow_V2_OutputAudioConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Required. Audio encoding of the synthesized audio content.
  public var audioEncoding: Google_Cloud_Dialogflow_V2_OutputAudioEncoding = .unspecified

  /// The synthesis sample rate (in hertz) for this audio. If not
  /// provided, then the synthesizer will use the default sample rate based on
  /// the audio encoding. If this is different from the voice's natural sample
  /// rate, then the synthesizer will honor this request by converting to the
  /// desired sample rate (which might result in worse audio quality).
  public var sampleRateHertz: Int32 = 0

  /// Configuration of how speech should be synthesized.
  public var synthesizeSpeechConfig: Google_Cloud_Dialogflow_V2_SynthesizeSpeechConfig {
    get {return _synthesizeSpeechConfig ?? Google_Cloud_Dialogflow_V2_SynthesizeSpeechConfig()}
    set {_synthesizeSpeechConfig = newValue}
  }
  /// Returns true if `synthesizeSpeechConfig` has been explicitly set.
  public var hasSynthesizeSpeechConfig: Bool {return self._synthesizeSpeechConfig != nil}
  /// Clears the value of `synthesizeSpeechConfig`. Subsequent reads from it will return its default value.
  public mutating func clearSynthesizeSpeechConfig() {self._synthesizeSpeechConfig = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _synthesizeSpeechConfig: Google_Cloud_Dialogflow_V2_SynthesizeSpeechConfig? = nil
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.cloud.dialogflow.v2"

extension Google_Cloud_Dialogflow_V2_AudioEncoding: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "AUDIO_ENCODING_UNSPECIFIED"),
    1: .same(proto: "AUDIO_ENCODING_LINEAR_16"),
    2: .same(proto: "AUDIO_ENCODING_FLAC"),
    3: .same(proto: "AUDIO_ENCODING_MULAW"),
    4: .same(proto: "AUDIO_ENCODING_AMR"),
    5: .same(proto: "AUDIO_ENCODING_AMR_WB"),
    6: .same(proto: "AUDIO_ENCODING_OGG_OPUS"),
    7: .same(proto: "AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE"),
  ]
}

extension Google_Cloud_Dialogflow_V2_SpeechModelVariant: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "SPEECH_MODEL_VARIANT_UNSPECIFIED"),
    1: .same(proto: "USE_BEST_AVAILABLE"),
    2: .same(proto: "USE_STANDARD"),
    3: .same(proto: "USE_ENHANCED"),
  ]
}

extension Google_Cloud_Dialogflow_V2_SsmlVoiceGender: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "SSML_VOICE_GENDER_UNSPECIFIED"),
    1: .same(proto: "SSML_VOICE_GENDER_MALE"),
    2: .same(proto: "SSML_VOICE_GENDER_FEMALE"),
    3: .same(proto: "SSML_VOICE_GENDER_NEUTRAL"),
  ]
}

extension Google_Cloud_Dialogflow_V2_OutputAudioEncoding: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "OUTPUT_AUDIO_ENCODING_UNSPECIFIED"),
    1: .same(proto: "OUTPUT_AUDIO_ENCODING_LINEAR_16"),
    2: .same(proto: "OUTPUT_AUDIO_ENCODING_MP3"),
    3: .same(proto: "OUTPUT_AUDIO_ENCODING_OGG_OPUS"),
  ]
}

extension Google_Cloud_Dialogflow_V2_SpeechContext: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechContext"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "phrases"),
    2: .same(proto: "boost"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedStringField(value: &self.phrases) }()
      case 2: try { try decoder.decodeSingularFloatField(value: &self.boost) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.phrases.isEmpty {
      try visitor.visitRepeatedStringField(value: self.phrases, fieldNumber: 1)
    }
    if self.boost != 0 {
      try visitor.visitSingularFloatField(value: self.boost, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dialogflow_V2_SpeechContext, rhs: Google_Cloud_Dialogflow_V2_SpeechContext) -> Bool {
    if lhs.phrases != rhs.phrases {return false}
    if lhs.boost != rhs.boost {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dialogflow_V2_SpeechWordInfo: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SpeechWordInfo"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    3: .same(proto: "word"),
    1: .standard(proto: "start_offset"),
    2: .standard(proto: "end_offset"),
    4: .same(proto: "confidence"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._startOffset) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._endOffset) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.word) }()
      case 4: try { try decoder.decodeSingularFloatField(value: &self.confidence) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._startOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._endOffset {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if !self.word.isEmpty {
      try visitor.visitSingularStringField(value: self.word, fieldNumber: 3)
    }
    if self.confidence != 0 {
      try visitor.visitSingularFloatField(value: self.confidence, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dialogflow_V2_SpeechWordInfo, rhs: Google_Cloud_Dialogflow_V2_SpeechWordInfo) -> Bool {
    if lhs.word != rhs.word {return false}
    if lhs._startOffset != rhs._startOffset {return false}
    if lhs._endOffset != rhs._endOffset {return false}
    if lhs.confidence != rhs.confidence {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dialogflow_V2_InputAudioConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".InputAudioConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "audio_encoding"),
    2: .standard(proto: "sample_rate_hertz"),
    3: .standard(proto: "language_code"),
    13: .standard(proto: "enable_word_info"),
    4: .standard(proto: "phrase_hints"),
    11: .standard(proto: "speech_contexts"),
    7: .same(proto: "model"),
    10: .standard(proto: "model_variant"),
    8: .standard(proto: "single_utterance"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.audioEncoding) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.sampleRateHertz) }()
      case 3: try { try decoder.decodeSingularStringField(value: &self.languageCode) }()
      case 4: try { try decoder.decodeRepeatedStringField(value: &self.phraseHints) }()
      case 7: try { try decoder.decodeSingularStringField(value: &self.model) }()
      case 8: try { try decoder.decodeSingularBoolField(value: &self.singleUtterance) }()
      case 10: try { try decoder.decodeSingularEnumField(value: &self.modelVariant) }()
      case 11: try { try decoder.decodeRepeatedMessageField(value: &self.speechContexts) }()
      case 13: try { try decoder.decodeSingularBoolField(value: &self.enableWordInfo) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.audioEncoding != .unspecified {
      try visitor.visitSingularEnumField(value: self.audioEncoding, fieldNumber: 1)
    }
    if self.sampleRateHertz != 0 {
      try visitor.visitSingularInt32Field(value: self.sampleRateHertz, fieldNumber: 2)
    }
    if !self.languageCode.isEmpty {
      try visitor.visitSingularStringField(value: self.languageCode, fieldNumber: 3)
    }
    if !self.phraseHints.isEmpty {
      try visitor.visitRepeatedStringField(value: self.phraseHints, fieldNumber: 4)
    }
    if !self.model.isEmpty {
      try visitor.visitSingularStringField(value: self.model, fieldNumber: 7)
    }
    if self.singleUtterance != false {
      try visitor.visitSingularBoolField(value: self.singleUtterance, fieldNumber: 8)
    }
    if self.modelVariant != .unspecified {
      try visitor.visitSingularEnumField(value: self.modelVariant, fieldNumber: 10)
    }
    if !self.speechContexts.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.speechContexts, fieldNumber: 11)
    }
    if self.enableWordInfo != false {
      try visitor.visitSingularBoolField(value: self.enableWordInfo, fieldNumber: 13)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dialogflow_V2_InputAudioConfig, rhs: Google_Cloud_Dialogflow_V2_InputAudioConfig) -> Bool {
    if lhs.audioEncoding != rhs.audioEncoding {return false}
    if lhs.sampleRateHertz != rhs.sampleRateHertz {return false}
    if lhs.languageCode != rhs.languageCode {return false}
    if lhs.enableWordInfo != rhs.enableWordInfo {return false}
    if lhs.phraseHints != rhs.phraseHints {return false}
    if lhs.speechContexts != rhs.speechContexts {return false}
    if lhs.model != rhs.model {return false}
    if lhs.modelVariant != rhs.modelVariant {return false}
    if lhs.singleUtterance != rhs.singleUtterance {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dialogflow_V2_VoiceSelectionParams: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".VoiceSelectionParams"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .standard(proto: "ssml_gender"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeSingularEnumField(value: &self.ssmlGender) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if self.ssmlGender != .unspecified {
      try visitor.visitSingularEnumField(value: self.ssmlGender, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dialogflow_V2_VoiceSelectionParams, rhs: Google_Cloud_Dialogflow_V2_VoiceSelectionParams) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.ssmlGender != rhs.ssmlGender {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dialogflow_V2_SynthesizeSpeechConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".SynthesizeSpeechConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "speaking_rate"),
    2: .same(proto: "pitch"),
    3: .standard(proto: "volume_gain_db"),
    5: .standard(proto: "effects_profile_id"),
    4: .same(proto: "voice"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularDoubleField(value: &self.speakingRate) }()
      case 2: try { try decoder.decodeSingularDoubleField(value: &self.pitch) }()
      case 3: try { try decoder.decodeSingularDoubleField(value: &self.volumeGainDb) }()
      case 4: try { try decoder.decodeSingularMessageField(value: &self._voice) }()
      case 5: try { try decoder.decodeRepeatedStringField(value: &self.effectsProfileID) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.speakingRate != 0 {
      try visitor.visitSingularDoubleField(value: self.speakingRate, fieldNumber: 1)
    }
    if self.pitch != 0 {
      try visitor.visitSingularDoubleField(value: self.pitch, fieldNumber: 2)
    }
    if self.volumeGainDb != 0 {
      try visitor.visitSingularDoubleField(value: self.volumeGainDb, fieldNumber: 3)
    }
    if let v = self._voice {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
    }
    if !self.effectsProfileID.isEmpty {
      try visitor.visitRepeatedStringField(value: self.effectsProfileID, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dialogflow_V2_SynthesizeSpeechConfig, rhs: Google_Cloud_Dialogflow_V2_SynthesizeSpeechConfig) -> Bool {
    if lhs.speakingRate != rhs.speakingRate {return false}
    if lhs.pitch != rhs.pitch {return false}
    if lhs.volumeGainDb != rhs.volumeGainDb {return false}
    if lhs.effectsProfileID != rhs.effectsProfileID {return false}
    if lhs._voice != rhs._voice {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Cloud_Dialogflow_V2_OutputAudioConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".OutputAudioConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "audio_encoding"),
    2: .standard(proto: "sample_rate_hertz"),
    3: .standard(proto: "synthesize_speech_config"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.audioEncoding) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.sampleRateHertz) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._synthesizeSpeechConfig) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.audioEncoding != .unspecified {
      try visitor.visitSingularEnumField(value: self.audioEncoding, fieldNumber: 1)
    }
    if self.sampleRateHertz != 0 {
      try visitor.visitSingularInt32Field(value: self.sampleRateHertz, fieldNumber: 2)
    }
    if let v = self._synthesizeSpeechConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Cloud_Dialogflow_V2_OutputAudioConfig, rhs: Google_Cloud_Dialogflow_V2_OutputAudioConfig) -> Bool {
    if lhs.audioEncoding != rhs.audioEncoding {return false}
    if lhs.sampleRateHertz != rhs.sampleRateHertz {return false}
    if lhs._synthesizeSpeechConfig != rhs._synthesizeSpeechConfig {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
