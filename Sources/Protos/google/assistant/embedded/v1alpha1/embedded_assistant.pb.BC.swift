// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: google/assistant/embedded/v1alpha1/embedded_assistant.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

// Copyright 2017 Google Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

/// Specifies how to process the `ConverseRequest` messages.
public struct Google_Assistant_Embedded_V1alpha1_ConverseConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Required* Specifies how to process the subsequent incoming audio.
  public var audioInConfig: Google_Assistant_Embedded_V1alpha1_AudioInConfig {
    get {return _audioInConfig ?? Google_Assistant_Embedded_V1alpha1_AudioInConfig()}
    set {_audioInConfig = newValue}
  }
  /// Returns true if `audioInConfig` has been explicitly set.
  public var hasAudioInConfig: Bool {return self._audioInConfig != nil}
  /// Clears the value of `audioInConfig`. Subsequent reads from it will return its default value.
  public mutating func clearAudioInConfig() {self._audioInConfig = nil}

  /// *Required* Specifies how to format the audio that will be returned.
  public var audioOutConfig: Google_Assistant_Embedded_V1alpha1_AudioOutConfig {
    get {return _audioOutConfig ?? Google_Assistant_Embedded_V1alpha1_AudioOutConfig()}
    set {_audioOutConfig = newValue}
  }
  /// Returns true if `audioOutConfig` has been explicitly set.
  public var hasAudioOutConfig: Bool {return self._audioOutConfig != nil}
  /// Clears the value of `audioOutConfig`. Subsequent reads from it will return its default value.
  public mutating func clearAudioOutConfig() {self._audioOutConfig = nil}

  /// *Required* Represents the current dialog state.
  public var converseState: Google_Assistant_Embedded_V1alpha1_ConverseState {
    get {return _converseState ?? Google_Assistant_Embedded_V1alpha1_ConverseState()}
    set {_converseState = newValue}
  }
  /// Returns true if `converseState` has been explicitly set.
  public var hasConverseState: Bool {return self._converseState != nil}
  /// Clears the value of `converseState`. Subsequent reads from it will return its default value.
  public mutating func clearConverseState() {self._converseState = nil}

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}

  fileprivate var _audioInConfig: Google_Assistant_Embedded_V1alpha1_AudioInConfig? = nil
  fileprivate var _audioOutConfig: Google_Assistant_Embedded_V1alpha1_AudioOutConfig? = nil
  fileprivate var _converseState: Google_Assistant_Embedded_V1alpha1_ConverseState? = nil
}

/// Specifies how to process the `audio_in` data that will be provided in
/// subsequent requests. For recommended settings, see the Google Assistant SDK
/// [best
/// practices](https://developers.google.com/assistant/sdk/develop/grpc/best-practices/audio).
public struct Google_Assistant_Embedded_V1alpha1_AudioInConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Required* Encoding of audio data sent in all `audio_in` messages.
  public var encoding: Google_Assistant_Embedded_V1alpha1_AudioInConfig.Encoding = .unspecified

  /// *Required* Sample rate (in Hertz) of the audio data sent in all `audio_in`
  /// messages. Valid values are from 16000-24000, but 16000 is optimal.
  /// For best results, set the sampling rate of the audio source to 16000 Hz.
  /// If that's not possible, use the native sample rate of the audio source
  /// (instead of re-sampling).
  public var sampleRateHertz: Int32 = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Audio encoding of the data sent in the audio message.
  /// Audio must be one-channel (mono). The only language supported is "en-US".
  public enum Encoding: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][].
    case unspecified // = 0

    /// Uncompressed 16-bit signed little-endian samples (Linear PCM).
    /// This encoding includes no header, only the raw audio bytes.
    case linear16 // = 1

    /// [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
    /// Codec) is the recommended encoding because it is
    /// lossless--therefore recognition is not compromised--and
    /// requires only about half the bandwidth of `LINEAR16`. This encoding
    /// includes the `FLAC` stream header followed by audio data. It supports
    /// 16-bit and 24-bit samples, however, not all fields in `STREAMINFO` are
    /// supported.
    case flac // = 2
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .linear16
      case 2: self = .flac
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .linear16: return 1
      case .flac: return 2
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}
}

#if swift(>=4.2)

extension Google_Assistant_Embedded_V1alpha1_AudioInConfig.Encoding: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Assistant_Embedded_V1alpha1_AudioInConfig.Encoding] = [
    .unspecified,
    .linear16,
    .flac,
  ]
}

#endif  // swift(>=4.2)

/// Specifies the desired format for the server to use when it returns
/// `audio_out` messages.
public struct Google_Assistant_Embedded_V1alpha1_AudioOutConfig {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Required* The encoding of audio data to be returned in all `audio_out`
  /// messages.
  public var encoding: Google_Assistant_Embedded_V1alpha1_AudioOutConfig.Encoding = .unspecified

  /// *Required* The sample rate in Hertz of the audio data returned in
  /// `audio_out` messages. Valid values are: 16000-24000.
  public var sampleRateHertz: Int32 = 0

  /// *Required* Current volume setting of the device's audio output.
  /// Valid values are 1 to 100 (corresponding to 1% to 100%).
  public var volumePercentage: Int32 = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Audio encoding of the data returned in the audio message. All encodings are
  /// raw audio bytes with no header, except as indicated below.
  public enum Encoding: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][].
    case unspecified // = 0

    /// Uncompressed 16-bit signed little-endian samples (Linear PCM).
    case linear16 // = 1

    /// MP3 audio encoding. The sample rate is encoded in the payload.
    case mp3 // = 2

    /// Opus-encoded audio wrapped in an ogg container. The result will be a
    /// file which can be played natively on Android and in some browsers (such
    /// as Chrome). The quality of the encoding is considerably higher than MP3
    /// while using the same bitrate. The sample rate is encoded in the payload.
    case opusInOgg // = 3
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .linear16
      case 2: self = .mp3
      case 3: self = .opusInOgg
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .linear16: return 1
      case .mp3: return 2
      case .opusInOgg: return 3
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}
}

#if swift(>=4.2)

extension Google_Assistant_Embedded_V1alpha1_AudioOutConfig.Encoding: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Assistant_Embedded_V1alpha1_AudioOutConfig.Encoding] = [
    .unspecified,
    .linear16,
    .mp3,
    .opusInOgg,
  ]
}

#endif  // swift(>=4.2)

/// Provides information about the current dialog state.
public struct Google_Assistant_Embedded_V1alpha1_ConverseState {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Required* The `conversation_state` value returned in the prior
  /// `ConverseResponse`. Omit (do not set the field) if there was no prior
  /// `ConverseResponse`. If there was a prior `ConverseResponse`, do not omit
  /// this field; doing so will end that conversation (and this new request will
  /// start a new conversation).
  public var conversationState: Data = Data()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// The audio containing the assistant's response to the query. Sequential chunks
/// of audio data are received in sequential `ConverseResponse` messages.
public struct Google_Assistant_Embedded_V1alpha1_AudioOut {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* The audio data containing the assistant's response to the
  /// query. Sequential chunks of audio data are received in sequential
  /// `ConverseResponse` messages.
  public var audioData: Data = Data()

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  public init() {}
}

/// The semantic result for the user's spoken query.
public struct Google_Assistant_Embedded_V1alpha1_ConverseResult {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// *Output-only* The recognized transcript of what the user said.
  public var spokenRequestText: String = String()

  /// *Output-only* The text of the assistant's spoken response. This is only
  /// returned for an IFTTT action.
  public var spokenResponseText: String = String()

  /// *Output-only* State information for subsequent `ConverseRequest`. This
  /// value should be saved in the client and returned in the
  /// `conversation_state` with the next `ConverseRequest`. (The client does not
  /// need to interpret or otherwise use this value.) There is no need to save
  /// this information across device restarts.
  public var conversationState: Data = Data()

  /// *Output-only* Specifies the mode of the microphone after this `Converse`
  /// RPC is processed.
  public var microphoneMode: Google_Assistant_Embedded_V1alpha1_ConverseResult.MicrophoneMode = .unspecified

  /// *Output-only* Updated volume level. The value will be 0 or omitted
  /// (indicating no change) unless a voice command such as "Increase the volume"
  /// or "Set volume level 4" was recognized, in which case the value will be
  /// between 1 and 100 (corresponding to the new volume level of 1% to 100%).
  /// Typically, a client should use this volume level when playing the
  /// `audio_out` data, and retain this value as the current volume level and
  /// supply it in the `AudioOutConfig` of the next `ConverseRequest`. (Some
  /// clients may also implement other ways to allow the current volume level to
  /// be changed, for example, by providing a knob that the user can turn.)
  public var volumePercentage: Int32 = 0

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Possible states of the microphone after a `Converse` RPC completes.
  public enum MicrophoneMode: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// No mode specified.
    case unspecified // = 0

    /// The service is not expecting a follow-on question from the user.
    /// The microphone should remain off until the user re-activates it.
    case closeMicrophone // = 1

    /// The service is expecting a follow-on question from the user. The
    /// microphone should be re-opened when the `AudioOut` playback completes
    /// (by starting a new `Converse` RPC call to send the new audio).
    case dialogFollowOn // = 2
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .closeMicrophone
      case 2: self = .dialogFollowOn
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .closeMicrophone: return 1
      case .dialogFollowOn: return 2
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}
}

#if swift(>=4.2)

extension Google_Assistant_Embedded_V1alpha1_ConverseResult.MicrophoneMode: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Assistant_Embedded_V1alpha1_ConverseResult.MicrophoneMode] = [
    .unspecified,
    .closeMicrophone,
    .dialogFollowOn,
  ]
}

#endif  // swift(>=4.2)

/// The top-level message sent by the client. Clients must send at least two, and
/// typically numerous `ConverseRequest` messages. The first message must
/// contain a `config` message and must not contain `audio_in` data. All
/// subsequent messages must contain `audio_in` data and must not contain a
/// `config` message.
public struct Google_Assistant_Embedded_V1alpha1_ConverseRequest {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Exactly one of these fields must be specified in each `ConverseRequest`.
  public var converseRequest: Google_Assistant_Embedded_V1alpha1_ConverseRequest.OneOf_ConverseRequest? = nil

  /// The `config` message provides information to the recognizer that
  /// specifies how to process the request.
  /// The first `ConverseRequest` message must contain a `config` message.
  public var config: Google_Assistant_Embedded_V1alpha1_ConverseConfig {
    get {
      if case .config(let v)? = converseRequest {return v}
      return Google_Assistant_Embedded_V1alpha1_ConverseConfig()
    }
    set {converseRequest = .config(newValue)}
  }

  /// The audio data to be recognized. Sequential chunks of audio data are sent
  /// in sequential `ConverseRequest` messages. The first `ConverseRequest`
  /// message must not contain `audio_in` data and all subsequent
  /// `ConverseRequest` messages must contain `audio_in` data. The audio bytes
  /// must be encoded as specified in `AudioInConfig`.
  /// Audio must be sent at approximately real-time (16000 samples per second).
  /// An error will be returned if audio is sent significantly faster or
  /// slower.
  public var audioIn: Data {
    get {
      if case .audioIn(let v)? = converseRequest {return v}
      return Data()
    }
    set {converseRequest = .audioIn(newValue)}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Exactly one of these fields must be specified in each `ConverseRequest`.
  public enum OneOf_ConverseRequest: Equatable {
    /// The `config` message provides information to the recognizer that
    /// specifies how to process the request.
    /// The first `ConverseRequest` message must contain a `config` message.
    case config(Google_Assistant_Embedded_V1alpha1_ConverseConfig)
    /// The audio data to be recognized. Sequential chunks of audio data are sent
    /// in sequential `ConverseRequest` messages. The first `ConverseRequest`
    /// message must not contain `audio_in` data and all subsequent
    /// `ConverseRequest` messages must contain `audio_in` data. The audio bytes
    /// must be encoded as specified in `AudioInConfig`.
    /// Audio must be sent at approximately real-time (16000 samples per second).
    /// An error will be returned if audio is sent significantly faster or
    /// slower.
    case audioIn(Data)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Assistant_Embedded_V1alpha1_ConverseRequest.OneOf_ConverseRequest, rhs: Google_Assistant_Embedded_V1alpha1_ConverseRequest.OneOf_ConverseRequest) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.config, .config): return {
        guard case .config(let l) = lhs, case .config(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.audioIn, .audioIn): return {
        guard case .audioIn(let l) = lhs, case .audioIn(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  public init() {}
}

/// The top-level message received by the client. A series of one or more
/// `ConverseResponse` messages are streamed back to the client.
public struct Google_Assistant_Embedded_V1alpha1_ConverseResponse {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Exactly one of these fields will be populated in each `ConverseResponse`.
  public var converseResponse: Google_Assistant_Embedded_V1alpha1_ConverseResponse.OneOf_ConverseResponse? = nil

  /// *Output-only* If set, returns a [google.rpc.Status][google.rpc.Status]
  /// message that specifies the error for the operation. If an error occurs
  /// during processing, this message will be set and there will be no further
  /// messages sent.
  public var error: Google_Rpc_Status {
    get {
      if case .error(let v)? = converseResponse {return v}
      return Google_Rpc_Status()
    }
    set {converseResponse = .error(newValue)}
  }

  /// *Output-only* Indicates the type of event.
  public var eventType: Google_Assistant_Embedded_V1alpha1_ConverseResponse.EventType {
    get {
      if case .eventType(let v)? = converseResponse {return v}
      return .unspecified
    }
    set {converseResponse = .eventType(newValue)}
  }

  /// *Output-only* The audio containing the assistant's response to the query.
  public var audioOut: Google_Assistant_Embedded_V1alpha1_AudioOut {
    get {
      if case .audioOut(let v)? = converseResponse {return v}
      return Google_Assistant_Embedded_V1alpha1_AudioOut()
    }
    set {converseResponse = .audioOut(newValue)}
  }

  /// *Output-only* The semantic result for the user's spoken query.
  public var result: Google_Assistant_Embedded_V1alpha1_ConverseResult {
    get {
      if case .result(let v)? = converseResponse {return v}
      return Google_Assistant_Embedded_V1alpha1_ConverseResult()
    }
    set {converseResponse = .result(newValue)}
  }

  public var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Exactly one of these fields will be populated in each `ConverseResponse`.
  public enum OneOf_ConverseResponse: Equatable {
    /// *Output-only* If set, returns a [google.rpc.Status][google.rpc.Status]
    /// message that specifies the error for the operation. If an error occurs
    /// during processing, this message will be set and there will be no further
    /// messages sent.
    case error(Google_Rpc_Status)
    /// *Output-only* Indicates the type of event.
    case eventType(Google_Assistant_Embedded_V1alpha1_ConverseResponse.EventType)
    /// *Output-only* The audio containing the assistant's response to the query.
    case audioOut(Google_Assistant_Embedded_V1alpha1_AudioOut)
    /// *Output-only* The semantic result for the user's spoken query.
    case result(Google_Assistant_Embedded_V1alpha1_ConverseResult)

  #if !swift(>=4.1)
    public static func ==(lhs: Google_Assistant_Embedded_V1alpha1_ConverseResponse.OneOf_ConverseResponse, rhs: Google_Assistant_Embedded_V1alpha1_ConverseResponse.OneOf_ConverseResponse) -> Bool {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch (lhs, rhs) {
      case (.error, .error): return {
        guard case .error(let l) = lhs, case .error(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.eventType, .eventType): return {
        guard case .eventType(let l) = lhs, case .eventType(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.audioOut, .audioOut): return {
        guard case .audioOut(let l) = lhs, case .audioOut(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      case (.result, .result): return {
        guard case .result(let l) = lhs, case .result(let r) = rhs else { preconditionFailure() }
        return l == r
      }()
      default: return false
      }
    }
  #endif
  }

  /// Indicates the type of event.
  public enum EventType: SwiftProtobuf.Enum {
    public typealias RawValue = Int

    /// No event specified.
    case unspecified // = 0

    /// This event indicates that the server has detected the end of the user's
    /// speech utterance and expects no additional speech. Therefore, the server
    /// will not process additional audio (although it may subsequently return
    /// additional results). The client should stop sending additional audio
    /// data, half-close the gRPC connection, and wait for any additional results
    /// until the server closes the gRPC connection.
    case endOfUtterance // = 1
    case UNRECOGNIZED(Int)

    public init() {
      self = .unspecified
    }

    public init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .unspecified
      case 1: self = .endOfUtterance
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    public var rawValue: Int {
      switch self {
      case .unspecified: return 0
      case .endOfUtterance: return 1
      case .UNRECOGNIZED(let i): return i
      }
    }

  }

  public init() {}
}

#if swift(>=4.2)

extension Google_Assistant_Embedded_V1alpha1_ConverseResponse.EventType: CaseIterable {
  // The compiler won't synthesize support with the UNRECOGNIZED case.
  public static var allCases: [Google_Assistant_Embedded_V1alpha1_ConverseResponse.EventType] = [
    .unspecified,
    .endOfUtterance,
  ]
}

#endif  // swift(>=4.2)

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "google.assistant.embedded.v1alpha1"

extension Google_Assistant_Embedded_V1alpha1_ConverseConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ConverseConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "audio_in_config"),
    2: .standard(proto: "audio_out_config"),
    3: .standard(proto: "converse_state"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._audioInConfig) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._audioOutConfig) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._converseState) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if let v = self._audioInConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }
    if let v = self._audioOutConfig {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    }
    if let v = self._converseState {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Assistant_Embedded_V1alpha1_ConverseConfig, rhs: Google_Assistant_Embedded_V1alpha1_ConverseConfig) -> Bool {
    if lhs._audioInConfig != rhs._audioInConfig {return false}
    if lhs._audioOutConfig != rhs._audioOutConfig {return false}
    if lhs._converseState != rhs._converseState {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Assistant_Embedded_V1alpha1_AudioInConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AudioInConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "encoding"),
    2: .standard(proto: "sample_rate_hertz"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.encoding) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.sampleRateHertz) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.encoding != .unspecified {
      try visitor.visitSingularEnumField(value: self.encoding, fieldNumber: 1)
    }
    if self.sampleRateHertz != 0 {
      try visitor.visitSingularInt32Field(value: self.sampleRateHertz, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Assistant_Embedded_V1alpha1_AudioInConfig, rhs: Google_Assistant_Embedded_V1alpha1_AudioInConfig) -> Bool {
    if lhs.encoding != rhs.encoding {return false}
    if lhs.sampleRateHertz != rhs.sampleRateHertz {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Assistant_Embedded_V1alpha1_AudioInConfig.Encoding: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "ENCODING_UNSPECIFIED"),
    1: .same(proto: "LINEAR16"),
    2: .same(proto: "FLAC"),
  ]
}

extension Google_Assistant_Embedded_V1alpha1_AudioOutConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AudioOutConfig"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "encoding"),
    2: .standard(proto: "sample_rate_hertz"),
    3: .standard(proto: "volume_percentage"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.encoding) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.sampleRateHertz) }()
      case 3: try { try decoder.decodeSingularInt32Field(value: &self.volumePercentage) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.encoding != .unspecified {
      try visitor.visitSingularEnumField(value: self.encoding, fieldNumber: 1)
    }
    if self.sampleRateHertz != 0 {
      try visitor.visitSingularInt32Field(value: self.sampleRateHertz, fieldNumber: 2)
    }
    if self.volumePercentage != 0 {
      try visitor.visitSingularInt32Field(value: self.volumePercentage, fieldNumber: 3)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Assistant_Embedded_V1alpha1_AudioOutConfig, rhs: Google_Assistant_Embedded_V1alpha1_AudioOutConfig) -> Bool {
    if lhs.encoding != rhs.encoding {return false}
    if lhs.sampleRateHertz != rhs.sampleRateHertz {return false}
    if lhs.volumePercentage != rhs.volumePercentage {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Assistant_Embedded_V1alpha1_AudioOutConfig.Encoding: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "ENCODING_UNSPECIFIED"),
    1: .same(proto: "LINEAR16"),
    2: .same(proto: "MP3"),
    3: .same(proto: "OPUS_IN_OGG"),
  ]
}

extension Google_Assistant_Embedded_V1alpha1_ConverseState: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ConverseState"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "conversation_state"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularBytesField(value: &self.conversationState) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.conversationState.isEmpty {
      try visitor.visitSingularBytesField(value: self.conversationState, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Assistant_Embedded_V1alpha1_ConverseState, rhs: Google_Assistant_Embedded_V1alpha1_ConverseState) -> Bool {
    if lhs.conversationState != rhs.conversationState {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Assistant_Embedded_V1alpha1_AudioOut: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".AudioOut"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "audio_data"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularBytesField(value: &self.audioData) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.audioData.isEmpty {
      try visitor.visitSingularBytesField(value: self.audioData, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Assistant_Embedded_V1alpha1_AudioOut, rhs: Google_Assistant_Embedded_V1alpha1_AudioOut) -> Bool {
    if lhs.audioData != rhs.audioData {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Assistant_Embedded_V1alpha1_ConverseResult: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ConverseResult"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "spoken_request_text"),
    2: .standard(proto: "spoken_response_text"),
    3: .standard(proto: "conversation_state"),
    4: .standard(proto: "microphone_mode"),
    5: .standard(proto: "volume_percentage"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.spokenRequestText) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.spokenResponseText) }()
      case 3: try { try decoder.decodeSingularBytesField(value: &self.conversationState) }()
      case 4: try { try decoder.decodeSingularEnumField(value: &self.microphoneMode) }()
      case 5: try { try decoder.decodeSingularInt32Field(value: &self.volumePercentage) }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.spokenRequestText.isEmpty {
      try visitor.visitSingularStringField(value: self.spokenRequestText, fieldNumber: 1)
    }
    if !self.spokenResponseText.isEmpty {
      try visitor.visitSingularStringField(value: self.spokenResponseText, fieldNumber: 2)
    }
    if !self.conversationState.isEmpty {
      try visitor.visitSingularBytesField(value: self.conversationState, fieldNumber: 3)
    }
    if self.microphoneMode != .unspecified {
      try visitor.visitSingularEnumField(value: self.microphoneMode, fieldNumber: 4)
    }
    if self.volumePercentage != 0 {
      try visitor.visitSingularInt32Field(value: self.volumePercentage, fieldNumber: 5)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Assistant_Embedded_V1alpha1_ConverseResult, rhs: Google_Assistant_Embedded_V1alpha1_ConverseResult) -> Bool {
    if lhs.spokenRequestText != rhs.spokenRequestText {return false}
    if lhs.spokenResponseText != rhs.spokenResponseText {return false}
    if lhs.conversationState != rhs.conversationState {return false}
    if lhs.microphoneMode != rhs.microphoneMode {return false}
    if lhs.volumePercentage != rhs.volumePercentage {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Assistant_Embedded_V1alpha1_ConverseResult.MicrophoneMode: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "MICROPHONE_MODE_UNSPECIFIED"),
    1: .same(proto: "CLOSE_MICROPHONE"),
    2: .same(proto: "DIALOG_FOLLOW_ON"),
  ]
}

extension Google_Assistant_Embedded_V1alpha1_ConverseRequest: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ConverseRequest"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "config"),
    2: .standard(proto: "audio_in"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        var v: Google_Assistant_Embedded_V1alpha1_ConverseConfig?
        if let current = self.converseRequest {
          try decoder.handleConflictingOneOf()
          if case .config(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.converseRequest = .config(v)}
      }()
      case 2: try {
        if self.converseRequest != nil {try decoder.handleConflictingOneOf()}
        var v: Data?
        try decoder.decodeSingularBytesField(value: &v)
        if let v = v {self.converseRequest = .audioIn(v)}
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.converseRequest {
    case .config?: try {
      guard case .config(let v)? = self.converseRequest else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }()
    case .audioIn?: try {
      guard case .audioIn(let v)? = self.converseRequest else { preconditionFailure() }
      try visitor.visitSingularBytesField(value: v, fieldNumber: 2)
    }()
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Assistant_Embedded_V1alpha1_ConverseRequest, rhs: Google_Assistant_Embedded_V1alpha1_ConverseRequest) -> Bool {
    if lhs.converseRequest != rhs.converseRequest {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Assistant_Embedded_V1alpha1_ConverseResponse: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  public static let protoMessageName: String = _protobuf_package + ".ConverseResponse"
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "error"),
    2: .standard(proto: "event_type"),
    3: .standard(proto: "audio_out"),
    5: .same(proto: "result"),
  ]

  public mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try {
        var v: Google_Rpc_Status?
        if let current = self.converseResponse {
          try decoder.handleConflictingOneOf()
          if case .error(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.converseResponse = .error(v)}
      }()
      case 2: try {
        if self.converseResponse != nil {try decoder.handleConflictingOneOf()}
        var v: Google_Assistant_Embedded_V1alpha1_ConverseResponse.EventType?
        try decoder.decodeSingularEnumField(value: &v)
        if let v = v {self.converseResponse = .eventType(v)}
      }()
      case 3: try {
        var v: Google_Assistant_Embedded_V1alpha1_AudioOut?
        if let current = self.converseResponse {
          try decoder.handleConflictingOneOf()
          if case .audioOut(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.converseResponse = .audioOut(v)}
      }()
      case 5: try {
        var v: Google_Assistant_Embedded_V1alpha1_ConverseResult?
        if let current = self.converseResponse {
          try decoder.handleConflictingOneOf()
          if case .result(let m) = current {v = m}
        }
        try decoder.decodeSingularMessageField(value: &v)
        if let v = v {self.converseResponse = .result(v)}
      }()
      default: break
      }
    }
  }

  public func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every case branch when no optimizations are
    // enabled. https://github.com/apple/swift-protobuf/issues/1034
    switch self.converseResponse {
    case .error?: try {
      guard case .error(let v)? = self.converseResponse else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    }()
    case .eventType?: try {
      guard case .eventType(let v)? = self.converseResponse else { preconditionFailure() }
      try visitor.visitSingularEnumField(value: v, fieldNumber: 2)
    }()
    case .audioOut?: try {
      guard case .audioOut(let v)? = self.converseResponse else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    }()
    case .result?: try {
      guard case .result(let v)? = self.converseResponse else { preconditionFailure() }
      try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
    }()
    case nil: break
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  public static func ==(lhs: Google_Assistant_Embedded_V1alpha1_ConverseResponse, rhs: Google_Assistant_Embedded_V1alpha1_ConverseResponse) -> Bool {
    if lhs.converseResponse != rhs.converseResponse {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Google_Assistant_Embedded_V1alpha1_ConverseResponse.EventType: SwiftProtobuf._ProtoNameProviding {
  public static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "EVENT_TYPE_UNSPECIFIED"),
    1: .same(proto: "END_OF_UTTERANCE"),
  ]
}
